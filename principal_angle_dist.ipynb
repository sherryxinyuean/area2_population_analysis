{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c69407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlb_tools.nwb_interface import NWBDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "from sklearn.linear_model import Ridge, LinearRegression, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, StratifiedShuffleSplit\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "from scipy.linalg import orth\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from numpy.linalg import svd\n",
    "from Area2_analysis.lr_funcs import principal_angles\n",
    "from Area2_analysis.lr_funcs import get_sses_pred, get_sses_mean, nans, calc_proj\n",
    "\n",
    "matplotlib.rc('font', size=18)\n",
    "figDir = '/Users/sherryan/Desktop/paper/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128fbc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "foldername = \"~/area2_population_analysis/s1-kinematics/actpas_NWB/\"\n",
    "monkey = \"Han_20171207\"\n",
    "filename = foldername + monkey + \"_COactpas_TD_offset6.nwb\"\n",
    "\n",
    "# monkey = 'Duncan_20190710'\n",
    "# filename = foldername + monkey + \"_COactpas_offset6.nwb\"\n",
    "\n",
    "dataset_10ms = NWBDataset(filename, split_heldout=False)\n",
    "dataset_10ms.resample(10) #in 10-ms bin, has to resample first for Duncan\n",
    "bin_width = dataset_10ms.bin_width\n",
    "print(bin_width)\n",
    "dataset_10ms.smooth_spk(40, name='smth_40')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd97ce22",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = dataset_10ms.trial_info.shape[0]\n",
    "print(n_trials,'total trials')\n",
    "n_neurons = dataset_10ms.data.spikes.shape[1]\n",
    "print(n_neurons,'neurons')\n",
    "\n",
    "#make dictionary for trial condition (reaching directions) for Stratified CV\n",
    "dataset = dataset_10ms\n",
    "active_mask = (dataset.trial_info.ctr_hold_bump==0) & (dataset.trial_info['split'] != 'none')\n",
    "passive_mask = (dataset.trial_info.ctr_hold_bump==1) & (dataset.trial_info['split'] != 'none')\n",
    "nan_mask = (np.isnan(dataset.trial_info.ctr_hold_bump)) & (dataset.trial_info['split'] != 'none')\n",
    "all_mask = (dataset.trial_info['split'] != 'none')\n",
    "\n",
    "trial_mask = all_mask\n",
    "valid_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(valid_n_trials,'valid trials')\n",
    "\n",
    "trial_mask = active_mask\n",
    "active_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "active_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(active_n_trials,'active trials')\n",
    "\n",
    "trial_mask = passive_mask\n",
    "passive_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "passive_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(passive_n_trials,'passive trials')\n",
    "\n",
    "trial_mask = nan_mask\n",
    "nan_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "nan_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(nan_n_trials,'reach bump trials')\n",
    "\n",
    "active_cond_dir_idx = []\n",
    "passive_cond_dir_idx = []\n",
    "nan_cond_dir_idx = []\n",
    "nan_bump_cond_dir_idx = []\n",
    "for direction in [0,45,90,135,180,225,270,315]:\n",
    "# for direction in [0,90,180,270]:\n",
    "    active_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & (dataset.trial_info['ctr_hold_bump'] == 0) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "    passive_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & (dataset.trial_info['ctr_hold_bump'] == 1) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "    nan_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & (np.isnan(dataset.trial_info.ctr_hold_bump)) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "    nan_bump_cond_dir_idx.append(np.where((dataset.trial_info['bump_dir']%360 == direction) & (np.isnan(dataset.trial_info.ctr_hold_bump)) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "\n",
    "active_cond_dict = nans([active_n_trials])\n",
    "i = 0\n",
    "for idx in active_trials_idx:\n",
    "    for cond in range(0,len(active_cond_dir_idx)):\n",
    "        if idx in active_cond_dir_idx[cond]:\n",
    "            active_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(active_cond_dict)\n",
    "print(len(active_cond_dict))\n",
    "\n",
    "passive_cond_dict = nans([passive_n_trials])\n",
    "i = 0\n",
    "for idx in passive_trials_idx:\n",
    "    for cond in range(0,len(passive_cond_dir_idx)):\n",
    "        if idx in passive_cond_dir_idx[cond]:\n",
    "            passive_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(passive_cond_dict)\n",
    "print(len(passive_cond_dict))\n",
    "\n",
    "nan_cond_dict = nans([nan_n_trials])\n",
    "i = 0\n",
    "for idx in nan_trials_idx:\n",
    "    for cond in range(0,len(nan_cond_dir_idx)):\n",
    "        if idx in nan_cond_dir_idx[cond]:\n",
    "            nan_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(nan_cond_dict)\n",
    "print(len(nan_cond_dict))\n",
    "\n",
    "nan_bump_cond_dict = nans([nan_n_trials])\n",
    "i = 0\n",
    "for idx in nan_trials_idx:\n",
    "    for cond in range(0,len(nan_bump_cond_dir_idx)):\n",
    "        if idx in nan_bump_cond_dir_idx[cond]:\n",
    "            nan_bump_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(nan_bump_cond_dict)\n",
    "print(len(nan_bump_cond_dict))\n",
    "\n",
    "if monkey == 'Duncan_20190710':\n",
    "    active_df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range = (-100,100), ignored_trials = ~active_mask)\n",
    "    del_indices = list(set(active_trials_idx) - set(active_df['trial_id'].unique()))\n",
    "    print('was',active_n_trials,'active trials')\n",
    "    active_n_trials = active_n_trials - len(list(set(active_trials_idx) - set(active_df['trial_id'].unique())))\n",
    "    active_cond_dict_onset = np.delete(active_cond_dict,np.where(np.isin(active_trials_idx, del_indices)))\n",
    "    print('now',active_n_trials,'active trials')\n",
    "    print(len(active_cond_dict_onset))\n",
    "\n",
    "    passive_df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range = (-100,100), ignored_trials = ~passive_mask)\n",
    "    del_indices = list(set(passive_trials_idx) - set(passive_df['trial_id'].unique()))\n",
    "    print('was',passive_n_trials,'passive trials')\n",
    "    passive_n_trials = passive_n_trials - len(list(set(passive_trials_idx) - set(passive_df['trial_id'].unique())))\n",
    "    passive_cond_dict = np.delete(passive_cond_dict,np.where(np.isin(passive_trials_idx, del_indices)))\n",
    "    print('now',passive_n_trials,'passive trials')\n",
    "    print(len(passive_cond_dict))\n",
    "\n",
    "active_df = dataset_10ms.make_trial_data(align_field='move_offset_time', align_range = (-100,0), ignored_trials = ~active_mask)\n",
    "del_indices = list(set(active_trials_idx) - set(active_df['trial_id'].unique()))\n",
    "print('was',active_n_trials,'active trials')\n",
    "active_cond_dict_offset = np.delete(active_cond_dict, np.where(np.isin(active_trials_idx, del_indices))[0])\n",
    "print('now')\n",
    "print(len(active_cond_dict_offset))\n",
    "if monkey == 'Duncan_20190710':\n",
    "    active_cond_dict = active_cond_dict_onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b458210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_axes(data, trial_mask, trial_cond_dict, cos_signal, sin_signal, N=10):\n",
    "\n",
    "    # Select subset of trials\n",
    "    X = data[trial_mask, :]\n",
    "    cos_y = cos_signal[trial_mask]\n",
    "    sin_y = sin_signal[trial_mask]\n",
    "    cond_dict = np.array(trial_cond_dict)[trial_mask]\n",
    "    n_trials, dim = X.shape\n",
    "\n",
    "    # Cosine fits \n",
    "    X_proc = X.copy()\n",
    "    y = cos_y.reshape((n_trials, -1, 1))[:, 0, :]\n",
    "\n",
    "    axes_list_x = np.full((N, y.shape[1], dim), np.nan)\n",
    "    r2_list_x = np.full((N,), np.nan)\n",
    "    r2_cv_list_x = np.full((N,), np.nan)\n",
    "\n",
    "    for i in range(N):\n",
    "        reg = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "        axes_list_x[i, :, :] = reg.best_estimator_.coef_\n",
    "        r2_list_x[i] = reg.best_estimator_.score(X_proc, y)\n",
    "        weights = reg.best_estimator_.coef_\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        true_concat = np.full((n_trials, 1), np.nan)\n",
    "        pred_concat = np.full((n_trials, 1), np.nan)\n",
    "        trial_save_idx = 0\n",
    "\n",
    "        for tr, te in skf.split(range(n_trials), cond_dict):\n",
    "            X_train, X_test = X_proc[tr, :], X_proc[te, :]\n",
    "            y_train, y_test = y[tr], y[te]\n",
    "            lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "            lr.fit(X_train, y_train)\n",
    "            y_pred = lr.predict(X_test)\n",
    "            n = len(y_test)\n",
    "            true_concat[trial_save_idx:trial_save_idx+n] = y_test.reshape(-1, 1)\n",
    "            pred_concat[trial_save_idx:trial_save_idx+n] = y_pred.reshape(-1, 1)\n",
    "            trial_save_idx += n\n",
    "\n",
    "        sses = get_sses_pred(true_concat, pred_concat)\n",
    "        sses_mean = get_sses_mean(true_concat)\n",
    "        R2 = 1 - np.sum(sses) / np.sum(sses_mean)\n",
    "        r2_cv_list_x[i] = R2\n",
    "        # project out the current axis\n",
    "        X_proc = X_proc - calc_proj(X_proc, weights.T).T\n",
    "\n",
    "    # Sine fits \n",
    "    X_proc = X.copy()\n",
    "    y = sin_y.reshape((n_trials, -1, 1))[:, 0, :]\n",
    "\n",
    "    axes_list_y = np.full((N, y.shape[1], dim), np.nan)\n",
    "    r2_list_y = np.full((N,), np.nan)\n",
    "    r2_cv_list_y = np.full((N,), np.nan)\n",
    "\n",
    "    for i in range(N):\n",
    "        reg = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "        axes_list_y[i, :, :] = reg.best_estimator_.coef_\n",
    "        r2_list_y[i] = reg.best_estimator_.score(X_proc, y)\n",
    "        weights = reg.best_estimator_.coef_\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        true_concat = np.full((n_trials, 1), np.nan)\n",
    "        pred_concat = np.full((n_trials, 1), np.nan)\n",
    "        trial_save_idx = 0\n",
    "\n",
    "        for tr, te in skf.split(range(n_trials), cond_dict):\n",
    "            X_train, X_test = X_proc[tr, :], X_proc[te, :]\n",
    "            y_train, y_test = y[tr], y[te]\n",
    "            lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "            lr.fit(X_train, y_train)\n",
    "            y_pred = lr.predict(X_test)\n",
    "            n = len(y_test)\n",
    "            true_concat[trial_save_idx:trial_save_idx+n] = y_test.reshape(-1, 1)\n",
    "            pred_concat[trial_save_idx:trial_save_idx+n] = y_pred.reshape(-1, 1)\n",
    "            trial_save_idx += n\n",
    "\n",
    "        sses = get_sses_pred(true_concat, pred_concat)\n",
    "        sses_mean = get_sses_mean(true_concat)\n",
    "        R2 = 1 - np.sum(sses) / np.sum(sses_mean)\n",
    "        r2_cv_list_y[i] = R2\n",
    "\n",
    "        X_proc = X_proc - calc_proj(X_proc, weights.T).T\n",
    "\n",
    "    return (\n",
    "        axes_list_x.squeeze(), r2_cv_list_x,\n",
    "        axes_list_y.squeeze(), r2_cv_list_y\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c48326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "x_field = 'spikes_smth_40'\n",
    "# Fix the trial universe to OFFSET-survivors \n",
    "offset_df = dataset.make_trial_data(\n",
    "    align_field='move_offset_time', align_range=(-100, 0), ignored_trials=~active_mask\n",
    ")\n",
    "offset_keep_ids = offset_df['trial_id'].unique()\n",
    "mask_offset_full = dataset.trial_info.trial_id.isin(offset_keep_ids).to_numpy()\n",
    "n_offset = len(offset_keep_ids)\n",
    "\n",
    "# Global z-scoring ONCE on (-100, 1500), but only on offset trials\n",
    "df_all = dataset.make_trial_data(\n",
    "    align_field='move_onset_time', align_range=(-100, 1500), ignored_trials=~mask_offset_full\n",
    ")\n",
    "dim = dataset.data[x_field].shape[1]\n",
    "X_all = df_all[x_field].to_numpy().reshape((-1, dim))\n",
    "mean, std = np.nanmean(X_all, axis=0), np.nanstd(X_all, axis=0)\n",
    "X_all_z = (X_all - mean) / (std )\n",
    "# pca = PCA(n_components=40)\n",
    "# pca.fit(X_all)\n",
    "# pca.fit(X_all_z)\n",
    "# print('PCA total var explained:',sum(pca.explained_variance_ratio_))\n",
    "# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "\n",
    "def prepare_epoch_data(align_field, align_range):\n",
    "    df = dataset.make_trial_data(\n",
    "        align_field=align_field, align_range=align_range, ignored_trials=~mask_offset_full\n",
    "    )\n",
    "    trial_ids = df['trial_id'].unique()\n",
    "    n_trials = len(trial_ids)\n",
    "\n",
    "    X = df[x_field].to_numpy().reshape((-1, dim))\n",
    "    X = (X - mean) / (std )  # use the SAME global mean/std from step 1\n",
    "    X_trials = X.reshape((n_trials, -1, dim))\n",
    "    X_mean = np.nanmean(X_trials, axis=1)\n",
    "    # X_mean_pca = pca.transform(X_mean)\n",
    "\n",
    "    dirs = np.array([\n",
    "        dataset.trial_info.loc[dataset.trial_info.trial_id == t, 'cond_dir'].item()\n",
    "        for t in trial_ids\n",
    "    ])\n",
    "    cos_sig = np.cos(np.deg2rad(dirs))\n",
    "    sin_sig = np.sin(np.deg2rad(dirs))\n",
    "    return X_mean, dirs, cos_sig, sin_sig\n",
    "\n",
    "# Onset (CD) and Offset (FB), both restricted to offset trial universe\n",
    "X_onset, dirs_onset, cos_onset, sin_onset  = prepare_epoch_data('move_onset_time',  (-100, 0))\n",
    "X_offset, dirs_offset, cos_offset, sin_offset = prepare_epoch_data('move_offset_time', (-100, 0))\n",
    "\n",
    "# both are over the same offset-survivor trials\n",
    "n_trials = X_offset.shape[0]\n",
    "conds = dirs_offset.astype(int)  # stratify by direction (same set for both epochs)\n",
    "\n",
    "# Make 100 shared 50/50 splits over the offset trial universe \n",
    "sss = StratifiedShuffleSplit(n_splits=100, test_size=0.5, random_state=0)\n",
    "splits = [(tr_idx, te_idx) for tr_idx, te_idx in sss.split(np.arange(n_trials), conds)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5e4a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "CD, FB, CD2, FB2, FB3 = [], [], [], [],[]\n",
    "\n",
    "for tr_idx, te_idx in splits:\n",
    "    mask_tr = np.zeros(n_trials, dtype=bool)\n",
    "    mask_te = np.zeros(n_trials, dtype=bool)\n",
    "    mask_tr[tr_idx] = True\n",
    "    mask_te[te_idx] = True\n",
    "\n",
    "    # CD subspace from train split\n",
    "    axes_x_cd, r2_x_cd, axes_y_cd, r2_y_cd = fit_axes(\n",
    "        X_onset, mask_tr, conds, cos_onset, sin_onset)\n",
    "    CD_axes = np.vstack((axes_x_cd[r2_x_cd > 0], axes_y_cd[r2_y_cd > 0]))\n",
    "\n",
    "    # FB subspace from train split\n",
    "    axes_x_fb, r2_x_fb, axes_y_fb, r2_y_fb = fit_axes(\n",
    "        X_offset, mask_tr, conds, cos_offset, sin_offset)\n",
    "    FB_axes = np.vstack((axes_x_fb[r2_x_fb > 0], axes_y_fb[r2_y_fb > 0]))\n",
    "\n",
    "    # CD2 and FB2 from test split\n",
    "    axes_x_cd2, r2_x_cd2, axes_y_cd2, r2_y_cd2 = fit_axes(\n",
    "        X_onset, mask_te, conds, cos_onset, sin_onset)\n",
    "    CD_axes2 = np.vstack((axes_x_cd2[r2_x_cd2 > 0], axes_y_cd2[r2_y_cd2 > 0]))\n",
    "\n",
    "    axes_x_fb2, r2_x_fb2, axes_y_fb2, r2_y_fb2 = fit_axes(\n",
    "        X_offset, mask_te, conds, cos_offset, sin_offset)\n",
    "    FB_axes2 = np.vstack((axes_x_fb2[r2_x_fb2 > 0], axes_y_fb2[r2_y_fb2 > 0]))\n",
    "\n",
    "    # FB3 from test split, orthogonal to CD2\n",
    "    X_offset_cdOut = X_offset - calc_proj(X_offset,CD_axes2.T).T\n",
    "    axes_x_fb3, r2_x_fb3, axes_y_fb3, r2_y_fb3 = fit_axes(\n",
    "        X_offset_cdOut, mask_te, conds, cos_offset, sin_offset)\n",
    "    FB_axes3 = np.vstack((axes_x_fb3[r2_x_fb3 > 0], axes_y_fb3[r2_y_fb3 > 0]))\n",
    "\n",
    "    CD.append(CD_axes)\n",
    "    FB.append(FB_axes)\n",
    "    CD2.append(CD_axes2)\n",
    "    FB2.append(FB_axes2)\n",
    "    FB3.append(FB_axes3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bc7ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(str(monkey)+\"_split_distributions\",\n",
    "         CD = CD, FB = FB, CD2 = CD2, FB2 = FB2, FB3 = FB3,splits = splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f439ed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey = \"Han_20171207\"\n",
    "\n",
    "\n",
    "data = np.load(str(monkey)+\"_split_distributions.npz\",allow_pickle=True)\n",
    "data.files\n",
    "CD = data['CD']\n",
    "FB = data['FB']\n",
    "CD2 = data['CD2']\n",
    "FB2 = data['FB2']\n",
    "FB3 = data['FB3']\n",
    "splits = data['splits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375c1738",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(str(monkey)+\"_split_distributions\",\n",
    "         CD = CD, FB = FB, CD2 = CD2, FB2 = FB2, FB3 = FB3,splits = splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_first_angles(listA, listB):\n",
    "    avg_angles = []\n",
    "    for A, B in zip(listA, listB):\n",
    "        angles_rad = principal_angles(A.T, B.T)\n",
    "        avg_angles.append(np.degrees(angles_rad[0]))\n",
    "    return np.array(avg_angles)\n",
    "\n",
    "angles_CD_FB = compute_first_angles(CD, FB)\n",
    "angles_CD_CD = compute_first_angles(CD, CD2)\n",
    "angles_FB_FB = compute_first_angles(FB, FB2)\n",
    "angles_CD_FB3 = compute_first_angles(CD, FB3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad758c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "figDir = '/Users/sherryan/Desktop/paper/'\n",
    "matplotlib.rc('font', size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab97579",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.concatenate([angles_CD_CD, angles_CD_FB, angles_CD_FB3, angles_FB_FB])\n",
    "xmin, xmax = all_data.min(), all_data.max()\n",
    "bins = np.linspace(xmin, xmax, 30)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.hist(angles_CD_CD,color='green',alpha=0.5,edgecolor='black',bins = bins, label = '(H0) CD vs CD2')\n",
    "plt.hist(angles_CD_FB3,color='orange',alpha=0.5,edgecolor='black',bins = bins, label = '(H1) CD vs FB3')\n",
    "plt.hist(angles_CD_FB,color='blue',alpha=0.5,edgecolor='black', bins = bins, label = 'CD vs FB')\n",
    "# plt.legend(fontsize=10)\n",
    "plt.xlabel('First principal angle (deg)')\n",
    "plt.ylabel('Count')\n",
    "# plt.savefig(figDir + monkey + '_angle_dist_first_supp.pdf',dpi = 'figure')\n",
    "plt.xlim([30,90])\n",
    "\n",
    "plt.text(40, plt.gca().get_ylim()[1] * 0.9,  # Adjust position as needed\n",
    "         str(np.mean(angles_CD_CD))+'_'+str(np.mean(angles_CD_FB))+'_'+str(np.mean(angles_CD_FB3)), color='black', fontsize=10)\n",
    "plt.savefig(figDir + monkey + '_angle_dist_first_main.pdf',dpi = 'figure')\n",
    "\n",
    "plt.hist(angles_FB_FB,color='magenta',alpha=0.5,edgecolor='black',bins = bins, label = '(H0) FB vs FB2')\n",
    "plt.text(50, plt.gca().get_ylim()[1] * 0.8,  # Adjust position as needed\n",
    "         str(np.mean(angles_FB_FB)), color='black', fontsize=10)\n",
    "plt.savefig(figDir + monkey + '_angle_dist_first_supp.pdf',dpi = 'figure')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318163c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_angles(listA, listB):\n",
    "    avg_angles = []\n",
    "    for A, B in zip(listA, listB):\n",
    "        angles_rad = principal_angles(A.T, B.T)\n",
    "        avg_angles.append(np.degrees(np.mean(angles_rad)))\n",
    "    return np.array(avg_angles)\n",
    "\n",
    "angles_CD_FB_avg = compute_avg_angles(CD, FB)\n",
    "angles_CD_CD_avg = compute_avg_angles(CD, CD2)\n",
    "angles_FB_FB_avg = compute_avg_angles(FB, FB2)\n",
    "angles_CD_FB3_avg = compute_avg_angles(CD, FB3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a38a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.concatenate([angles_CD_CD_avg, angles_CD_FB_avg, angles_CD_FB3_avg, angles_FB_FB_avg])\n",
    "xmin, xmax = all_data.min(), all_data.max()\n",
    "bins = np.linspace(xmin, xmax, 30)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.hist(angles_CD_CD_avg,color='green',alpha=0.5,edgecolor='black',bins = bins, label = '(H0) CD vs CD2')\n",
    "plt.hist(angles_CD_FB3_avg,color='orange',alpha=0.5,edgecolor='black',bins = bins, label = '(H1) CD vs FB3')\n",
    "plt.hist(angles_CD_FB_avg,color='blue',alpha=0.5,edgecolor='black', bins = bins, label = 'CD vs FB')\n",
    "# plt.legend(fontsize=10)\n",
    "plt.xlabel('Average principal angle (deg)')\n",
    "plt.ylabel('Count')\n",
    "# plt.savefig(figDir + monkey + '_angle_dist_first_supp.pdf',dpi = 'figure')\n",
    "plt.xlim([45,90])\n",
    "\n",
    "plt.text(40, plt.gca().get_ylim()[1] * 0.9,  # Adjust position as needed\n",
    "         str(np.mean(angles_CD_CD_avg))+'_'+str(np.mean(angles_CD_FB_avg))+'_'+str(np.mean(angles_CD_FB3_avg)), color='black', fontsize=10)\n",
    "plt.savefig(figDir + monkey + '_angle_dist_avg_main.pdf',dpi = 'figure')\n",
    "\n",
    "plt.hist(angles_FB_FB_avg,color='magenta',alpha=0.5,edgecolor='black',bins = bins, label = '(H0) FB vs FB2')\n",
    "plt.text(50, plt.gca().get_ylim()[1] * 0.8,  # Adjust position as needed\n",
    "         str(np.mean(angles_FB_FB_avg)), color='black', fontsize=10)\n",
    "plt.savefig(figDir + monkey + '_angle_dist_avg_supp.pdf',dpi = 'figure')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce45f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# ax.spines['top'].set_visible(False)\n",
    "\n",
    "# plt.hist(angles_CD_CD_avg, color='green', alpha=0.5, edgecolor='black', bins=20, label='(H0) CD vs CD2')\n",
    "# plt.hist(angles_FB_FB_avg, color='magenta', alpha=0.5, edgecolor='black', bins=20, label='(H0) FB vs FB2')\n",
    "# plt.hist(angles_CD_FB_avg, color='blue', alpha=0.5, edgecolor='black', bins=20, label='CD vs FB')\n",
    "# plt.hist(angles_CD_FB3_avg, color='orange', alpha=0.5, edgecolor='black', bins=20, label='(H1) CD vs FB3')\n",
    "\n",
    "# plt.legend(fontsize=10)\n",
    "# plt.xlabel('Average principal angle (deg)')\n",
    "# plt.ylabel('Count')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e496933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a509a5b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e91c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_alignment_index(X_epoch, CD_axes, FB_axes):\n",
    "    # center trials\n",
    "    X = X_epoch - X_epoch.mean(axis=0, keepdims=True)\n",
    "\n",
    "    # orthonormal bases in neuron space\n",
    "    D_CD, _  = np.linalg.qr(CD_axes.T)   # (n_neurons x k_cd)\n",
    "    D_FB, _  = np.linalg.qr(FB_axes.T)   # (n_neurons x k_fb)\n",
    "\n",
    "    # DS component of the epoch activity (project X into CD subspace)\n",
    "    P_CD = D_CD @ D_CD.T         \n",
    "    X_CD = X @ P_CD     \n",
    "    C_CD = np.cov(X_CD, rowvar=False, bias=True)  # make sure it's (n_neurons x n_neurons)\n",
    "\n",
    "    # variance of CD-space that lies in FB dims\n",
    "    num = np.trace(D_FB.T @ C_CD @ D_FB)\n",
    "\n",
    "    # total CD-space variance\n",
    "    den = np.trace(D_CD.T @ C_CD @ D_CD) \n",
    "\n",
    "    return float(num / den)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78686a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ebe9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masks_from_splits(splits, n_trials):\n",
    "    m_tr_list, m_te_list = [], []\n",
    "    for tr_idx, te_idx in splits:\n",
    "        m_tr = np.zeros(n_trials, dtype=bool); m_tr[tr_idx] = True\n",
    "        m_te = np.zeros(n_trials, dtype=bool); m_te[te_idx] = True\n",
    "        m_tr_list.append(m_tr); m_te_list.append(m_te)\n",
    "    return m_tr_list, m_te_list\n",
    "\n",
    "m_tr_list, m_te_list = masks_from_splits(splits, n_trials)\n",
    "\n",
    "# DS alignment distributions\n",
    "A_DS_CD_FB = []\n",
    "A_DS_CD_CD = []\n",
    "A_DS_FB_FB = []\n",
    "A_DS_CD_FB3 = []\n",
    "\n",
    "for k in range(len(CD)):\n",
    "    A_DS_CD_FB.append(ds_alignment_index(X_onset[m_tr_list[k]], CD[k], FB[k]))\n",
    "    A_DS_CD_CD.append(ds_alignment_index(X_onset[m_tr_list[k]], CD[k], CD2[k]))\n",
    "    A_DS_FB_FB.append(ds_alignment_index(X_offset[m_tr_list[k]], FB[k], FB2[k]))\n",
    "    A_DS_CD_FB3.append(ds_alignment_index(X_onset[m_tr_list[k]], CD[k], FB3[k]))\n",
    "\n",
    "A_DS_CD_FB = np.array(A_DS_CD_FB)\n",
    "A_DS_CD_CD = np.array(A_DS_CD_CD)\n",
    "A_DS_FB_FB = np.array(A_DS_FB_FB)\n",
    "A_DS_CD_FB3 = np.array(A_DS_CD_FB3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9622c338",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b90d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.concatenate([A_DS_CD_CD, A_DS_CD_FB, A_DS_CD_FB3, A_DS_FB_FB])\n",
    "xmin, xmax = all_data.min(), all_data.max()\n",
    "bins = np.linspace(xmin, xmax, 30)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.hist(A_DS_CD_CD,color='green',alpha=0.5,edgecolor='black',bins = bins, label = '(H0) CD vs CD2')\n",
    "plt.hist(A_DS_CD_FB3,color='orange',alpha=0.5,edgecolor='black',bins = bins, label = '(H1) CD vs FB3')\n",
    "plt.hist(A_DS_CD_FB,color='blue',alpha=0.5,edgecolor='black', bins = bins, label = 'CD vs FB')\n",
    "# plt.legend(fontsize=10)\n",
    "plt.xlabel('Alignment index')\n",
    "plt.ylabel('Count')\n",
    "# plt.savefig(figDir + monkey + '_angle_dist_first_supp.pdf',dpi = 'figure') \n",
    "plt.xlim([0,.8])\n",
    "\n",
    "plt.text(.2, plt.gca().get_ylim()[1] * 0.9,  # Adjust position as needed\n",
    "         str(np.mean(A_DS_CD_CD))+'_'+str(np.mean(A_DS_CD_FB))+'_'+str(np.mean(A_DS_CD_FB3)), color='black', fontsize=10)\n",
    "\n",
    "plt.savefig(figDir + monkey + '_angle_dist_align_main.pdf',dpi = 'figure')\n",
    "\n",
    "plt.hist(A_DS_FB_FB,color='magenta',alpha=0.5,edgecolor='black',bins = bins, label = '(H0) FB vs FB2')\n",
    "plt.text(.3, plt.gca().get_ylim()[1] * 0.8,  # Adjust position as needed\n",
    "         str(np.mean(A_DS_FB_FB)), color='black', fontsize=10)\n",
    "plt.savefig(figDir + monkey + '_angle_dist_align_supp.pdf',dpi = 'figure')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab935f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False); ax.spines['top'].set_visible(False)\n",
    "plt.hist(A_DS_CD_CD, color='green',  alpha=0.5, edgecolor='black', bins=20, label='(H0) CD vs CD2')\n",
    "plt.hist(A_DS_FB_FB, color='magenta',alpha=0.5, edgecolor='black', bins=20, label='(H0) FB vs FB2')\n",
    "plt.hist(A_DS_CD_FB, color='blue',   alpha=0.5, edgecolor='black', bins=20, label='CD vs FB')\n",
    "plt.hist(A_DS_CD_FB3, color='orange',alpha=0.5, edgecolor='black', bins=20, label='(H1) CD vs FB3')\n",
    "plt.legend(fontsize=10)\n",
    "plt.xlabel('Alignment index')\n",
    "plt.ylabel('Count')\n",
    "# plt.title('Direction-Selective Alignment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09c9582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc63bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alec's code;calculate the alignment index using the Elsayed's method.\n",
    "def subspace_overlap(data,space):\n",
    "    # data should be N_neurons x N_time, space should be n_components x N_neurons\n",
    "    return np.sum(np.diag(space@np.cov(data)@space.T))/np.sum(TruncatedSVD(n_components=n_PC).fit(np.cov(data)).singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b8d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pca_basis(X, k):\n",
    "    # center per neuron\n",
    "    X = np.asarray(X, dtype=np.float64)\n",
    "    Xc = X - X.mean(axis=0, keepdims=True)\n",
    "    # SVD of centered data gives PCA; covariance eigenvalues = S^2 / N\n",
    "    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
    "    D = Vt[:k].T                         # neurons x k\n",
    "    lam = (S**2 / Xc.shape[0])[:k]       # top-k eigenvalues\n",
    "    return D, lam\n",
    "\n",
    "def alignment_index_prep_to_move(X_prep, X_move, k=10):\n",
    "    \"\"\"\n",
    "    Elsayed alignment index:\n",
    "      A = Tr(D_move^T C_prep D_move) / sum_{i=1..k} lambda_prep(i)\n",
    "    where:\n",
    "      C_prep is covariance of prep epoch,\n",
    "      D_move are top-k PCs of move epoch (neurons x k),\n",
    "      lambda_prep are top-k eigenvalues of C_prep.\n",
    "    \"\"\"\n",
    "    # covariance of prep (centered)\n",
    "    Xp = X_prep - X_prep.mean(0, keepdims=True)\n",
    "    C_prep = np.cov(Xp, rowvar=False, bias=True)\n",
    "\n",
    "    # move PCs (neurons x k)\n",
    "    D_move, _ = _pca_basis(X_move, k)\n",
    "\n",
    "    # numerator: prep variance captured by move PCs\n",
    "    num = np.trace(D_move.T @ C_prep @ D_move)\n",
    "\n",
    "    # denominator: best possible prep variance captured by k PCs\n",
    "    evals = np.linalg.eigvalsh(C_prep)           # ascending\n",
    "    denom = np.sum(evals[-k:])\n",
    "\n",
    "    return float(num / (denom + 1e-12))\n",
    "\n",
    "def alignment_index_symmetric(X_onset, X_offset, k=10):\n",
    "    # symmetric version: average of prep→move and move→prep\n",
    "    A_pm = alignment_index_prep_to_move(X_onset, X_offset, k)\n",
    "    A_mp = alignment_index_prep_to_move(X_offset, X_onset, k)\n",
    "    return A_pm, A_mp, 0.5*(A_pm + A_mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861ce592",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_pm_splits, A_mp_splits, A_sym_splits = [], [], []\n",
    "\n",
    "for tr_idx, te_idx in splits:\n",
    "    m_tr = np.zeros(n_trials, dtype=bool); m_tr[tr_idx] = True\n",
    "    m_te = np.zeros(n_trials, dtype=bool); m_te[te_idx] = True\n",
    "\n",
    "    A_pm = alignment_index_prep_to_move(X_onset[m_tr],  X_offset[m_te])\n",
    "    A_mp = alignment_index_prep_to_move(X_offset[m_tr], X_onset[m_te])\n",
    "    A_sym = 0.5 * (A_pm + A_mp)\n",
    "\n",
    "    A_pm_splits.append(A_pm)\n",
    "    A_mp_splits.append(A_mp)\n",
    "    A_sym_splits.append(A_sym)\n",
    "\n",
    "A_pm_splits  = np.array(A_pm_splits)\n",
    "A_mp_splits  = np.array(A_mp_splits)\n",
    "A_sym_splits = np.array(A_sym_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bda5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "plt.hist(A_pm_splits,  bins=20, alpha=0.5, edgecolor='black', label='Prep→Move')\n",
    "plt.hist(A_mp_splits,  bins=20, alpha=0.5, edgecolor='black', label='Move→Prep')\n",
    "# plt.hist(A_sym_splits, bins=20, alpha=0.5, edgecolor='black', label='Symmetric')\n",
    "\n",
    "plt.xlabel('Alignment index')                               \n",
    "plt.ylabel('Count')\n",
    "plt.title('Elsayed Alignment Index')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdt_env",
   "language": "python",
   "name": "sdt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
