{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlb_tools.nwb_interface import NWBDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "from sklearn.linear_model import Ridge, LinearRegression, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, StratifiedShuffleSplit\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "from scipy.linalg import orth\n",
    "\n",
    "from Neural_Decoding.preprocessing_funcs import get_spikes_with_history\n",
    "from Area2_analysis.lr_funcs import process_train_test, gaussian_filter1d_oneside, gaussian_filter1d_twoside, comp_cc, xcorr, r2_score\n",
    "from Area2_analysis.lr_funcs import get_sses_pred, get_sses_mean, nans\n",
    "# from Area2_analysis.lr_funcs import fit_and_predict, sub_and_predict, pred_with_new_weights\n",
    "# from Area2_analysis.lr_funcs import fit_and_predict_lasso, sub_and_predict_lasso, \n",
    "from Area2_analysis.lr_funcs import fit_and_predict_MC, calc_proj, principal_angles, angle_between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import Area2_analysis.lr_funcs\n",
    "# importlib.reload(Area2_analysis.lr_funcs)\n",
    "# from Area2_analysis.lr_funcs import fit_and_predict_DNN, fit_and_predict_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figDir = '/Users/sherryan/Desktop/paper/'\n",
    "matplotlib.rc('font', size=18)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldername = \"~/area2_population_analysis/s1-kinematics/actpas_NWB/\"\n",
    "# monkey = \"Han_20171207\"\n",
    "# monkey = 'Duncan_20190710'\n",
    "# monkey = \"Chips_20170913\"\n",
    "# monkey = \"Lando_20170731\"\n",
    "\n",
    "# monkey = \"Han_20171201\"\n",
    "# monkey = \"Han_20171204\"\n",
    "# monkey = 'Duncan_20190710'\n",
    "# monkey = 'Duncan_20191016'\n",
    "# monkey = 'Duncan_20191106'\n",
    "\n",
    "monkey = \"Lando_20170803\"\n",
    "\n",
    "\n",
    "filename = foldername + monkey + \"_COactpas_TD_offset6.nwb\"\n",
    "\n",
    "dataset_10ms = NWBDataset(filename, split_heldout=False)\n",
    "\n",
    "dataset_10ms.resample(10) #in 10-ms bin, has to resample first for Duncan\n",
    "bin_width = dataset_10ms.bin_width\n",
    "print(bin_width)\n",
    "\n",
    "# xyz_force = np.array([dataset_5ms.data['force']['x'].to_numpy(), dataset_5ms.data['force']['y'].to_numpy(), dataset_5ms.data['force']['z'].to_numpy()]).T\n",
    "# dataset_10ms.add_continuous_data(xyz_force,'manip_force',chan_names = ['x','y','z'])\n",
    "\n",
    "# dataset_10ms.smooth_spk(40, name='smth_40')\n",
    "\n",
    "# dataset_10ms.smooth_spk(20, name='smth_20')\n",
    "\n",
    "# gaussian_kernel_width = 150 #in ms\n",
    "# sigma = int(gaussian_kernel_width/bin_width)\n",
    "# data_smoothed = gaussian_filter1d_oneside(dataset_10ms.data.spikes.to_numpy().astype(np.float64),sigma,axis=0)\n",
    "# dataset_10ms.add_continuous_data(data_smoothed,'spikes_smth_150_oneside')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xy_vel = dataset_10ms.data['hand_vel'].to_numpy()\n",
    "# xy_acc = np.diff(xy_vel, axis = 0, prepend=[xy_vel[0]])\n",
    "# dataset_10ms.add_continuous_data(xy_acc,'hand_acc',chan_names = ['x','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = '/Users/sherryan/area2_population_analysis/s1-kinematics/'+monkey+'_COactpas_with_emg_TD.mat'\n",
    "# import scipy.io\n",
    "# mat = scipy.io.loadmat(filename)\n",
    "# EMG = mat['trial_data']['emg'][0,0]\n",
    "# dataset_10ms.add_continuous_data(EMG,'EMG')\n",
    "# mat['trial_data']['emg_names'][0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = dataset_10ms.data['EMG'].to_numpy()\n",
    "# print(all_data.shape)\n",
    "# data_for_pca = all_data[~np.isnan(all_data).any(axis=1)]\n",
    "# print(data_for_pca.shape)\n",
    "# explained_var = []\n",
    "# for n in range(20):\n",
    "#     scaler = StandardScaler()\n",
    "#     X = scaler.fit_transform(data_for_pca)\n",
    "#     pca = PCA(n_components=n)\n",
    "#     X = pca.fit(X)\n",
    "#     explained_var.append(np.sum(pca.explained_variance_ratio_))\n",
    "# plt.plot(range(20),explained_var)\n",
    "# print(explained_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = dataset_10ms.data['EMG'].to_numpy()\n",
    "# print(all_data.shape)\n",
    "# data_for_pca = all_data[~np.isnan(all_data).any(axis=1)]\n",
    "# print(data_for_pca.shape)\n",
    "# n_dims = 10\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(data_for_pca)\n",
    "# pca = PCA(n_components=n_dims,random_state = 42)\n",
    "# X = pca.fit(X)\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(data_for_pca)\n",
    "# pca = PCA(n_components=n_dims,random_state = 42)\n",
    "# X = pca.fit(X)\n",
    "    \n",
    "# PCA_data = nans([all_data.shape[0],n_dims])\n",
    "# idx = 0\n",
    "# for dp in all_data:\n",
    "#     dp = dp.reshape((1, -1))\n",
    "#     if np.isnan(dp).any():\n",
    "#         dp_pca = nans([1,n_dims])\n",
    "#     else:\n",
    "#         dp_pca = pca.transform(scaler.transform(dp))\n",
    "#     PCA_data[idx,:] = dp_pca\n",
    "#     idx+=1\n",
    "# print(PCA_data.shape)\n",
    "# dataset_10ms.add_continuous_data(PCA_data,'EMG_PCA')\n",
    "# print('PCA total var explained:',sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muscle_len = dataset_10ms.data['muscle_len'].to_numpy()\n",
    "# muscle_vel = dataset_10ms.data['muscle_vel'].to_numpy()\n",
    "# joint_ang = dataset_10ms.data['joint_ang'].to_numpy()\n",
    "# joint_vel = dataset_10ms.data['joint_vel'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = np.hstack([muscle_len,muscle_vel])\n",
    "# print(all_data.shape)\n",
    "# data_for_pca = all_data[~np.isnan(all_data).any(axis=1)]\n",
    "# print(data_for_pca.shape)\n",
    "# explained_var = []\n",
    "# for n in range(20):\n",
    "#     scaler = StandardScaler()\n",
    "#     X = scaler.fit_transform(data_for_pca)\n",
    "#     pca = PCA(n_components=n)\n",
    "#     X = pca.fit(X)\n",
    "#     explained_var.append(np.sum(pca.explained_variance_ratio_))\n",
    "# plt.plot(range(20),explained_var)\n",
    "# plt.title('muscle len+vel')\n",
    "# print(explained_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = np.hstack([joint_ang,joint_vel])\n",
    "# print(all_data.shape)\n",
    "# data_for_pca = all_data[~np.isnan(all_data).any(axis=1)]\n",
    "# print(data_for_pca.shape)\n",
    "# explained_var = []\n",
    "# for n in range(14):\n",
    "#     scaler = StandardScaler()\n",
    "#     X = scaler.fit_transform(data_for_pca)\n",
    "#     pca = PCA(n_components=n)\n",
    "#     X = pca.fit(X)\n",
    "#     explained_var.append(np.sum(pca.explained_variance_ratio_))\n",
    "# plt.plot(range(14),explained_var)\n",
    "# plt.title('joint ang+vel')\n",
    "# print(explained_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = np.hstack([muscle_len,muscle_vel])\n",
    "# print(all_data.shape)\n",
    "# data_for_pca = all_data[~np.isnan(all_data).any(axis=1)]\n",
    "# print(data_for_pca.shape)\n",
    "# n_dims = 10\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(data_for_pca)\n",
    "# pca = PCA(n_components=n_dims,random_state = 42)\n",
    "# X = pca.fit(X)\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(data_for_pca)\n",
    "# pca = PCA(n_components=n_dims,random_state = 42)\n",
    "# X = pca.fit(X)\n",
    "    \n",
    "# PCA_data = nans([all_data.shape[0],n_dims])\n",
    "# idx = 0\n",
    "# for dp in all_data:\n",
    "#     dp = dp.reshape((1, -1))\n",
    "#     if np.isnan(dp).any():\n",
    "#         dp_pca = nans([1,n_dims])\n",
    "#     else:\n",
    "#         dp_pca = pca.transform(scaler.transform(dp))\n",
    "#     PCA_data[idx,:] = dp_pca\n",
    "#     idx+=1\n",
    "# print(PCA_data.shape)\n",
    "# dataset_10ms.add_continuous_data(PCA_data,'muscle_PCA')\n",
    "# print('PCA total var explained:',sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = np.hstack([joint_ang,joint_vel])\n",
    "# print(all_data.shape)\n",
    "# data_for_pca = all_data[~np.isnan(all_data).any(axis=1)]\n",
    "# print(data_for_pca.shape)\n",
    "# n_dims = 10\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(data_for_pca)\n",
    "# pca = PCA(n_components=n_dims,random_state = 42)\n",
    "# X = pca.fit(X)\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(data_for_pca)\n",
    "# pca = PCA(n_components=n_dims,random_state = 42)\n",
    "# X = pca.fit(X)\n",
    "    \n",
    "# PCA_data = nans([all_data.shape[0],n_dims])\n",
    "# idx = 0\n",
    "# for dp in all_data:\n",
    "#     dp = dp.reshape((1, -1))\n",
    "#     if np.isnan(dp).any():\n",
    "#         dp_pca = nans([1,n_dims])\n",
    "#     else:\n",
    "#         dp_pca = pca.transform(scaler.transform(dp))\n",
    "#     PCA_data[idx,:] = dp_pca\n",
    "#     idx+=1\n",
    "# print(PCA_data.shape)\n",
    "# dataset_10ms.add_continuous_data(PCA_data,'joint_PCA')\n",
    "# print('PCA total var explained:',sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_10ms.smooth_spk(40, name='smth_40')\n",
    "n_dims = 20 \n",
    "all_data = np.array(dataset_10ms.data.spikes_smth_40)\n",
    "print(all_data.shape)\n",
    "if not np.isnan(all_data).any():\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(all_data)\n",
    "    pca = PCA(n_components=n_dims,random_state = 42)\n",
    "    PCA_data = pca.fit_transform(X)\n",
    "print(PCA_data.shape)\n",
    "dataset_10ms.add_continuous_data(PCA_data,'PCA_40')\n",
    "print('PCA total var explained:',sum(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataset = dataset_10ms\n",
    "trial_info = dataset.trial_info\n",
    "\n",
    "# Basic counts\n",
    "n_trials = trial_info.shape[0]\n",
    "print(n_trials, 'total trials')\n",
    "\n",
    "n_neurons = dataset.data.spikes.shape[1]\n",
    "print(n_neurons, 'neurons')\n",
    "\n",
    "# Masks\n",
    "valid_mask   = (trial_info['split'] != 'none')\n",
    "active_mask  = (trial_info.ctr_hold_bump == 0) & valid_mask\n",
    "passive_mask = (trial_info.ctr_hold_bump == 1) & valid_mask\n",
    "nan_mask     = trial_info.ctr_hold_bump.isna() & valid_mask\n",
    "\n",
    "print(trial_info.loc[valid_mask].shape[0],   'valid trials')\n",
    "print(trial_info.loc[active_mask].shape[0],  'active trials')\n",
    "print(trial_info.loc[passive_mask].shape[0], 'passive trials')\n",
    "print(trial_info.loc[nan_mask].shape[0],     'reach bump trials')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cond_dict_from_df(aligned_df, trial_info, dir_col='cond_dir'):\n",
    "    \"\"\"\n",
    "    aligned_df: output of make_trial_data(...), must contain 'trial_id'\n",
    "    trial_info: DataFrame indexed by trial_id\n",
    "    dir_col:    'cond_dir' or 'bump_dir'\n",
    "\n",
    "    Returns:\n",
    "        trial_ids: trial IDs in aligned_df\n",
    "        cond_dict: integer condition index per trial in trial_ids\n",
    "        cond_dirs: np.array of unique directions actually present (sorted)\n",
    "        dir_to_cond: mapping {direction_deg: cond_index}\n",
    "    \"\"\"\n",
    "    # unique trial IDs in aligned data, in the order they appear\n",
    "    trial_ids = aligned_df['trial_id'].drop_duplicates().to_numpy()\n",
    "\n",
    "    # ensure indexing on trial_id\n",
    "    if trial_info.index.name != 'trial_id':\n",
    "        trial_info_by_id = trial_info.set_index('trial_id')\n",
    "    else:\n",
    "        trial_info_by_id = trial_info\n",
    "\n",
    "    # directions for these trials\n",
    "    trial_dirs = (trial_info_by_id.loc[trial_ids, dir_col] % 360).to_numpy()\n",
    "\n",
    "    # NEW: only use the directions that actually exist\n",
    "    cond_dirs = np.sort(np.unique(trial_dirs))       # e.g. [0,90,180,270]\n",
    "    dir_to_cond = {d: i for i, d in enumerate(cond_dirs)}\n",
    "\n",
    "    # map each trial's direction to its condition index\n",
    "    cond_dict = np.array([dir_to_cond[d] for d in trial_dirs])\n",
    "\n",
    "    return trial_ids, cond_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- ACTIVE: aligned to move offset -----\n",
    "active_df_offset = dataset_10ms.make_trial_data(\n",
    "    align_field='move_offset_time', \n",
    "    align_range=(-100, 0),\n",
    "    ignored_trials=~active_mask\n",
    ")\n",
    "\n",
    "active_trial_ids_offset, active_cond_dict_offset = make_cond_dict_from_df(\n",
    "    active_df_offset, trial_info, dir_col='cond_dir'\n",
    ")\n",
    "print('active (offset):', len(active_trial_ids_offset), 'trials')\n",
    "print('active_cond_dict_offset length:', len(active_cond_dict_offset))\n",
    "\n",
    "# ----- ACTIVE: aligned to move onset (if you need this for *any* monkey) -----\n",
    "active_df_onset = dataset_10ms.make_trial_data(\n",
    "    align_field='move_onset_time', \n",
    "    align_range=(-100, 0),\n",
    "    ignored_trials=~active_mask\n",
    ")\n",
    "\n",
    "active_trial_ids_onset, active_cond_dict_onset = make_cond_dict_from_df(\n",
    "    active_df_onset, trial_info, dir_col='cond_dir'\n",
    ")\n",
    "print('active (onset):', len(active_trial_ids_onset), 'trials')\n",
    "print('active_cond_dict_onset length:', len(active_cond_dict_onset))\n",
    "active_n_trials = len(active_trial_ids_onset)\n",
    "active_cond_dict = active_cond_dict_onset  # choose which to use downstream\n",
    "\n",
    "# ----- PASSIVE -----\n",
    "passive_df = dataset_10ms.make_trial_data(\n",
    "    align_field='move_onset_time',\n",
    "    align_range=(-100, 0),\n",
    "    ignored_trials=~passive_mask\n",
    ")\n",
    "\n",
    "passive_trial_ids, passive_cond_dict = make_cond_dict_from_df(\n",
    "    passive_df, trial_info, dir_col='cond_dir'\n",
    ")\n",
    "print('passive:', len(passive_trial_ids), 'trials')\n",
    "print('passive_cond_dict length:', len(passive_cond_dict))\n",
    "passive_n_trials = len(passive_trial_ids)\n",
    "\n",
    "# # ----- NAN / reach bump trials: use cond_dir or bump_dir as needed -----\n",
    "# nan_df = dataset_10ms.make_trial_data(\n",
    "#     align_field='move_onset_time',\n",
    "#     align_range=(-100, 0),\n",
    "#     ignored_trials=~nan_mask\n",
    "# )\n",
    "\n",
    "# nan_trial_ids_cond, nan_cond_dict = make_cond_dict_from_df(\n",
    "#     nan_df, trial_info, dir_col='cond_dir'\n",
    "# )\n",
    "\n",
    "# nan_trial_ids_bump, nan_bump_cond_dict = make_cond_dict_from_df(\n",
    "#     nan_df, trial_info, dir_col='bump_dir'\n",
    "# )\n",
    "\n",
    "# print('nan trials:', len(nan_trial_ids_cond), 'trials')\n",
    "# print('nan_cond_dict length:', len(nan_cond_dict))\n",
    "# print('nan_bump_cond_dict length:', len(nan_bump_cond_dict))\n",
    "# nan_n_trials = len(nan_trial_ids_cond)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # trial_mask = active_mask\n",
    "# n_trials = dataset_10ms.trial_info.shape[0]\n",
    "# print(n_trials,'total trials')\n",
    "# n_neurons = dataset_10ms.data.spikes.shape[1]\n",
    "# print(n_neurons,'neurons')\n",
    "\n",
    "\n",
    "# #make dictionary for trial condition (reaching directions) for Stratified CV\n",
    "# dataset = dataset_10ms\n",
    "# active_mask = (dataset.trial_info.ctr_hold_bump==0) & (dataset.trial_info['split'] != 'none')\n",
    "# passive_mask = (dataset.trial_info.ctr_hold_bump==1) & (dataset.trial_info['split'] != 'none')\n",
    "# nan_mask = (np.isnan(dataset.trial_info.ctr_hold_bump)) & (dataset.trial_info['split'] != 'none')\n",
    "# all_mask = (dataset.trial_info['split'] != 'none')\n",
    "\n",
    "# trial_mask = all_mask\n",
    "# valid_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "# print(valid_n_trials,'valid trials')\n",
    "\n",
    "# trial_mask = active_mask\n",
    "# active_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "# active_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "# print(active_n_trials,'active trials')\n",
    "\n",
    "# trial_mask = passive_mask\n",
    "# passive_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "# passive_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "# print(passive_n_trials,'passive trials')\n",
    "\n",
    "# trial_mask = nan_mask\n",
    "# nan_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "# nan_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "# print(nan_n_trials,'reach bump trials')\n",
    "\n",
    "# active_cond_dir_idx = []\n",
    "# passive_cond_dir_idx = []\n",
    "# nan_cond_dir_idx = []\n",
    "# nan_bump_cond_dir_idx = []\n",
    "# for direction in [0,45,90,135,180,225,270,315]:\n",
    "# # for direction in [0,90,180,270]:\n",
    "#     active_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & (dataset.trial_info['ctr_hold_bump'] == 0) & \\\n",
    "#            (dataset.trial_info['split'] != 'none'))[0])\n",
    "#     passive_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & (dataset.trial_info['ctr_hold_bump'] == 1) & \\\n",
    "#            (dataset.trial_info['split'] != 'none'))[0])\n",
    "#     nan_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & (np.isnan(dataset.trial_info.ctr_hold_bump)) & \\\n",
    "#            (dataset.trial_info['split'] != 'none'))[0])\n",
    "#     nan_bump_cond_dir_idx.append(np.where((dataset.trial_info['bump_dir']%360 == direction) & (np.isnan(dataset.trial_info.ctr_hold_bump)) & \\\n",
    "#            (dataset.trial_info['split'] != 'none'))[0])\n",
    "\n",
    "# active_cond_dict = nans([active_n_trials])\n",
    "# i = 0\n",
    "# for idx in active_trials_idx:\n",
    "#     for cond in range(0,len(active_cond_dir_idx)):\n",
    "#         if idx in active_cond_dir_idx[cond]:\n",
    "#             active_cond_dict[i] = cond\n",
    "#             break\n",
    "#     i+=1\n",
    "# print(active_cond_dict)\n",
    "# print(len(active_cond_dict))\n",
    "\n",
    "# passive_cond_dict = nans([passive_n_trials])\n",
    "# i = 0\n",
    "# for idx in passive_trials_idx:\n",
    "#     for cond in range(0,len(passive_cond_dir_idx)):\n",
    "#         if idx in passive_cond_dir_idx[cond]:\n",
    "#             passive_cond_dict[i] = cond\n",
    "#             break\n",
    "#     i+=1\n",
    "# print(passive_cond_dict)\n",
    "# print(len(passive_cond_dict))\n",
    "\n",
    "# nan_cond_dict = nans([nan_n_trials])\n",
    "# i = 0\n",
    "# for idx in nan_trials_idx:\n",
    "#     for cond in range(0,len(nan_cond_dir_idx)):\n",
    "#         if idx in nan_cond_dir_idx[cond]:\n",
    "#             nan_cond_dict[i] = cond\n",
    "#             break\n",
    "#     i+=1\n",
    "# print(nan_cond_dict)\n",
    "# print(len(nan_cond_dict))\n",
    "\n",
    "# nan_bump_cond_dict = nans([nan_n_trials])\n",
    "# i = 0\n",
    "# for idx in nan_trials_idx:\n",
    "#     for cond in range(0,len(nan_bump_cond_dir_idx)):\n",
    "#         if idx in nan_bump_cond_dir_idx[cond]:\n",
    "#             nan_bump_cond_dict[i] = cond\n",
    "#             break\n",
    "#     i+=1\n",
    "# print(nan_bump_cond_dict)\n",
    "# print(len(nan_bump_cond_dict))\n",
    "\n",
    "# if monkey == 'Duncan_20190710':\n",
    "#     active_df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range = (-100,100), ignored_trials = ~active_mask)\n",
    "#     del_indices = list(set(active_trials_idx) - set(active_df['trial_id'].unique()))\n",
    "#     print('was',active_n_trials,'active trials')\n",
    "#     active_n_trials = active_n_trials - len(list(set(active_trials_idx) - set(active_df['trial_id'].unique())))\n",
    "#     active_cond_dict_onset = np.delete(active_cond_dict,np.where(np.isin(active_trials_idx, del_indices)))\n",
    "#     print('now',active_n_trials,'active trials')\n",
    "#     print(len(active_cond_dict_onset))\n",
    "\n",
    "#     passive_df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range = (-100,100), ignored_trials = ~passive_mask)\n",
    "#     del_indices = list(set(passive_trials_idx) - set(passive_df['trial_id'].unique()))\n",
    "#     print('was',passive_n_trials,'passive trials')\n",
    "#     passive_n_trials = passive_n_trials - len(list(set(passive_trials_idx) - set(passive_df['trial_id'].unique())))\n",
    "#     passive_cond_dict = np.delete(passive_cond_dict,np.where(np.isin(passive_trials_idx, del_indices)))\n",
    "#     print('now',passive_n_trials,'passive trials')\n",
    "#     print(len(passive_cond_dict))\n",
    "\n",
    "#     # nan_df = dataset.make_trial_data(align_field='move_onset_time', align_range = (-100,100), ignored_trials = ~nan_mask)\n",
    "#     # del_indices = list(set(nan_trials_idx) - set(nan_df['trial_id'].unique()))\n",
    "#     # print('was',nan_n_trials,'nan trials')\n",
    "#     # nan_n_trials = nan_n_trials - len(list(set(nan_trials_idx) - set(nan_df['trial_id'].unique())))\n",
    "#     # nan_cond_dict = np.delete(nan_cond_dict,np.where(np.isin(nan_trials_idx, del_indices)))\n",
    "#     # nan_bump_cond_dict = np.delete(nan_bump_cond_dict,np.where(np.isin(nan_trials_idx, del_indices)))\n",
    "#     # print('now',nan_n_trials,'nan trials')\n",
    "#     # print(len(nan_bump_cond_dict))\n",
    "\n",
    "# active_df = dataset_10ms.make_trial_data(align_field='move_offset_time', align_range = (-100,0), ignored_trials = ~active_mask)\n",
    "# del_indices = list(set(active_trials_idx) - set(active_df['trial_id'].unique()))\n",
    "# print('was',active_n_trials,'active trials')\n",
    "# active_cond_dict_offset = np.delete(active_cond_dict, np.where(np.isin(active_trials_idx, del_indices))[0])\n",
    "# print('now')\n",
    "# print(len(active_cond_dict_offset))\n",
    "# if monkey == 'Duncan_20190710':\n",
    "#     active_cond_dict = active_cond_dict_onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sca.models import SCA\n",
    "# align_range = (-100, 1000)\n",
    "# active_trial_data = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=align_range, ignored_trials=~active_mask)\n",
    "# active_trial_spsm = np.array(active_trial_data.spikes_smth_40)\n",
    "# target_n_trials = active_trial_data['trial_id'].nunique()\n",
    "# n_timepoints = int((align_range[1]-align_range[0])/bin_width)\n",
    "# active_sample_weights= np.ones((target_n_trials, n_timepoints))\n",
    "# # active_sample_weights[:,:int(100/dataset_10ms.bin_width)] = 10\n",
    "# active_sample_weights = active_sample_weights.flatten()\n",
    "# print(active_sample_weights.shape)\n",
    "\n",
    "# align_range = (-100, 500)\n",
    "# passive_trial_data = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=align_range, ignored_trials=~passive_mask)\n",
    "# passive_trial_spsm = np.array(passive_trial_data.spikes_smth_40)\n",
    "# target_n_trials = passive_trial_data['trial_id'].nunique()\n",
    "# n_timepoints = int((align_range[1]-align_range[0])/bin_width)\n",
    "# passive_sample_weights= np.ones((target_n_trials, n_timepoints))\n",
    "# passive_sample_weights = passive_sample_weights.flatten()\n",
    "# print(passive_sample_weights.shape)\n",
    "\n",
    "# sample_weights = np.hstack((active_sample_weights, passive_sample_weights))\n",
    "# print(sample_weights.shape)\n",
    "# all_trial_spsm = np.concatenate((active_trial_spsm, passive_trial_spsm),axis=0)\n",
    "# print(all_trial_spsm.shape)\n",
    "\n",
    "# all_data = np.array(dataset_10ms.data.spikes_smth_40)\n",
    "# print(all_data.shape)\n",
    "# if not np.isnan(all_trial_spsm).any():\n",
    "#     scaler = StandardScaler()\n",
    "#     X_trial = scaler.fit_transform(all_trial_spsm,sample_weight=sample_weights)\n",
    "#     sca = SCA(n_components=n_dims)\n",
    "#     sca.fit(X_trial) # scaler and sca fit to trial data\n",
    "#     X_all = scaler.transform(all_data) #scaler and sca transform all data\n",
    "#     SCA_data = sca.transform(X_all)\n",
    "# print(SCA_data.shape)\n",
    "# dataset_10ms.add_continuous_data(SCA_data,'SCA_40')\n",
    "# print('SCA_40 var explained:',sca.r2_score)\n",
    "\n",
    "# ssa_order_smth40 = np.argsort(-np.array(sca.explained_squared_activity))\n",
    "# print('SCA_40 activity explained:',sca.explained_squared_activity[ssa_order_smth40])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_range = [-100, 1000]\n",
    "lag_range = [-300, 300]\n",
    "kin_range = [trial_range[0] + lag_range[0] + (-200), trial_range[1] + lag_range[1] + (+200)]                                \n",
    "lag_axis = np.arange(lag_range[0], lag_range[1]+1, 10)\n",
    "nrn_axis = np.arange(trial_range[0]+lag_range[0], trial_range[1]+lag_range[1]+1, 10)\n",
    "# To predict trial_range, we need wider neural_range, which requires wider kin_range\n",
    "\n",
    "df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=kin_range, ignored_trials=~active_mask, allow_overlap=True)\n",
    "n_trials = df['trial_id'].nunique()\n",
    "# acc_array = df['hand_acc'].to_numpy().reshape(active_n_trials, -1, 2)\n",
    "vel_array = df['hand_vel'].to_numpy().reshape(active_n_trials, -1, 2)\n",
    "\n",
    "kin_axis = np.arange(kin_range[0], kin_range[1]+1, dataset_10ms.bin_width)\n",
    "print('neural axis',nrn_axis[0], nrn_axis[-1])\n",
    "print('kinematics axis',kin_axis[0], kin_axis[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 0,1\n",
    "n_nrn = 100\n",
    "\n",
    "def generate_orth_weights():\n",
    "    weights_raw = np.random.normal(mu, sigma, size=(n_nrn, 4))\n",
    "    weights_orth = orth(weights_raw)\n",
    "    return weights_orth[:, 0:2], weights_orth[:, 2:4]  # CD weights, FB weights\n",
    "def rotate_vector(v, angle_rad):\n",
    "    \"\"\"Rotate vector v by angle_rad in a random orthogonal direction.\"\"\"\n",
    "    # Generate a random vector\n",
    "    rand_vec = np.random.randn(*v.shape)\n",
    "    # Make it orthogonal to v\n",
    "    rand_vec -= (np.dot(rand_vec, v) / np.dot(v, v)) * v\n",
    "    rand_vec /= np.linalg.norm(rand_vec)  # normalize\n",
    "    \n",
    "    # Construct rotated vector in plane (v, rand_vec)\n",
    "    v_norm = v / np.linalg.norm(v)\n",
    "    rotated = np.cos(angle_rad) * v_norm + np.sin(angle_rad) * rand_vec\n",
    "    return rotated * np.linalg.norm(v)  # scale back to original magnitude\n",
    "\n",
    "nrn_weight_cd_save, nrn_weight_fb_save = generate_orth_weights()\n",
    "nrn_weights_save = np.stack([nrn_weight_cd_save, nrn_weight_fb_save], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "n_runs = 1\n",
    "\n",
    "cd_lag = -100\n",
    "fb_lag = 50\n",
    "noise_level = 5\n",
    "noise_lv = \"hi\"\n",
    "type = \"approx_oppo\"\n",
    "\n",
    "for b in np.arange(0,n_runs):\n",
    "    if b == 0:\n",
    "        nrn_weight_cd = nrn_weight_cd_save\n",
    "        nrn_weight_fb = nrn_weight_fb_save\n",
    "    else:\n",
    "        nrn_weight_cd, nrn_weight_fb = generate_orth_weights()\n",
    "    if type == \"align\":\n",
    "        nrn_weight_fb = nrn_weight_cd.copy()\n",
    "    elif type == \"oppo\":\n",
    "        nrn_weight_fb = -nrn_weight_cd\n",
    "    elif type == \"approx_align\":\n",
    "        angle = np.deg2rad(10)  # 10 degrees\n",
    "        nrn_weight_fb = np.array([rotate_vector(v, angle) for v in nrn_weight_cd])\n",
    "    elif type == \"approx_oppo\":\n",
    "        angle = np.deg2rad(180 - 10)  # 170 degrees\n",
    "        nrn_weight_fb = np.array([rotate_vector(v, angle) for v in nrn_weight_cd])\n",
    "\n",
    "    n_bins = len(nrn_axis)\n",
    "    nrn_activity = nans([n_trials, n_bins, n_nrn])\n",
    "    cd_nrn_activity = nans([n_trials, n_bins, n_nrn])\n",
    "    fb_nrn_activity = nans([n_trials, n_bins, n_nrn])\n",
    "\n",
    "    # Precompute index offsets once\n",
    "    cd_start = np.argwhere(kin_axis == nrn_axis[0] + (-cd_lag))[0, 0]\n",
    "    fb_start = np.argwhere(kin_axis == nrn_axis[0] + (-fb_lag))[0, 0]\n",
    "\n",
    "    cd_idx = np.arange(n_bins) + cd_start\n",
    "    fb_idx = np.arange(n_bins) + fb_start\n",
    "\n",
    "    # Slice velocity arrays in one go\n",
    "    vel_cd = vel_array[:, cd_idx, :]   # shape (n_trials, n_bins, 2)\n",
    "    vel_fb = vel_array[:, fb_idx, :]   # shape (n_trials, n_bins, 2)\n",
    "\n",
    "    # Project onto neuron weights using matrix multiply (batch matmul)\n",
    "    cd_nrn_activity = vel_cd @ nrn_weight_cd.T  # (n_trials, n_bins, n_nrn)\n",
    "    fb_nrn_activity = vel_fb @ nrn_weight_fb.T  # (n_trials, n_bins, n_nrn)\n",
    "\n",
    "    # Combine\n",
    "    nrn_activity = cd_nrn_activity + fb_nrn_activity\n",
    "\n",
    "    # Add Gaussian noise\n",
    "    nrn_activity_flat = nrn_activity.reshape(-1,n_nrn)\n",
    "    sig_noise = np.zeros((n_nrn,n_nrn))\n",
    "    np.fill_diagonal(sig_noise,1)\n",
    "    noise = np.random.multivariate_normal(np.zeros(n_nrn), sig_noise, nrn_activity_flat.shape[0]) * noise_level\n",
    "    noisy_nrn_activity = (nrn_activity_flat+noise).reshape(nrn_activity.shape)\n",
    "\n",
    "    noise = np.random.multivariate_normal(np.zeros(n_nrn), sig_noise, nrn_activity_flat.shape[0]) * noise_level\n",
    "    cd_nrn_activity_flat = cd_nrn_activity.reshape(-1,n_nrn)\n",
    "    noisy_cd_nrn_activity = (cd_nrn_activity_flat+noise).reshape(cd_nrn_activity.shape)\n",
    "\n",
    "    noise = np.random.multivariate_normal(np.zeros(n_nrn), sig_noise, nrn_activity_flat.shape[0]) * noise_level\n",
    "    fb_nrn_activity_flat = fb_nrn_activity.reshape(-1,n_nrn)\n",
    "    noisy_fb_nrn_activity = (fb_nrn_activity_flat+noise).reshape(fb_nrn_activity.shape)    \n",
    "\n",
    "    # noisy_nrn_activity = noisy_cd_nrn_activity + noisy_fb_nrn_activity\n",
    "    n_splits = 20\n",
    "    r2_array = nans([len(lag_axis), n_splits]); \n",
    "    cd_r2 = nans([len(lag_axis), n_splits]); fb_r2 = nans([len(lag_axis), n_splits])\n",
    "    y = vel_array[:,np.argwhere(kin_axis==trial_range[0])[0,0]:np.argwhere(kin_axis==trial_range[1])[0,0],:]\n",
    "    # y_reshaped = y.reshape(-1,2)\n",
    "    for j in range(len(lag_axis)):\n",
    "        start, end = j, y.shape[1] + j\n",
    "        def run_decoding(activity):\n",
    "            r2_arr = nans([n_splits])\n",
    "            X = activity[:, start:end, :]\n",
    "            sss = ShuffleSplit(n_splits=n_splits,random_state = 42)\n",
    "            for k,(training_set, test_set) in enumerate(sss.split(range(0,n_trials))):\n",
    "                X_train = X[training_set,:,:].reshape(-1,n_nrn)\n",
    "                X_test = X[test_set,:,:].reshape(-1,n_nrn)\n",
    "                y_train = y[training_set,:,:].reshape(-1,2)\n",
    "                y_test = y[test_set,:,:].reshape(-1,2)\n",
    "                r2_arr[k] =LinearRegression().fit(X_train, y_train).score(X_test, y_test)\n",
    "            return r2_arr\n",
    "        cd_r2[j,:] = run_decoding(noisy_cd_nrn_activity)\n",
    "        fb_r2[j,:] = run_decoding(noisy_fb_nrn_activity)\n",
    "        r2_array[j,:] = run_decoding(noisy_nrn_activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(str(monkey)+\"_sim_\"+str(type),\\\n",
    "         noisy_cd_nrn_activity = noisy_cd_nrn_activity, noisy_fb_nrn_activity = noisy_fb_nrn_activity, \\\n",
    "         noisy_nrn_activity = noisy_nrn_activity,\\\n",
    "         cd_r2 = cd_r2,fb_r2 = fb_r2, \\\n",
    "         r2_array = r2_array, nrn_weights_save=nrn_weights_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "tr_idx = 164\n",
    "nrn_idx = 10\n",
    "\n",
    "plt.plot(nrn_axis,cd_nrn_activity[tr_idx,:,nrn_idx],color = 'green',label = 'cd signal')\n",
    "plt.plot(nrn_axis,fb_nrn_activity[tr_idx,:,nrn_idx],color = 'magenta',label = 'fb signal')\n",
    "plt.plot(nrn_axis,nrn_activity[tr_idx,:,nrn_idx],color = 'brown',label = 'sum signal')\n",
    "plt.plot(nrn_axis,noisy_nrn_activity[tr_idx,:,nrn_idx],color = 'gray',alpha=0.8,label = 'noisy signal')\n",
    "# plt.legend(fontsize=8)\n",
    "plt.xlim([-200, 1200])\n",
    "plt.axvline(0,color = 'k', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey+\"_\"+type+\"_example_nrn_\"+noise_lv+\"_noise.pdf\",dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5.5,4))\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "# Compute means and standard deviations\n",
    "mean_r2 = np.mean(r2_array, axis=1)\n",
    "std_r2 = np.std(r2_array, axis=1)\n",
    "\n",
    "mean_cd = np.mean(cd_r2, axis=1)\n",
    "std_cd = np.std(cd_r2, axis=1)\n",
    "\n",
    "mean_fb = np.mean(fb_r2, axis=1)\n",
    "std_fb = np.std(fb_r2, axis=1)\n",
    "\n",
    "# Plot means\n",
    "plt.plot(lag_axis, mean_r2, color='brown')\n",
    "plt.plot(lag_axis, mean_cd, color='green', linestyle='--',alpha=0.5)\n",
    "plt.plot(lag_axis, mean_fb, color='magenta',linestyle='--',alpha=0.5)\n",
    "\n",
    "# Fill between mean ± std\n",
    "plt.fill_between(lag_axis, mean_r2 - std_r2, mean_r2 + std_r2, color='brown', alpha=0.3)\n",
    "plt.fill_between(lag_axis, mean_cd - std_cd, mean_cd + std_cd, color='green', alpha=0.3)\n",
    "plt.fill_between(lag_axis, mean_fb - std_fb, mean_fb + std_fb, color='magenta', alpha=0.3)\n",
    "\n",
    "plt.axvline(cd_lag, color = 'k', linestyle='--')\n",
    "plt.axvline(fb_lag, color = 'k', linestyle='--')\n",
    "plt.xlabel(\"Time lag (ms)\")\n",
    "plt.ylabel('R²')\n",
    "plt.ylim([-0.1, 1.0])\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + \"_\"+type+\"_sim_r2.pdf\", dpi = 'figure')\n",
    "plt.show()\n",
    "# plt.hist(ang_runs)\n",
    "# plt.xlabel(\"Angle (deg)\")\n",
    "# plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_range = [-100, 1000]\n",
    "# trial_range = [-100, 1100]\n",
    "\n",
    "lag_range = [-300, 300]\n",
    "kin_range = [trial_range[0] + lag_range[0] + (-200), trial_range[1] + lag_range[1] + (+500)]                                \n",
    "lag_axis = np.arange(lag_range[0], lag_range[1]+1, 10)\n",
    "nrn_axis = np.arange(trial_range[0]+lag_range[0], trial_range[1]+lag_range[1]+1, 10)\n",
    "# To predict trial_range, we need wider neural_range, which requires wider kin_range\n",
    "\n",
    "df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=kin_range, ignored_trials=~active_mask, allow_overlap=True)\n",
    "n_trials = df['trial_id'].nunique()\n",
    "vel_array = df['hand_vel'].to_numpy().reshape(active_n_trials, -1, 2)\n",
    "\n",
    "kin_axis = np.arange(kin_range[0], kin_range[1], dataset_10ms.bin_width)\n",
    "print('neural axis',nrn_axis[0], nrn_axis[-1])\n",
    "print('kinematics axis',kin_axis[0], kin_axis[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_dir_dict = (\n",
    "    dataset_10ms.trial_info.cond_dir\n",
    ").to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "perturb_time_ms = 325\n",
    "perturb_width = 40\n",
    "bump_strength = 20  # fixed magnitude\n",
    "relative_bump_angles = np.array([0, 90, 180, 270])\n",
    "relative_bump_angles_per_trial = np.zeros(n_trials)\n",
    "# absolute_bump_angles = np.zeros(n_trials)\n",
    "condition_angles = np.zeros(n_trials)\n",
    "\n",
    "\n",
    "# Time setup\n",
    "perturb_idx = np.argmin(np.abs(kin_axis - perturb_time_ms))\n",
    "gaussian_bump = np.exp(-0.5 * ((kin_axis - perturb_time_ms) / perturb_width) ** 2)  # shape: [time]\n",
    "\n",
    "# Output arrays\n",
    "perturbed_vel_array = vel_array.copy()\n",
    "bump_vel_array = np.full_like(vel_array, np.nan)\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    # Get base direction in degrees\n",
    "    base_dir_deg = cond_dir_dict[df['trial_id'].unique()[trial]]  # e.g., 0–315\n",
    "\n",
    "    # Pick relative bump angle randomly\n",
    "    rel_bump_angle = np.random.choice(relative_bump_angles)\n",
    "    bump_dir_deg = (base_dir_deg + rel_bump_angle) % 360\n",
    "    \n",
    "    condition_angles[trial] = base_dir_deg%360\n",
    "    relative_bump_angles_per_trial[trial] = rel_bump_angle\n",
    "    # absolute_bump_angles[trial] = bump_dir_deg\n",
    "\n",
    "    # Convert bump direction to unit vector\n",
    "    bump_dir_rad = np.deg2rad(bump_dir_deg)\n",
    "    bump_vector = np.array([np.cos(bump_dir_rad), np.sin(bump_dir_rad)])  # shape: (2,)\n",
    "\n",
    "    # Create bump signal\n",
    "    bump = gaussian_bump[:, None] * bump_vector[None, :] * bump_strength  # shape: [time, 2]\n",
    "\n",
    "    # Store\n",
    "    bump_vel_array[trial, :, :] = bump\n",
    "    perturbed_vel_array[trial, :, :] += bump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(perturbed_vel_array).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,2))\n",
    "plt.plot(kin_axis,bump)\n",
    "plt.axvline([250])\n",
    "plt.xlim([0, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dir =  np.array([0.0, 45.0, 90.0, 135.0, 180.0, 225.0, 270.0, 315.0])\n",
    "plot_dir = [0.0, 90.0, 180.0, 270.0] \n",
    "dir_idx = [np.argwhere(all_dir==dir)[0,0] for dir in plot_dir]\n",
    "\n",
    "plot_dim = 0 # plot x velocity\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "fig, ax = plt.subplots(figsize=(10,2))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "for i in range(len(plot_dir)):\n",
    "    indices = np.array([np.argwhere(active_cond_dict==dir_idx[i])]).flatten()\n",
    "    plt.plot(kin_axis,vel_array[indices,:,plot_dim].T, color = colors[i],linewidth=0.5)\n",
    "plt.axvline([0],color='k',linestyle='--')\n",
    "plt.axvline([-100],color='k')\n",
    "plt.axvline([500],color='k')\n",
    "plt.ylim([-50, 50])\n",
    "plt.xlabel('Time after move onset (ms)')\n",
    "plt.ylabel('Hand velocity (cm/s)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_dim = 0 # plot x velocity\n",
    "all_dirs = np.array([0.0, 45.0, 90.0, 135.0, 180.0, 225.0, 270.0, 315.0])\n",
    "plot_dirs = np.array([0.0, 45.0, 90.0, 135.0, 180.0, 225.0, 270.0, 315.0])\n",
    "dir_idx = [np.argwhere(all_dirs==dir)[0,0] for dir in plot_dirs]\n",
    "cmap = plt.get_cmap('coolwarm',len(plot_dirs))\n",
    "custom_palette = [mpl.colors.rgb2hex(cmap(i)) for i in range(len(all_dirs))]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,3))\n",
    "# fig, ax = plt.subplots(figsize=(5,3))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "for i in range(len(plot_dirs)):\n",
    "    indices = np.array([np.argwhere(active_cond_dict==dir_idx[i])]).flatten()\n",
    "    plt.plot(kin_axis,perturbed_vel_array[indices,:,plot_dim].T, color = custom_palette[i],linewidth=0.1)\n",
    "    # plt.plot(kin_axis,vel_array[indices,:,plot_dim].T, color = custom_palette[i],linewidth=0.1)\n",
    "plt.axvline([0],color='k',linestyle='--')\n",
    "plt.xlim([-450, 1600])\n",
    "# plt.xlim([-350, 1700])\n",
    "# plt.xlim([0, 500])\n",
    "plt.ylim([-55, 55])\n",
    "# plt.axvline([150],color='k')\n",
    "plt.axvline([245],color='k',linestyle='--')\n",
    "# plt.axvline([350],color='k')\n",
    "# plt.axvline([450],color='k')\n",
    "# plt.axvline([550],color='k')\n",
    "# plt.axvline([650],color='k')\n",
    "plt.xlabel('Time after move onset (ms)')\n",
    "plt.ylabel('Velocity (cm/s)')\n",
    "# plt.title('perturbation added')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_activebump_simulation_vel.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_dim = 0 # plot x velocity\n",
    "all_dirs = np.array([0.0, 45.0, 90.0, 135.0, 180.0, 225.0, 270.0, 315.0])\n",
    "plot_dirs = np.array([0.0, 45.0, 90.0, 135.0, 180.0, 225.0, 270.0, 315.0])\n",
    "dir_idx = [np.argwhere(all_dirs==dir)[0,0] for dir in plot_dirs]\n",
    "cmap = plt.get_cmap('coolwarm',len(plot_dirs))\n",
    "custom_palette = [mpl.colors.rgb2hex(cmap(i)) for i in range(len(all_dirs))]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "for i in range(len(plot_dirs)):\n",
    "    indices = np.array([np.argwhere(active_cond_dict==dir_idx[i])]).flatten()\n",
    "    plt.plot(kin_axis,bump_vel_array[indices,:,plot_dim].T, color = custom_palette[i],linewidth=0.1)\n",
    "plt.axvline([0],color='k',linestyle='--')\n",
    "plt.xlim([0, 450])\n",
    "plt.ylim([-55, 55])\n",
    "# plt.axvline([-100],color='k')\n",
    "# plt.axvline([500],color='k')\n",
    "plt.xlabel('Time after move onset (ms)')\n",
    "plt.ylabel('Velocity (cm/s)')\n",
    "# plt.title('perturbation added')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_activebump_simulation_vel.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_dim = 0 # plot x velocity\n",
    "all_dirs = np.array([0.0, 45.0, 90.0, 135.0, 180.0, 225.0, 270.0, 315.0])\n",
    "plot_dirs = np.array([0.0, 45.0, 90.0, 135.0, 180.0, 225.0, 270.0, 315.0])\n",
    "dir_idx = [np.argwhere(all_dirs==dir)[0,0] for dir in plot_dirs]\n",
    "cmap = plt.get_cmap('coolwarm',len(plot_dirs))\n",
    "custom_palette = [mpl.colors.rgb2hex(cmap(i)) for i in range(len(all_dirs))]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "for i in range(len(plot_dirs)):\n",
    "    indices = np.array([np.argwhere(active_cond_dict==dir_idx[i])]).flatten()\n",
    "    plt.plot(kin_axis,vel_array[indices,:,plot_dim].T, color = custom_palette[i],linewidth=0.1)\n",
    "plt.axvline([0],color='k',linestyle='--')\n",
    "plt.xlim([0, 700])\n",
    "plt.ylim([-55, 55])\n",
    "plt.axvline([150],color='k')\n",
    "plt.axvline([250],color='k')\n",
    "plt.axvline([350],color='k')\n",
    "plt.axvline([450],color='k')\n",
    "plt.axvline([550],color='k')\n",
    "plt.axvline([650],color='k')\n",
    "\n",
    "plt.xlabel('Time after move onset (ms)')\n",
    "plt.ylabel('Velocity (cm/s)')\n",
    "# plt.title('perturbation added')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_activebump_simulation_vel.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 0,1\n",
    "n_nrn = 100\n",
    "\n",
    "def generate_orth_weights():\n",
    "    weights_raw = np.random.normal(mu, sigma, size=(n_nrn, 4))\n",
    "    weights_orth = orth(weights_raw)\n",
    "    return weights_orth[:, 0:2], weights_orth[:, 2:4]  # CD weights, FB weights\n",
    "def rotate_vector(v, angle_rad):\n",
    "    \"\"\"Rotate vector v by angle_rad in a random orthogonal direction.\"\"\"\n",
    "    # Generate a random vector\n",
    "    rand_vec = np.random.randn(*v.shape)\n",
    "    # Make it orthogonal to v\n",
    "    rand_vec -= (np.dot(rand_vec, v) / np.dot(v, v)) * v\n",
    "    rand_vec /= np.linalg.norm(rand_vec)  # normalize\n",
    "    \n",
    "    # Construct rotated vector in plane (v, rand_vec)\n",
    "    v_norm = v / np.linalg.norm(v)\n",
    "    rotated = np.cos(angle_rad) * v_norm + np.sin(angle_rad) * rand_vec\n",
    "    return rotated * np.linalg.norm(v)  # scale back to original magnitude\n",
    "\n",
    "nrn_weight_cd_save, nrn_weight_fb_save = generate_orth_weights()\n",
    "nrn_weights_save = np.stack([nrn_weight_cd_save, nrn_weight_fb_save], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = np.deg2rad(10)  \n",
    "nrn_weight_fb = np.array([rotate_vector(v, angle) for v in nrn_weight_cd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nrn_weight_fb)\n",
    "plt.plot(nrn_weight_cd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "n_runs = 1\n",
    "\n",
    "# vel_r2_runs = nans([n_runs, len(lag_axis)])\n",
    "# vel_cd_r2_arr = nans([n_runs, len(lag_axis)])\n",
    "# vel_fb_r2_arr = nans([n_runs, len(lag_axis)])\n",
    "\n",
    "# bump_r2_runs = nans([n_runs, len(lag_axis)])\n",
    "# bump_cd_r2_arr = nans([n_runs, len(lag_axis)])\n",
    "# bump_fb_r2_arr = nans([n_runs, len(lag_axis)])\n",
    "\n",
    "bump_acc_runs = []\n",
    "bump_cd_acc_arr = []\n",
    "bump_fb_acc_arr = []\n",
    "\n",
    "cd_lag = -100\n",
    "fb_lag = 0\n",
    "noise_level = 5\n",
    "noise_lv = \"hi\"\n",
    "type = \"orthog\"\n",
    "\n",
    "for b in np.arange(0,n_runs):\n",
    "    if b == 0:\n",
    "        nrn_weight_cd = nrn_weight_cd_save\n",
    "        nrn_weight_fb = nrn_weight_fb_save\n",
    "    else:\n",
    "        nrn_weight_cd, nrn_weight_fb = generate_orth_weights()\n",
    "    if type == \"align\":\n",
    "        nrn_weight_fb = nrn_weight_cd.copy()\n",
    "    elif type == \"oppo\":\n",
    "        nrn_weight_fb = -nrn_weight_cd\n",
    "    elif type == \"approx_align\":\n",
    "        angle = np.deg2rad(45)  \n",
    "        nrn_weight_fb = np.array([rotate_vector(v, angle) for v in nrn_weight_cd])\n",
    "    elif type == \"approx_oppo\":\n",
    "        angle = np.deg2rad(180 - 45)  \n",
    "        nrn_weight_fb = np.array([rotate_vector(v, angle) for v in nrn_weight_cd])\n",
    "\n",
    "    n_bins = len(nrn_axis)\n",
    "    nrn_activity = nans([n_trials, n_bins, n_nrn])\n",
    "    cd_nrn_activity = nans([n_trials, n_bins, n_nrn])\n",
    "    fb_nrn_activity = nans([n_trials, n_bins, n_nrn])\n",
    "\n",
    "    for i in range(n_bins):\n",
    "        cd_idx = i + np.argwhere(kin_axis == nrn_axis[0] + (-cd_lag))[0, 0]\n",
    "        fb_idx = i + np.argwhere(kin_axis == nrn_axis[0] + (-fb_lag))[0, 0]\n",
    "        vel_cd = vel_array[:, cd_idx, :]  # shape: (n_trials, 2)\n",
    "        vel_fb = perturbed_vel_array[:, fb_idx, :]  # shape: (n_trials, 2)\n",
    "        cd_signal = vel_cd @ nrn_weight_cd.T  # (n_trials, n_nrn)\n",
    "        fb_signal = vel_fb @ nrn_weight_fb.T    \n",
    "        cd_nrn_activity[:, i, :] = cd_signal\n",
    "        fb_nrn_activity[:, i, :] = fb_signal\n",
    "        nrn_activity[:, i, :] = cd_signal + fb_signal\n",
    "\n",
    "    nrn_activity_flat = nrn_activity.reshape(-1,n_nrn)\n",
    "    sig_noise = np.zeros((n_nrn,n_nrn))\n",
    "    np.fill_diagonal(sig_noise,1)\n",
    "    noise = np.random.multivariate_normal(np.zeros(n_nrn), sig_noise, nrn_activity_flat.shape[0]) * noise_level\n",
    "    noisy_nrn_activity = (nrn_activity_flat+noise).reshape(nrn_activity.shape)\n",
    "\n",
    "    noise = np.random.multivariate_normal(np.zeros(n_nrn), sig_noise, nrn_activity_flat.shape[0]) * noise_level\n",
    "    cd_nrn_activity_flat = cd_nrn_activity.reshape(-1,n_nrn)\n",
    "    noisy_cd_nrn_activity = (cd_nrn_activity_flat+noise).reshape(cd_nrn_activity.shape)\n",
    "\n",
    "    noise = np.random.multivariate_normal(np.zeros(n_nrn), sig_noise, nrn_activity_flat.shape[0]) * noise_level\n",
    "    fb_nrn_activity_flat = fb_nrn_activity.reshape(-1,n_nrn)\n",
    "    noisy_fb_nrn_activity = (fb_nrn_activity_flat+noise).reshape(fb_nrn_activity.shape)    \n",
    "\n",
    "    # noisy_nrn_activity = noisy_cd_nrn_activity + noisy_fb_nrn_activity\n",
    "    # coefs_arr = nans([len(lag_axis), n_nrn])\n",
    "    pos_labels_all = [0, 45, 90, 135]\n",
    "    for pos_label in pos_labels_all:\n",
    "        neg_label = pos_label+180\n",
    "        reach_mask = np.isin(condition_angles%360,[pos_label, neg_label])\n",
    "        # --- Select trials: only assist vs against ---\n",
    "        bump_mask = np.isin(relative_bump_angles_per_trial, [0, 180])\n",
    "        \n",
    "        trial_mask = reach_mask & bump_mask\n",
    "        selected_trials = np.where(trial_mask)[0]\n",
    "        print(len(selected_trials), \"trials selected\")\n",
    "\n",
    "        # --- Build 4-class labels: 0=up assist, 1=up against, 2=down assist, 3=down against ---\n",
    "        labels = []\n",
    "        for tid in selected_trials:\n",
    "            cond = (condition_angles[tid]) % 360\n",
    "            # print()\n",
    "            # print(cond)\n",
    "            bump = relative_bump_angles_per_trial[tid] % 360\n",
    "            # print(bump)\n",
    "            if cond == pos_label:  # upward\n",
    "                label = 0 if bump == 0 else 1\n",
    "            elif cond == neg_label:  # downward\n",
    "                label = 2 if bump == 0 else 3\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected cond {cond} for trial {tid}\")\n",
    "            labels.append(label)\n",
    "        labels = np.array(labels)\n",
    "        # print(labels)\n",
    "\n",
    "        sss = StratifiedShuffleSplit(n_splits=100, test_size=0.5, random_state=42)\n",
    "\n",
    "        activity_range = [270,320]\n",
    "        r2_arr = nans([len(lag_axis)]); \n",
    "        cd_r2 = nans([len(lag_axis)]); fb_r2 = nans([len(lag_axis)])\n",
    "        nrn_start_idx = np.argwhere(nrn_axis == (activity_range[0] ))[0, 0]\n",
    "        nrn_end_idx   = np.argwhere(nrn_axis == (activity_range[1] ))[0, 0] + 1  # include endpoint\n",
    "        def get_X(activity):\n",
    "        # average across the window\n",
    "            return activity[:, nrn_start_idx:nrn_end_idx, :].mean(axis=1)  # shape: (n_trials, n_nrn)\n",
    "\n",
    "        X_cd   = get_X(noisy_cd_nrn_activity)[selected_trials]\n",
    "        X_fb   = get_X(noisy_fb_nrn_activity)[selected_trials]\n",
    "        X_full = get_X(noisy_nrn_activity)[selected_trials]\n",
    "\n",
    "        # --- Stratified CV with class balancing ---\n",
    "        sss = StratifiedShuffleSplit(n_splits=100, test_size=0.5, random_state=42)\n",
    "\n",
    "        def run_classification(X, labels):\n",
    "            acc_arr = np.zeros(sss.get_n_splits())\n",
    "            for i, (train_idx, test_idx) in enumerate(sss.split(X, labels)):\n",
    "                # Balance training set\n",
    "                train_labels = labels[train_idx]\n",
    "                min_count = min(Counter(train_labels).values())\n",
    "                balanced_train_idx = np.hstack([\n",
    "                    np.random.choice(np.where(train_labels == c)[0], min_count, replace=False)\n",
    "                    for c in np.unique(train_labels)\n",
    "                ])\n",
    "                np.random.shuffle(balanced_train_idx)\n",
    "                train_idx_bal = train_idx[balanced_train_idx]\n",
    "\n",
    "                # Balance test set\n",
    "                test_labels = labels[test_idx]\n",
    "                min_count_test = min(Counter(test_labels).values())\n",
    "                balanced_test_idx = np.hstack([\n",
    "                    np.random.choice(np.where(test_labels == c)[0], min_count_test, replace=False)\n",
    "                    for c in np.unique(test_labels)\n",
    "                ])\n",
    "                np.random.shuffle(balanced_test_idx)\n",
    "                test_idx_bal = test_idx[balanced_test_idx]\n",
    "\n",
    "                # Build X and y\n",
    "                # scaler = StandardScaler()\n",
    "                # X_train = scaler.fit_transform(X[train_idx_bal])\n",
    "                # X_test = scaler.transform(X[test_idx_bal])\n",
    "                X_train = X[train_idx_bal]\n",
    "                X_test =  X[test_idx_bal]\n",
    "                y_train = np.isin(labels[train_idx_bal], [1,2]).astype(int)\n",
    "                y_test = np.isin(labels[test_idx_bal], [1,2]).astype(int)\n",
    "\n",
    "                clf = LogisticRegression()\n",
    "                clf.fit(X_train, y_train)\n",
    "                acc_arr[i] = clf.score(X_test, y_test)\n",
    "            return acc_arr\n",
    "\n",
    "        bump_cd_acc_arr.append(run_classification(X_cd, labels))\n",
    "        bump_fb_acc_arr.append(run_classification(X_fb, labels))\n",
    "        bump_acc_runs.append(run_classification(X_full, labels))\n",
    "\n",
    "\n",
    "    # r2_arr = nans([len(lag_axis)]); \n",
    "    # cd_r2 = nans([len(lag_axis)]); fb_r2 = nans([len(lag_axis)])\n",
    "    # y = perturbed_vel_array[:,np.argwhere(kin_axis==trial_range[0])[0,0]:np.argwhere(kin_axis==trial_range[1])[0,0],:]    \n",
    "    # y_reshaped = y.reshape(-1,2)\n",
    "    # for j in range(len(lag_axis)):\n",
    "    #     start, end = j, y.shape[1] + j\n",
    "    #     def get_X(activity):\n",
    "    #         X = activity[:, start:end, :].reshape(-1, n_nrn)\n",
    "    #         return X\n",
    "    #     X_cd = get_X(noisy_cd_nrn_activity)\n",
    "    #     X_fb = get_X(noisy_fb_nrn_activity)\n",
    "    #     X_full = get_X(noisy_nrn_activity)\n",
    "\n",
    "    #     cd_r2[j] = LinearRegression().fit(X_cd, y_reshaped).score(X_cd, y_reshaped)\n",
    "    #     fb_r2[j] = LinearRegression().fit(X_fb, y_reshaped).score(X_fb, y_reshaped)\n",
    "\n",
    "    #     lr = LinearRegression().fit(X_full, y_reshaped)\n",
    "    #     r2_arr[j] = lr.score(X_full, y_reshaped)\n",
    "    # vel_r2_runs[b,:] = r2_arr; \n",
    "    # vel_cd_r2_arr[b,:] = cd_r2; vel_fb_r2_arr[b,:] = fb_r2\n",
    "\n",
    "    # bump_range = [250,350]\n",
    "    # bump_coefs_arr = nans([len(lag_axis),2,n_nrn])\n",
    "    # r2_arr = nans([len(lag_axis)]); \n",
    "    # cd_r2 = nans([len(lag_axis)]); fb_r2 = nans([len(lag_axis)])    \n",
    "    # nrn_bump_range = [bump_range[0]+lag_range[0]]\n",
    "    # nrn_add_idx = np.argwhere(nrn_axis==(bump_range[0]+lag_range[0]))[0,0]\n",
    "    # y = bump_vel_array[:,np.argwhere(kin_axis==bump_range[0])[0,0]:np.argwhere(kin_axis==bump_range[1])[0,0]+1,:]\n",
    "    # y_reshaped = y.reshape(-1,2)\n",
    "    # for j in range(len(lag_axis)):\n",
    "    #     start, end = nrn_add_idx+j, nrn_add_idx+y.shape[1] + j\n",
    "    #     def get_X(activity):\n",
    "    #         X = activity[:, start:end, :].reshape(-1, n_nrn)\n",
    "    #         return X\n",
    "    #     X_cd = get_X(noisy_cd_nrn_activity)\n",
    "    #     X_fb = get_X(noisy_fb_nrn_activity)\n",
    "    #     X_full = get_X(noisy_nrn_activity)\n",
    "\n",
    "    #     cd_r2[j] = LinearRegression().fit(X_cd, y_reshaped).score(X_cd, y_reshaped)\n",
    "    #     fb_r2[j] = LinearRegression().fit(X_fb, y_reshaped).score(X_fb, y_reshaped)\n",
    "\n",
    "    #     lr = LinearRegression().fit(X_full, y_reshaped)\n",
    "    #     r2_arr[j] = lr.score(X_full, y_reshaped)\n",
    "    #     bump_coefs_arr[j] = lr.coef_\n",
    "    # bump_r2_runs[b,:] = r2_arr; \n",
    "    # bump_cd_r2_arr[b,:] = cd_r2; bump_fb_r2_arr[b,:] = fb_r2\n",
    "\n",
    "# print(r2_runs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "# ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "colors = ['green', 'magenta', 'brown']  # CD_proj, FB_proj, CD_FB_proj\n",
    "mean_acc_list = [bump_cd_acc_arr,bump_fb_acc_arr,bump_acc_runs]\n",
    "x_fields = ['CD', 'FB', 'CD+FB']\n",
    "for i, x_field in enumerate(x_fields):\n",
    "    arr = np.array(mean_acc_list[i]).flatten()\n",
    "    plt.errorbar(x_field, np.mean(arr), yerr=np.std(arr), fmt='o',\n",
    "                 capsize=5, markersize=8, color=colors[i], label=x_field)\n",
    "\n",
    "plt.ylabel('Classification Accuracy',fontsize=15)\n",
    "# plt.xlabel('Variable', fontsize=10)\n",
    "plt.xticks()\n",
    "plt.yticks()\n",
    "plt.ylim(0.3,1.05)\n",
    "# plt.title('Up vs Down Bump Classification Accuracy', fontsize=11)\n",
    "# plt.legend(fontsize=9)\n",
    "# chance_level = max(np.sum(y == 0), np.sum(y == 1)) / len(y)\n",
    "chance_level = .5\n",
    "plt.axhline(chance_level, color='gray', linestyle='--', label='Chance level')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + \"_\"+type+\"_sim_bump_class.pdf\", dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(str(monkey)+\"_sim_perturb_\"+str(type)+\"_\"+str(noise_lv),\\\n",
    "         noisy_cd_nrn_activity = noisy_cd_nrn_activity, noisy_fb_nrn_activity = noisy_fb_nrn_activity, \\\n",
    "         noisy_nrn_activity = noisy_nrn_activity,\\\n",
    "         bump_coefs_arr = bump_coefs_arr,\\\n",
    "         vel_cd_r2_arr = vel_cd_r2_arr,vel_fb_r2_arr = vel_fb_r2_arr, \\\n",
    "         vel_r2_runs = vel_r2_runs, \\\n",
    "         bump_cd_r2_arr = bump_cd_r2_arr,bump_fb_r2_arr = bump_fb_r2_arr, \\\n",
    "         bump_r2_runs = bump_r2_runs, \\\n",
    "         nrn_weights_save=nrn_weights_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type = 'orthog'\n",
    "data = np.load(str(monkey)+\"_sim_perturb_\"+str(type)+\"_hi.npz\")\n",
    "print(data.files)\n",
    "\n",
    "vel_cd_r2_arr = data['vel_cd_r2_arr'] \n",
    "vel_fb_r2_arr = data['vel_fb_r2_arr'] \n",
    "vel_r2_runs = data['vel_r2_runs'] \n",
    "\n",
    "bump_cd_r2_arr = data['bump_cd_r2_arr'] \n",
    "bump_fb_r2_arr = data['bump_fb_r2_arr'] \n",
    "bump_r2_runs = data['bump_r2_runs'] \n",
    "\n",
    "bump_coefs_arr = data['bump_coefs_arr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_cd_nrn_activity[:, nrn_start:nrn_end, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bump_range = [250,350]\n",
    "nrn_add_idx = np.argwhere(nrn_axis==(bump_range[0]+lag_range[0]))[0,0]\n",
    "best_lag_idx = np.argmax(np.nanmean(bump_r2_runs, axis=0))\n",
    "best_model_coef = bump_coefs_arr[best_lag_idx,:,:]\n",
    "bump_start = np.argwhere(kin_axis==bump_range[0])[0,0]\n",
    "bump_end = np.argwhere(kin_axis==bump_range[1])[0,0]\n",
    "nrn_start = nrn_add_idx + best_lag_idx\n",
    "nrn_end = nrn_start + (bump_end - bump_start)\n",
    "\n",
    "X_cd = noisy_cd_nrn_activity[:, nrn_start:nrn_end, :].reshape(-1, n_nrn)\n",
    "X_fb = noisy_fb_nrn_activity[:, nrn_start:nrn_end, :].reshape(-1, n_nrn)\n",
    "X_full = X_cd + X_fb\n",
    "y = bump_vel_array[:, bump_start:bump_end,:]\n",
    "cd_contrib = X_cd @ best_model_coef.T\n",
    "fb_contrib = X_fb @ best_model_coef.T\n",
    "full_contrib = cd_contrib + fb_contrib\n",
    "trial_pred_summary = {\n",
    "    \"cd\": cd_contrib.reshape(y.shape),\n",
    "    \"fb\": fb_contrib.reshape(y.shape),\n",
    "    \"full\": full_contrib.reshape(y.shape),\n",
    "    \"true\": y,\n",
    "    \"rel_angle\": relative_bump_angles_per_trial.copy(),\n",
    "    \"abs_angle\": absolute_bump_angles.copy(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_angle = trial_pred_summary[\"rel_angle\"]\n",
    "abs_angle = trial_pred_summary[\"abs_angle\"]\n",
    "\n",
    "# Define conditions\n",
    "is_assist = rel_angle == 0\n",
    "is_against = rel_angle == 180\n",
    "is_disturb = np.isin(rel_angle, [90, 270])\n",
    "\n",
    "condition_masks = {\n",
    "    \"assist\": is_assist,\n",
    "    \"against\": is_against,\n",
    "    \"disturb\": is_disturb,\n",
    "}\n",
    "\n",
    "plot_angles = [0, 90, 180, 270]\n",
    "colors = {0: 'red', 90: 'blue', 180: 'green', 270: 'orange'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_by_condition(trial_pred_summary, condition_masks, plot_angles, signal_key, title_prefix):\n",
    "    signal = trial_pred_summary[signal_key]  # shape: [n_trials, time, 2]\n",
    "    abs_angle = trial_pred_summary[\"abs_angle\"]\n",
    "    time = np.arange(signal.shape[1])  # Time axis (in bins or ms)\n",
    "\n",
    "    for cond_name, cond_mask in condition_masks.items():\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        for ang in plot_angles:\n",
    "            sel = cond_mask & (abs_angle == ang)\n",
    "            if not np.any(sel): continue\n",
    "            avg = np.nanmean(signal[sel, :, 0], axis=0)  # [time, 2]\n",
    "            plt.plot(time, avg, label=f'{ang}°', color=colors[ang],linewidth=5)\n",
    "\n",
    "        plt.title(f\"{title_prefix} – {cond_name}\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Velocity\")\n",
    "        plt.ylim([-20, 20])\n",
    "        # plt.legend(title=\"Abs bump angle\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions_by_condition(trial_pred_summary, condition_masks, plot_angles, \"cd\", \"CD Prediction\")\n",
    "plot_predictions_by_condition(trial_pred_summary, condition_masks, plot_angles, \"fb\", \"FB Prediction\")\n",
    "plot_predictions_by_condition(trial_pred_summary, condition_masks, plot_angles, \"full\", \"Full Prediction\")\n",
    "plot_predictions_by_condition(trial_pred_summary, condition_masks, plot_angles, \"true\", \"True Velocity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(vel_r2_runs))\n",
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "# Compute means and standard deviations\n",
    "mean_r2 = np.mean(vel_r2_runs, axis=0)\n",
    "std_r2 = np.std(vel_r2_runs, axis=0)\n",
    "\n",
    "mean_cd = np.mean(vel_cd_r2_arr, axis=0)\n",
    "std_cd = np.std(vel_cd_r2_arr, axis=0)\n",
    "\n",
    "mean_fb = np.mean(vel_fb_r2_arr, axis=0)\n",
    "std_fb = np.std(vel_fb_r2_arr, axis=0)\n",
    "\n",
    "# Plot means\n",
    "plt.plot(lag_axis, mean_r2, color='brown', label='r2_runs')\n",
    "plt.plot(lag_axis, mean_cd, color='green', label='cd_r2_arr',linestyle='--',alpha=0.5)\n",
    "plt.plot(lag_axis, mean_fb, color='magenta', label='fb_r2_arr',linestyle='--',alpha=0.5)\n",
    "\n",
    "# Fill between mean ± std\n",
    "plt.fill_between(lag_axis, mean_r2 - std_r2, mean_r2 + std_r2, color='brown', alpha=0.3)\n",
    "plt.fill_between(lag_axis, mean_cd - std_cd, mean_cd + std_cd, color='green', alpha=0.3)\n",
    "plt.fill_between(lag_axis, mean_fb - std_fb, mean_fb + std_fb, color='magenta', alpha=0.3)\n",
    "\n",
    "plt.axvline(cd_lag, color = 'k', linestyle='--')\n",
    "plt.axvline(fb_lag, color = 'k', linestyle='--')\n",
    "plt.xlabel(\"Time lag (ms)\")\n",
    "plt.ylabel('R²')\n",
    "plt.ylim([-0.05,1.05])\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + \"_\"+type+\"_sim_perturb_vel_r2_\"+noise_lv+\"_noise.pdf\", dpi = 'figure')\n",
    "plt.show()\n",
    "# plt.hist(ang_runs)\n",
    "# plt.xlabel(\"Angle (deg)\")\n",
    "# plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(bump_r2_runs))\n",
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "# Compute means and standard deviations\n",
    "mean_r2 = np.mean(bump_r2_runs, axis=0)\n",
    "std_r2 = np.std(bump_r2_runs, axis=0)\n",
    "\n",
    "mean_cd = np.mean(bump_cd_r2_arr, axis=0)\n",
    "std_cd = np.std(bump_cd_r2_arr, axis=0)\n",
    "\n",
    "mean_fb = np.mean(bump_fb_r2_arr, axis=0)\n",
    "std_fb = np.std(bump_fb_r2_arr, axis=0)\n",
    "\n",
    "# Plot means\n",
    "plt.plot(lag_axis, mean_r2, color='brown', label='r2_runs')\n",
    "plt.plot(lag_axis, mean_cd, color='green', label='cd_r2_arr',linestyle='--',alpha=0.5)\n",
    "plt.plot(lag_axis, mean_fb, color='magenta', label='fb_r2_arr',linestyle='--',alpha=0.5)\n",
    "\n",
    "# Fill between mean ± std\n",
    "plt.fill_between(lag_axis, mean_r2 - std_r2, mean_r2 + std_r2, color='brown', alpha=0.3)\n",
    "plt.fill_between(lag_axis, mean_cd - std_cd, mean_cd + std_cd, color='green', alpha=0.3)\n",
    "plt.fill_between(lag_axis, mean_fb - std_fb, mean_fb + std_fb, color='magenta', alpha=0.3)\n",
    "\n",
    "plt.axvline(cd_lag, color = 'k', linestyle='--')\n",
    "plt.axvline(fb_lag, color = 'k', linestyle='--')\n",
    "plt.xlabel(\"Time lag (ms)\")\n",
    "plt.ylabel('R²')\n",
    "plt.ylim([-0.05, 0.45])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + \"_\"+type+\"_sim_perturb_bump_r2_\"+noise_lv+\"_noise.pdf\", dpi = 'figure')ß\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ac2_CD_proj_spikes_smth_40_oneside'].to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_c_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(-300, 300, 10)[np.argmax(avg_c_x1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cross-correlation\n",
    "df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range = (-100,1000), ignored_trials = ~active_mask)\n",
    "n_trials = df['trial_id'].nunique()\n",
    "\n",
    "cd_array = df['ac2_CD_proj_spikes_smth_40_oneside'].to_numpy().reshape(n_trials, -1, 3)\n",
    "\n",
    "# pos_array = df['hand_pos'].to_numpy().reshape(n_trials, -1, 2)\n",
    "# acc_array = df['hand_acc'].to_numpy().reshape(n_trials, -1, 2)\n",
    "vel_array = df['hand_vel'].to_numpy().reshape(n_trials, -1, 2)\n",
    "maxlags = 30 # times binsize is in ms (300ms for best display)\n",
    "X = cd_array\n",
    "Y = vel_array\n",
    "\n",
    "print(X.shape)\n",
    "cc_arr = nans([n_trials, maxlags*2+1])\n",
    "for i in range(n_trials):\n",
    "    x = X[i,:,0]\n",
    "    y = Y[i,:,0]\n",
    "    lags, c = xcorr(x, y, maxlags)\n",
    "    cc_arr[i,:] = c\n",
    "avg_c_x = np.mean(cc_arr, axis=0)\n",
    "\n",
    "cc_arr = nans([n_trials, maxlags*2+1])\n",
    "for i in range(n_trials):\n",
    "    x = X[i,:,1]\n",
    "    y = Y[i,:,0]\n",
    "    lags, c = xcorr(x, y, maxlags)\n",
    "    cc_arr[i,:] = c\n",
    "avg_c_x1 = np.mean(cc_arr, axis=0)\n",
    "\n",
    "\n",
    "cc_arr = nans([n_trials, maxlags*2+1])\n",
    "for i in range(n_trials):\n",
    "    x = X[i,:,2]\n",
    "    y = Y[i,:,1]\n",
    "    lags, c = xcorr(x, y, maxlags)\n",
    "    cc_arr[i,:] = c\n",
    "avg_c_y = np.mean(cc_arr, axis=0)\n",
    "\n",
    "x_axis = lags*dataset_10ms.bin_width\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_axis, avg_c_x, color = 'green', label = 'x1')\n",
    "ax.plot(x_axis, avg_c_x1, color = 'green', label = 'x2')\n",
    "\n",
    "ax.plot(x_axis, avg_c_y, color = 'blue', label = 'y')\n",
    "tmax = x_axis[int(np.mean((np.argmax(abs(avg_c_x)),np.argmax(abs(avg_c_y)))))]\n",
    "# ax.axvline(tmax, color = 'k',linestyle = '--')\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "print(tmax)\n",
    "plt.legend()\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('Normalized cross-corr')\n",
    "plt.title('CD dims vs. hand vel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Autocorrelation\n",
    "df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range = (-100,1000), ignored_trials = ~active_mask)\n",
    "n_trials = df['trial_id'].nunique()\n",
    "\n",
    "vel_array = df['hand_vel'].to_numpy().reshape(n_trials, -1, 2)\n",
    "maxTimeLag = 500 #in ms\n",
    "X = vel_array\n",
    "\n",
    "print(X.shape)\n",
    "binSize = dataset_10ms.bin_width\n",
    "numBin = X.shape[1]\n",
    "x1 = X[:,:,0]\n",
    "x2 = X[:,:,0]\n",
    "ac_x = comp_cc(x1,x2,maxTimeLag,binSize,numBin)\n",
    "\n",
    "x1 = X[:,:,1]\n",
    "x2 = X[:,:,1]\n",
    "ac_y = comp_cc(x1,x2,maxTimeLag,binSize,numBin)\n",
    "\n",
    "time_axis = np.arange(0, maxTimeLag, binSize)\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.plot(time_axis,ac_x/ac_x[0],color = 'green', label = 'x')\n",
    "ax.plot(time_axis,ac_y/ac_y[0],color = 'blue', label = 'y')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time lag (ms)\")\n",
    "plt.ylabel(\"Normalized autocorrelation\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_autocorrelation_vel.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dir = np.array([0,45,90,135,180,225,270,315]) \n",
    "directions = np.array([0,45,90,135,180,225,270,315])\n",
    "\n",
    "# plot_dir = np.array([0,90,180,270]) \n",
    "# directions = np.array([0,90,180,270])\n",
    "\n",
    "cmap = plt.get_cmap('coolwarm',len(plot_dir))\n",
    "custom_palette = [mpl.colors.rgb2hex(cmap(i)) for i in range(len(plot_dir))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA projections over trial, for different reaching directions\n",
    "plot_field = 'SCA_40'\n",
    "n_dims = dataset.data[plot_field].shape[1]\n",
    "# order = np.arange(n_dims)\n",
    "order = ssa_order_smth40\n",
    "\n",
    "pred_range = (-100, 1000)\n",
    "trial_mask = active_mask\n",
    "cond_dict = active_cond_dict\n",
    "n_timepoints = int((pred_range[1] - pred_range[0])/dataset_10ms.bin_width)\n",
    "data = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=pred_range, ignored_trials=~trial_mask)\n",
    "n_trials = data['trial_id'].nunique()\n",
    "trials_pca = nans([n_trials,n_timepoints,n_dims])\n",
    "i = 0\n",
    "for idx, trial in data.groupby('trial_id'):\n",
    "    trials_pca[i,:,:]=trial[plot_field].to_numpy()\n",
    "    i+=1\n",
    "print(trials_pca.shape)\n",
    "\n",
    "x_axis = np.arange(pred_range[0], pred_range[1], dataset_10ms.bin_width)\n",
    "# define some useful time points\n",
    "move_idx=0\n",
    "ret_idx = 500\n",
    "\n",
    "plot_dims = 10\n",
    "\n",
    "fig,ax=plt.subplots(plot_dims,1,figsize=(10,15))\n",
    "for i in range(plot_dims):\n",
    "    for j in range(len(plot_dir)):\n",
    "        color = custom_palette[j]\n",
    "        dir_idx = np.argwhere(directions == plot_dir[j])[0]\n",
    "        cond_mean_proj = np.mean(trials_pca[np.argwhere(cond_dict==dir_idx).flatten(),:,:], axis = 0)[:,order[i]] \n",
    "        pca_mean = np.mean(data[plot_field].to_numpy(),axis = 0)[order[i]] \n",
    "        ax[i].plot(x_axis,cond_mean_proj - pca_mean,linewidth=2.25,color = color,label = plot_dir[j])\n",
    "        \n",
    "        ax[i].axvline(move_idx, color='k',linewidth = .5)\n",
    "        ax[i].axvline(ret_idx, color='k',linewidth = .5)\n",
    "        ax[i].set_xlim([-100,1000])\n",
    "        # ax[i].set_ylim([-15, 15])\n",
    "        ax[i].axhline(0,color ='k',ls = '--')\n",
    "        if i<plot_dims-1:\n",
    "            ax[i].set_xticks([])\n",
    "        else:\n",
    "            ax[i].set_xlabel('Time after movement onset (ms)')\n",
    "            \n",
    "        # ax[i].set_yticks([])\n",
    "        ax[i].set_ylabel('Dim. '+str(i+1))\n",
    "\n",
    "    ax[0].set_title('Active trials ' + plot_field)\n",
    "    \n",
    "plt.legend(bbox_to_anchor = (1, 1), loc = 'upper left')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_PCA_active.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA projections over trial, for different reaching directions\n",
    "order = np.arange(n_dims)\n",
    "# order = ssa_order_smth40\n",
    "\n",
    "pred_range = (-100, 500)\n",
    "trial_mask = passive_mask\n",
    "cond_dict = passive_cond_dict\n",
    "n_timepoints = int((pred_range[1] - pred_range[0])/dataset_10ms.bin_width)\n",
    "# n_trials = dataset_10ms.trial_info.loc[trial_mask].shape[0]\n",
    "data = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=pred_range, ignored_trials=~trial_mask)\n",
    "n_trials = data['trial_id'].nunique()\n",
    "trials_pca = nans([n_trials,n_timepoints,n_dims])\n",
    "i = 0\n",
    "for idx, trial in data.groupby('trial_id'):\n",
    "    trials_pca[i,:,:]=trial[plot_field].to_numpy()\n",
    "    i+=1\n",
    "print(trials_pca.shape)\n",
    "\n",
    "x_axis = np.arange(pred_range[0], pred_range[1], dataset_10ms.bin_width)\n",
    "# define some useful time points\n",
    "move_idx=0\n",
    "ret_idx = 200\n",
    "\n",
    "plot_dims = 10\n",
    "\n",
    "fig,ax=plt.subplots(plot_dims,1,figsize=(10,15))\n",
    "for i in range(plot_dims):\n",
    "    for j in range(len(plot_dir)):\n",
    "        color = custom_palette[j]\n",
    "        dir_idx = np.argwhere(directions == plot_dir[j])[0]\n",
    "        cond_mean_proj = np.mean(trials_pca[np.argwhere(cond_dict==dir_idx).flatten(),:,:], axis = 0)[:,order[i]] \n",
    "        pca_mean = np.mean(data[plot_field].to_numpy(),axis = 0)[order[i]]\n",
    "        ax[i].plot(x_axis,cond_mean_proj - pca_mean,linewidth=2.25,color = color,label = plot_dir[j])\n",
    "        \n",
    "        ax[i].axvline(move_idx, color='k',linewidth = .5)\n",
    "        ax[i].axvline(ret_idx, color='k',linewidth = .5)\n",
    "        ax[i].set_xlim([-100,500])\n",
    "        # ax[i].set_ylim([-15, 15])\n",
    "        ax[i].axhline(0,color ='k',ls = '--')\n",
    "        if i<plot_dims-1:\n",
    "            ax[i].set_xticks([])\n",
    "        else:\n",
    "            ax[i].set_xlabel('Time after movement onset (ms)')\n",
    "            \n",
    "        # ax[i].set_yticks([])\n",
    "        ax[i].set_ylabel('Dim. '+str(i+1))\n",
    "\n",
    "    ax[0].set_title('Passive trials ' + plot_field)\n",
    "    \n",
    "plt.legend(bbox_to_anchor = (1, 1), loc = 'upper left')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_PCA_passive.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Get a specific neuron's original index from GLM results to plot\n",
    "# with np.load(monkey+'_hf_neuron_filter.npz') as data:\n",
    "#     neuron_filter = data['neuron_filter']\n",
    "# fr_filtered_idx = np.argwhere(neuron_filter==1).flatten()\n",
    "# index_in_glm = [18, 27, 34, 35, 42, 48]\n",
    "\n",
    "# index_original = fr_filtered_idx[index_in_glm]\n",
    "# print(index_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dir = np.array([0,45,90,135,180,225,270,315]) \n",
    "directions = np.array([0,45,90,135,180,225,270,315])\n",
    "\n",
    "# plot_dir = np.array([0,90,180,270]) \n",
    "# directions = np.array([0,90,180,270])\n",
    "\n",
    "cmap = plt.get_cmap('coolwarm',len(plot_dir))\n",
    "custom_palette = [mpl.colors.rgb2hex(cmap(i)) for i in range(len(plot_dir))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot single neuron activity over trial, for different reaching directions\n",
    "dataset = dataset_10ms\n",
    "nrn_idx = 75\n",
    "pred_range = (-200, 1000)\n",
    "trial_mask = active_mask\n",
    "cond_dict = active_cond_dict\n",
    "n_timepoints = int((pred_range[1] - pred_range[0])/dataset.bin_width)\n",
    "n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "data = dataset.make_trial_data(align_field='move_onset_time', align_range=pred_range, ignored_trials=~trial_mask)\n",
    "trials_activity = nans([n_trials,n_timepoints])\n",
    "i = 0\n",
    "for idx, trial in data.groupby('trial_id'):\n",
    "    trials_activity[i,:]=trial.spikes_smth_40.to_numpy()[:,nrn_idx]\n",
    "    i+=1\n",
    "print(trials_activity.shape)\n",
    "\n",
    "plot_dir = np.array([0,45,90,135,180,225,270,315]) \n",
    "directions = np.array([0,45,90,135,180,225,270,315])\n",
    "# plot_dir = np.array([0,90,180,270]) \n",
    "# directions = np.array([0,90,180,270]) \n",
    "x_axis = np.arange(pred_range[0], pred_range[1], dataset.bin_width)\n",
    "\n",
    "# define some useful time points\n",
    "move_idx=0\n",
    "ret_idx = 500\n",
    "\n",
    "fig,ax=plt.subplots(1,1,figsize=(8,2))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for j in range(len(plot_dir)):\n",
    "    color = custom_palette[j]\n",
    "    dir_idx = np.argwhere(directions == plot_dir[j])[0]\n",
    "    cond_mean_proj = np.mean(trials_activity[np.argwhere(cond_dict==dir_idx).flatten(),:], axis = 0)\n",
    "    ax.plot(x_axis,cond_mean_proj/dataset.bin_width*1000,linewidth=2.25,color = color,label = plot_dir[j])\n",
    "\n",
    "ax.axvline(move_idx,color='k')\n",
    "ax.axvline(ret_idx,color='k')\n",
    "ax.set_xlim([-200,1000])\n",
    "ax.set_xlabel('Time after movement onset (ms)')\n",
    "ax.set_ylabel('Firing rate (sp/s)')\n",
    "ax.set_title('Active trials')\n",
    "\n",
    "plt.legend(bbox_to_anchor = (1, 1), loc = 'upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_example_fb_nrn_active.pdf',dpi = 'figure')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot single neuron activity over trial, for different reaching directions\n",
    "dataset = dataset_10ms\n",
    "pred_range = (-100, 500)\n",
    "trial_mask = passive_mask\n",
    "cond_dict = passive_cond_dict\n",
    "n_timepoints = int((pred_range[1] - pred_range[0])/dataset.bin_width)\n",
    "n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "data = dataset.make_trial_data(align_field='move_onset_time', align_range=pred_range, ignored_trials=~trial_mask)\n",
    "trials_activity = nans([n_trials,n_timepoints])\n",
    "i = 0\n",
    "for idx, trial in data.groupby('trial_id'):\n",
    "    trials_activity[i,:]=trial.spikes_smth_40.to_numpy()[:,nrn_idx]\n",
    "    i+=1\n",
    "print(trials_activity.shape)\n",
    "\n",
    "plot_dir = np.array([0,45,90,135,180,225,270,315]) \n",
    "directions = np.array([0,45,90,135,180,225,270,315])\n",
    "# plot_dir = np.array([0,90,180,270]) \n",
    "# directions = np.array([0,90,180,270]) \n",
    "x_axis = np.arange(pred_range[0], pred_range[1], dataset.bin_width)\n",
    "\n",
    "# define some useful time points\n",
    "move_idx=0\n",
    "ret_idx = 120\n",
    "\n",
    "fig,ax=plt.subplots(1,1,figsize=(8,2))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for j in range(len(plot_dir)):\n",
    "    color = custom_palette[j]\n",
    "    dir_idx = np.argwhere(directions == plot_dir[j])[0]\n",
    "    cond_mean_proj = np.mean(trials_activity[np.argwhere(cond_dict==dir_idx).flatten(),:], axis = 0)\n",
    "    ax.plot(x_axis,cond_mean_proj/dataset.bin_width*1000,linewidth=2.25,color = color,label = plot_dir[j])\n",
    "\n",
    "ax.axvline(move_idx,color='k')\n",
    "ax.axvline(ret_idx,color='k')\n",
    "ax.set_xlim([-100,500])\n",
    "ax.set_xlabel('Time after movement onset (ms)')\n",
    "ax.set_ylabel('Firing rate (sp/s)')\n",
    "ax.set_title('Passive trials')\n",
    "\n",
    "plt.legend(bbox_to_anchor = (1, 1), loc = 'upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_example_fb_nrn_passive.pdf',dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed = np.sqrt(np.sum(dataset_10ms.data['hand_vel'][:].T**2,axis=0)).to_numpy().reshape((-1,1))\n",
    "# dataset_10ms.add_continuous_data(speed,'speed')\n",
    "# acceleration = np.diff(speed, axis = 0, prepend=[speed[0]])\n",
    "# dataset_10ms.add_continuous_data(acceleration,'acceleration') #technically change of speed, for timing plots\n",
    "# jerk = np.diff(acceleration, axis = 0, prepend=[acceleration[0]])\n",
    "# dataset_10ms.add_continuous_data(jerk,'jerk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_range = (-1000,500)\n",
    "x_axis = np.arange(plot_range[0], plot_range[1], dataset_10ms.bin_width)\n",
    "active_df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=plot_range, ignored_trials=~active_mask, allow_overlap=True)\n",
    "passive_df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=plot_range, ignored_trials=~passive_mask, allow_overlap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_trials_spikes = []\n",
    "for _, trial in active_df.groupby('trial_id'):\n",
    "    active_trials_spikes.append(np.mean(trial.spikes,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = (-155, -145)\n",
    "sum((x_axis >= window[0]) & (x_axis <= window[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel, wilcoxon\n",
    "def get_window_mean(trial_spikes, x_axis, window):\n",
    "    idx = (x_axis >= window[0]) & (x_axis <= window[1])\n",
    "    # print(sum(idx))\n",
    "    return np.mean(trial_spikes[idx])\n",
    "\n",
    "baseline_window = (-250, -150)\n",
    "baseline_vals = [get_window_mean(trial, x_axis, baseline_window) for trial in active_trials_spikes]\n",
    "window_size = 20   # ms\n",
    "step_size = 10      # ms\n",
    "\n",
    "test_centers = np.arange(-150, 0, step_size)  # test windows centered around these times\n",
    "p_ttest = []\n",
    "p_wilcoxon = []\n",
    "for center in test_centers:\n",
    "    test_window = (center - window_size//2, center + window_size//2)\n",
    "    print(test_window)\n",
    "    test_vals = [get_window_mean(trial, x_axis, test_window) for trial in active_trials_spikes]\n",
    "    _, p_t = ttest_rel(test_vals, baseline_vals)\n",
    "    _, p_w = wilcoxon(test_vals, baseline_vals)\n",
    "    p_ttest.append(p_t); p_wilcoxon.append(p_w)\n",
    "plt.plot(test_centers, p_wilcoxon, marker='o',label='wilcoxon')\n",
    "plt.plot(test_centers, p_ttest, marker='o',label='ttest')\n",
    "plt.legend()\n",
    "plt.axhline(0.05, color='red', linestyle='--')  # significance threshold\n",
    "plt.xlabel('Time relative to movement onset (ms)')\n",
    "plt.ylabel('p-value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #PSTH\n",
    "# active_trials_spikes = []\n",
    "# for _, trial in active_df.groupby('trial_id'):\n",
    "#     active_trials_spikes.append(np.mean(trial.spikes,axis=1))\n",
    "# fig, ax = plt.subplots(figsize=(10,3))\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# ax.plot(x_axis,np.sum(active_trials_spikes,axis = 0)/dataset_10ms.bin_width*1000/len(active_trials_spikes),\"-o\",markersize=5, color = 'k',label = 'Active')\n",
    "# ax.set_ylabel('Average FR (sp/s)')\n",
    "# plt.xlabel('Time after movement onset (ms)')\n",
    "# plt.axvline(0, color = 'k',linestyle = '--')    \n",
    "\n",
    "# plt.xlabel('Time after move onset (ms)')\n",
    "# plt.axvline(0, color = 'k',linestyle = '--')    \n",
    "# plt.axvline(-250, color = 'k',linestyle = '--')    \n",
    "# plt.axvline(-150, color = 'k',linestyle = '--')    \n",
    "# plt.title('Active trials')\n",
    "# plt.xlim([-500, 500])\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = active_df\n",
    "# # df = passive_df\n",
    "# var = 'FB_proj'\n",
    "# plot_dir = [0.0, 90.0, 180.0, 270.0] \n",
    "# # plot_dir = [45.0, 135.0, 225.0, 315.0] \n",
    "# plot_dim = 0 # plot x velocity\n",
    "# colors = ['red', 'blue', 'green', 'orange']\n",
    "# fig, ax = plt.subplots(figsize=(10,2))\n",
    "\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# for trial_dir, color in zip(plot_dir, colors):\n",
    "#     cond_ids = dataset.trial_info[dataset.trial_info.cond_dir%360 == trial_dir].trial_id\n",
    "#     for _, trial in df[np.isin(df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "#         # plt.plot(x_axis, trial[var][plot_dim], color=color, linewidth=0.5)\n",
    "#         plt.plot(x_axis, np.array(trial[var])[:,plot_dim], color=color, linewidth=0.5)\n",
    "# # plt.xlim([-500, 2000])\n",
    "# # plt.ylim([-50, 50])\n",
    "# plt.axvline([0],color='k',linestyle='--')\n",
    "# # plt.axhline([.5])\n",
    "# plt.xlabel('Time before movement offset (ms)')\n",
    "# plt.ylabel('Hand velocity (cm/s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_dir = [0.0, 90.0, 180.0, 270.0] \n",
    "# # plot_dir = [45.0, 135.0, 225.0, 315.0] \n",
    "# plot_dim = 4 # plot x velocity\n",
    "# colors = ['red', 'blue', 'green', 'orange']\n",
    "# fig, ax = plt.subplots(figsize=(10,2))\n",
    "\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# for trial_dir, color in zip(plot_dir, colors):\n",
    "#     cond_ids = dataset.trial_info[dataset.trial_info.cond_dir%360 == trial_dir].trial_id\n",
    "#     for _, trial in df[np.isin(df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "#         # plt.plot(x_axis, trial[var][plot_dim], color=color, linewidth=0.5)\n",
    "#         plt.plot(x_axis, np.array(trial[var])[:,plot_dim], color=color, linewidth=0.5)\n",
    "# # plt.ylim([-50, 50])\n",
    "# plt.axvline([0],color='k',linestyle='--')\n",
    "# # plt.axhline([.5])\n",
    "# plt.xlabel('Time before movement offset (ms)')\n",
    "# plt.ylabel('Hand velocity (cm/s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = active_df\n",
    "# # df = passive_df\n",
    "# var = 'acceleration'\n",
    "# # plot_dir = [0.0, 90.0, 180.0, 270.0] \n",
    "# plot_dir = [45.0, 135.0, 225.0, 315.0] \n",
    "# colors = ['red', 'blue', 'green', 'orange']\n",
    "# # fig, ax = plt.subplots(figsize=(10,2))\n",
    "\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# for trial_dir, color in zip(plot_dir, colors):\n",
    "#     cond_ids = dataset.trial_info[dataset.trial_info.cond_dir%360 == trial_dir].trial_id\n",
    "#     for _, trial in df[np.isin(df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "#         plt.plot(x_axis, trial[var], color=color, linewidth=0.5)\n",
    "# plt.xlim([-200, 200])\n",
    "# # plt.ylim([-50, 50])\n",
    "# plt.axvline([0],color='k',linestyle='--')\n",
    "# # plt.axhline([5])\n",
    "# plt.xlabel('Time after movement onset (ms)')\n",
    "# plt.ylabel('Speed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df = active_df\n",
    "# df = passive_df\n",
    "# var = 'acceleration'\n",
    "# plot_dir = [0.0, 90.0, 180.0, 270.0] \n",
    "# # plot_dir = [45.0, 135.0, 225.0, 315.0] \n",
    "# colors = ['red', 'blue', 'green', 'orange']\n",
    "# # fig, ax = plt.subplots(figsize=(10,2))\n",
    "\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# for trial_dir, color in zip(plot_dir, colors):\n",
    "#     cond_ids = dataset.trial_info[dataset.trial_info.cond_dir%360 == trial_dir].trial_id\n",
    "#     for _, trial in df[np.isin(df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "#         plt.plot(x_axis, trial[var], color=color, linewidth=0.5)\n",
    "# plt.xlim([-200, 200])\n",
    "# # plt.ylim([-50, 50])\n",
    "# plt.axvline([0],color='k',linestyle='--')\n",
    "# plt.axhline([1])\n",
    "# plt.xlabel('Time after movement onset (ms)')\n",
    "# plt.ylabel('Speed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trial length\n",
    "# dt = (dataset.trial_info.go_cue_time - dataset.trial_info.start_time).dt.total_seconds()*1000\n",
    "# dt[active_mask]\n",
    "# plt.hist(dt[active_mask])\n",
    "# # plt.xlim([-100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_range = (-500,2000)\n",
    "# x_axis = np.arange(plot_range[0], plot_range[1], dataset_10ms.bin_width)\n",
    "# active_df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=plot_range, ignored_trials=~active_mask, allow_overlap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #PSTH\n",
    "# active_trials_spikes = []\n",
    "# for _, trial in active_df.groupby('trial_id'):\n",
    "#     active_trials_spikes.append(np.sum(trial.spikes,axis=1))\n",
    "# fig, ax = plt.subplots(figsize=(10,3))\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# ax.plot(x_axis,np.sum(active_trials_spikes,axis = 0)/dataset_10ms.bin_width*1000/len(active_trials_spikes)/n_neurons,\"-o\",markersize=5, color = 'k',label = 'Active')\n",
    "# ax.set_ylabel('Average FR (sp/s)')\n",
    "# plt.xlabel('Time after movement onset (ms)')\n",
    "# plt.axvline(0, color = 'k',linestyle = '--')    \n",
    "\n",
    "# plt.xlabel('Time after move onset (ms)')\n",
    "# plt.axvline(0, color = 'k',linestyle = '--')    \n",
    "# plt.xlim([-500, 2000])\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_range = (-2000,500)\n",
    "# x_axis = np.arange(plot_range[0], plot_range[1], dataset_10ms.bin_width)\n",
    "# active_df = dataset_10ms.make_trial_data(align_field='move_offset_time', align_range=plot_range, ignored_trials=~active_mask, allow_overlap=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #PSTH\n",
    "# active_trials_spikes = []\n",
    "# for _, trial in active_df.groupby('trial_id'):\n",
    "#     active_trials_spikes.append(np.sum(trial.spikes,axis=1))\n",
    "# # passive_trials_spikes = []\n",
    "# # for _, trial in passive_df.groupby('trial_id'):\n",
    "# #     passive_trials_spikes.append(np.sum(trial.spikes,axis=1))\n",
    "# fig, ax = plt.subplots(figsize=(10,3))\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# ax.plot(x_axis,np.sum(active_trials_spikes,axis = 0)/dataset_10ms.bin_width*1000/len(active_trials_spikes)/n_neurons,\"-o\",markersize=5, color = 'k',label = 'Active')\n",
    "\n",
    "# ax.set_ylabel('Average FR (sp/s)')\n",
    "# plt.xlabel('Time after movement offset (ms)')\n",
    "# plt.axvline(0, color = 'k',linestyle = '--')    \n",
    "\n",
    "\n",
    "# plt.xlabel('Time after move offset (ms)')\n",
    "# plt.axvline(0, color = 'k',linestyle = '--')    \n",
    "# plt.xlim([-2000, 500])\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(figDir + monkey + '_psth.pdf',dpi = 'figure')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_range = (-300,600)\n",
    "x_axis = np.arange(plot_range[0], plot_range[1], dataset_10ms.bin_width)\n",
    "active_df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=plot_range, ignored_trials=~active_mask, allow_overlap=True)\n",
    "passive_df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=plot_range, ignored_trials=~passive_mask, allow_overlap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PSTH\n",
    "active_trials_spikes = []\n",
    "for _, trial in active_df.groupby('trial_id'):\n",
    "    active_trials_spikes.append(np.sum(trial.spikes,axis=1))\n",
    "passive_trials_spikes = []\n",
    "for _, trial in passive_df.groupby('trial_id'):\n",
    "    passive_trials_spikes.append(np.sum(trial.spikes,axis=1))\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.plot(x_axis,np.sum(active_trials_spikes,axis = 0)/dataset_10ms.bin_width*1000/len(active_trials_spikes)/n_neurons,\"-o\",markersize=5, color = 'k',label = 'Active')\n",
    "ax.plot(x_axis,np.sum(passive_trials_spikes,axis = 0)/dataset_10ms.bin_width*1000/len(passive_trials_spikes)/n_neurons,\"-o\",markersize=5, color = 'red',label = 'Passive')\n",
    "# plt.title('Peristimulus aligned to move_onset')\n",
    "# plt.legend()\n",
    "ax.set_ylabel('Average Firing Rate (sp/s)')\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "plt.axvline(0, color = 'k',linestyle = '--')    \n",
    "\n",
    "\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "plt.axvline(0, color = 'k',linestyle = '--')    \n",
    "plt.xlim([-300, 600])\n",
    "# plt.ylim([6.5, 13.2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_psth.pdf',dpi = 'figure')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Kinematics\n",
    "# var = 'speed'\n",
    "# fig, ax = plt.subplots(figsize=(10,6))\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# count = 0\n",
    "# for _, trial in active_df.groupby('trial_id'):\n",
    "#     if len(trial[var]) == len(x_axis):\n",
    "#         plt.plot(x_axis,trial[var], color='k', linewidth=0.5)\n",
    "#         count+=1\n",
    "# print(count,'active trials')\n",
    "# # count = 0\n",
    "# # for _, trial in passive_df.groupby('trial_id'):\n",
    "# #     if len(trial[var]) == len(x_axis):\n",
    "# #         plt.plot(x_axis, trial[var], color='red', linewidth=0.5)\n",
    "# #         count+=1\n",
    "# # print(count,'passive trials')\n",
    "\n",
    "# plt.xlabel('Time after movement onset (ms)')\n",
    "# plt.ylabel('Hand speed (cm/s)')\n",
    "# plt.axvline(0, color = 'k',linestyle = '--')\n",
    "# # plt.title('Speed aligned to move_onset')\n",
    "# # plt.axvline(120, color = 'k',linestyle = '--')\n",
    "# plt.xlim([-2000,500])\n",
    "# # plt.ylim([-3,100])\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(figDir + monkey + '_speed_whole.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reach-bump trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_10ms\n",
    "x_field = 'spikes'\n",
    "data = np.load(monkey+'_v6_zscore_smoothed_cdfb_data_'+x_field+'.npz')\n",
    "data.files\n",
    "dataset.add_continuous_data(data['CD_FB_proj'],'CD_FB_proj')\n",
    "dataset.add_continuous_data(data['CD_proj'],'CD_proj')\n",
    "dataset.add_continuous_data(data['FB_proj'],'FB_proj')\n",
    "dataset.data.keys().unique(0)\n",
    "\n",
    "x_field = 'spikes'\n",
    "data = np.load(monkey+'_v6_zscore_unsmoothed_cdfb_data_'+x_field+'.npz')\n",
    "data.files\n",
    "dataset.add_continuous_data(data['CD_FB_proj'],'unsmooth_CD_FB_proj')\n",
    "dataset.add_continuous_data(data['CD_proj'],'unsmooth_CD_proj')\n",
    "dataset.add_continuous_data(data['FB_proj'],'unsmooth_FB_proj')\n",
    "dataset.data.keys().unique(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_range = (-600,1600)\n",
    "# plot_range = (-1600,600)\n",
    "x_axis = np.arange(plot_range[0], plot_range[1], dataset_10ms.bin_width)\n",
    "active_df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=plot_range, ignored_trials=~active_mask, allow_overlap=True)\n",
    "nan_df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=plot_range, ignored_trials=~nan_mask, allow_overlap=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bump_latencies = (\n",
    "    (dataset_10ms.trial_info.bump_time - dataset_10ms.trial_info.move_onset_time)\n",
    "    .dt.total_seconds() * 100\n",
    ").to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_dir_dict = (\n",
    "    dataset_10ms.trial_info.cond_dir\n",
    ").to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'hand_vel'\n",
    "lag_range = [-300, 300]\n",
    "# plot_dim = 0  # 0 = x-velocity, 1 = y-velocity\n",
    "# colors = ['red', 'blue', 'green', 'orange']\n",
    "# all_dirs = [0.0, 45.0, 90.0, 135.0, 180.0, 225.0, 270.0, 315.0] \n",
    "# plot_dirs = [0.0, 90.0, 180.0, 270.0]  # These are bump_dir values\n",
    "\n",
    "plot_dim = 0 # plot x velocity\n",
    "all_dirs = np.array([0.0, 45.0, 90.0, 135.0, 180.0, 225.0, 270.0, 315.0])\n",
    "plot_dirs = np.array([0.0, 45.0, 90.0, 135.0, 180.0, 225.0, 270.0, 315.0])\n",
    "cmap = plt.get_cmap('coolwarm',len(plot_dirs))\n",
    "custom_palette = [mpl.colors.rgb2hex(cmap(i)) for i in range(len(all_dirs))]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# Step 1: Compute condition means by cond_dir\n",
    "cond_means = {}  # cond_dir → mean velocity trace\n",
    "\n",
    "for cond_dir in all_dirs:\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir % 360 == cond_dir].trial_id\n",
    "    subset = active_df[np.isin(active_df.trial_id, cond_ids)].copy()\n",
    "    n_trials = subset.trial_id.nunique()\n",
    "    vel_array = np.vstack(subset[var].values)\n",
    "    vel_trials = vel_array.reshape(n_trials, -1, 2)\n",
    "    cond_means[cond_dir] = vel_trials.mean(axis=0)  # shape: (timepoints, 2)\n",
    "\n",
    "bump_vel_list = []; full_vel_list = []\n",
    "bump_cd_list = []; bump_fb_list = []; bump_cdfb_list = []\n",
    "bump_pc_list = []; bump_spikes_list = []\n",
    "bump_dir_idx = []\n",
    "i = 0\n",
    "# Step 2: Plot trials grouped by cond_dir, subtracting their cond_dir mean\n",
    "# for plot_dir, color in zip(plot_dirs, custom_palette):\n",
    "for plot_dir in plot_dirs:    \n",
    "    # cond_ids = dataset.trial_info[(dataset.trial_info.cond_dir-dataset.trial_info.bump_dir) % 360 == plot_dir].trial_id\n",
    "    bump_cond_ids = dataset.trial_info[(dataset.trial_info.bump_dir) % 360 == plot_dir].trial_id\n",
    "    assist_list = []; against_list = []; disturb_list = []\n",
    "    for id, trial in nan_df[np.isin(nan_df.trial_id, bump_cond_ids)].groupby('trial_id'):\n",
    "        vel = np.array(trial[var])  # shape: (timepoints, 2)\n",
    "        cond_mean = cond_means.get(cond_dir_dict[id]%360)\n",
    "        if cond_dir_dict[id]%360 == plot_dir:\n",
    "            assist_list.append(i)\n",
    "        elif (cond_dir_dict[id]+180)%360 == plot_dir:\n",
    "            against_list.append(i)\n",
    "        else:\n",
    "            disturb_list.append(i)\n",
    "        trace = vel - cond_mean\n",
    "        n_bin_decode_start, n_bin_decode_end = 0, 5\n",
    "        move_onset_idx = int(abs(plot_range[0]/dataset.bin_width))\n",
    "        start, end = move_onset_idx+int(bump_latencies[id])+n_bin_decode_start, move_onset_idx+int(bump_latencies[id])+n_bin_decode_end+1\n",
    "        bump_vel = trace[start:end,:]\n",
    "        # ax.plot(bump_vel[:,plot_dim], color=color, linewidth=0.3)\n",
    "        bump_vel_list.append(bump_vel)\n",
    "        full_vel_list.append(vel[start:end,:])\n",
    "        start, end = start+int(lag_range[0]/dataset.bin_width), end+int(lag_range[1]/dataset.bin_width)\n",
    "        bump_cd_list.append(np.array(trial['unsmooth_CD_proj'])[start:end])\n",
    "        bump_fb_list.append(np.array(trial['unsmooth_FB_proj'])[start:end])\n",
    "        bump_cdfb_list.append(np.array(trial['unsmooth_CD_FB_proj'])[start:end])\n",
    "        # bump_pc_list.append(np.array(trial['PCA_40'])[start:end])\n",
    "        # bump_spikes_list.append(np.array(trial['spikes_smth_40'])[start:end])\n",
    "        color = custom_palette[np.argwhere(cond_dir_dict[id]%360==all_dirs)[0,0]] # colored by target directions\n",
    "        ax.plot(trace[int(bump_latencies[id]):,0], color=color, linewidth=0.3)\n",
    "        i+=1\n",
    "    bump_dir_idx.append([assist_list,against_list,disturb_list])\n",
    "\n",
    "# Final plot formatting\n",
    "# plt.xlim([-600, 1450])\n",
    "plt.xlim([0, 205])\n",
    "plt.ylim([-55, 55])\n",
    "plt.axvline(60, color='k', linestyle='--')\n",
    "plt.axvline(70, color='k', linestyle='--')\n",
    "plt.axvline(80, color='k', linestyle='--')\n",
    "plt.axvline(100, color='k', linestyle='--')\n",
    "\n",
    "plt.xlabel('Time after bump onset (ms)')\n",
    "plt.ylabel('Velocity (cm/s)')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_reachbump_bump_onset_bump_vel.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "from tqdm import tqdm\n",
    "\n",
    "def lagged_ridge_regression_shuffle(X, Y, lags_ms, norm_x = True, pos_bool=False, split_pred = False,n_cd_dims = 0, bin_size=10, n_splits=20):\n",
    "    n_trials, T_X, n_features = X.shape\n",
    "    _, T_Y, n_outputs = Y.shape\n",
    "    lags_bins = (np.array(lags_ms) // bin_size).astype(int)\n",
    "\n",
    "    Y_norm = Y - np.nanmean(Y, axis=(0, 1))\n",
    "\n",
    "    r2_combined = []\n",
    "    r2_individual = []\n",
    "    coefs = []\n",
    "    pred_vels = []\n",
    "    cd_pred_vels = []\n",
    "    fb_pred_vels = []\n",
    "\n",
    "    cv = ShuffleSplit(n_splits=n_splits, random_state = 42)\n",
    "\n",
    "    for j in tqdm(range(len(lags_bins)), desc=\"Lag regression\"):\n",
    "        scaler = int((lags_ms[1]-lags_ms[0])/bin_size)\n",
    "        X_window = X[:, 0+j*scaler:0+T_Y+j*scaler, :]\n",
    "        # Normalize\n",
    "        if norm_x:\n",
    "            X_lag = (X_window - np.nanmean(X_window, axis=(0, 1))) / np.nanstd(X_window, axis=(0, 1))\n",
    "        else:\n",
    "            X_lag = X_window - np.nanmean(X_window, axis=(0, 1))\n",
    "\n",
    "        Y_lag = Y_norm\n",
    "        lr_all = GridSearchCV(Ridge(positive=pos_bool), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        X_lag = X_lag.reshape(-1, n_features)\n",
    "        lr_all.fit(X_lag, Y_lag.reshape(-1, n_outputs))\n",
    "        coefs.append(lr_all.best_estimator_.coef_)\n",
    "        Y_hat = lr_all.predict(X_lag)\n",
    "        pred_vel = Y_hat + np.nanmean(Y, axis=(0, 1))\n",
    "        pred_vels.append(pred_vel.reshape(Y.shape))\n",
    "        if split_pred:\n",
    "            cd_pred = X_lag[:,:n_cd_dims] @ lr_all.best_estimator_.coef_[:,:n_cd_dims].T \n",
    "            fb_pred = X_lag[:,n_cd_dims:] @ lr_all.best_estimator_.coef_[:,n_cd_dims:].T \n",
    "            # cd_pred = X_lag[:,1:n_cd_dims] @ lr_all.best_estimator_.coef_[:,1:n_cd_dims].T \n",
    "            # fb_pred = X_lag[:,n_cd_dims+3:9] @ lr_all.best_estimator_.coef_[:,n_cd_dims+3:9].T \n",
    "            cd_pred_vels.append(cd_pred.reshape(Y.shape))\n",
    "            fb_pred_vels.append(fb_pred.reshape(Y.shape))\n",
    "        r2s_comb = []\n",
    "        r2s_indiv = []\n",
    "        for train_idx, test_idx in cv.split(np.arange(n_trials)):\n",
    "            # Get train/test sets\n",
    "            X_train, X_test, Y_train, Y_test = process_train_test(X_window,Y,train_idx,test_idx,norm_x)\n",
    "            # Ridge regression with CV\n",
    "            model = GridSearchCV(Ridge(positive=pos_bool), {'alpha': np.logspace(-3, 3, 7)})\n",
    "            model.fit(X_train, Y_train)\n",
    "            Y_pred = model.predict(X_test)\n",
    "            # R² per output dim\n",
    "            r2_ind = []\n",
    "            for i in range(n_outputs):\n",
    "                ss_res = np.sum((Y_test[:, i] - Y_pred[:, i]) ** 2)\n",
    "                ss_tot = np.sum((Y_test[:, i] - np.mean(Y_test[:, i])) ** 2)\n",
    "                r2_ind.append(1 - ss_res / ss_tot)\n",
    "            r2s_indiv.append(r2_ind)\n",
    "            # R² combined\n",
    "            ss_res = np.sum((Y_test - Y_pred) ** 2)\n",
    "            ss_tot = np.sum((Y_test - np.mean(Y_test, axis=0)) ** 2)\n",
    "            r2s_comb.append(1 - ss_res / ss_tot)\n",
    "        r2_combined.append(r2s_comb)\n",
    "        r2_individual.append(r2s_indiv)\n",
    "    if split_pred:   \n",
    "        return np.array(r2_combined), np.array(r2_individual), np.array(coefs), np.array(pred_vels),\\\n",
    "        np.array(cd_pred_vels), np.array(fb_pred_vels)\n",
    "    else:\n",
    "        return np.array(r2_combined), np.array(r2_individual), np.array(coefs), np.array(pred_vels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "assist, against, disturb = 0,1,2\n",
    "trial = assist\n",
    "assist_dict = [0] + list(itertools.accumulate([len(dir_idx[trial]) for dir_idx in bump_dir_idx]))\n",
    "all_assist = np.concatenate([dir_idx[trial] for dir_idx in bump_dir_idx])\n",
    "all_assist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = against\n",
    "against_dict = [0] + list(itertools.accumulate([len(dir_idx[trial]) for dir_idx in bump_dir_idx]))\n",
    "all_against = np.concatenate([dir_idx[trial] for dir_idx in bump_dir_idx])\n",
    "all_against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assist_against_indices = np.concatenate([all_assist,all_against])\n",
    "assist_against_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(bump_vel_list)\n",
    "norm_x = True\n",
    "lags = np.arange(-300, 310, 20) \n",
    "pos_bool = False\n",
    "\n",
    "# X = np.array(bump_cd_list)\n",
    "# r2_comb, r2_each, coefs, pred_vels = lagged_ridge_regression_shuffle(X, Y, lags, norm_x=norm_x, pos_bool=pos_bool)\n",
    "\n",
    "y_dir_indices = [1,2,6,7,8]\n",
    "\n",
    "X = np.array(bump_cdfb_list)\n",
    "X = X[assist_against_indices,:,:]\n",
    "X = X[:,:,y_dir_indices]\n",
    "Y = Y[assist_against_indices,:,1][:,:,np.newaxis]\n",
    "# r2_comb, r2_each, coefs, pred_vels,cd_pred_vels,fb_pred_vels = lagged_ridge_regression_shuffle(X, Y, lags, norm_x=norm_x, pos_bool=pos_bool,\\\n",
    "#                                                                                                split_pred = True,n_cd_dims=2)\n",
    "\n",
    "# lag_idx =np.argmax(np.mean(r2_comb,axis=1))\n",
    "# print(np.max(np.mean(r2_comb,axis=1)))\n",
    "# print(lags[lag_idx])\n",
    "\n",
    "# best_coef = coefs[lag_idx,:,:]\n",
    "# best_pred_vel = pred_vels[lag_idx,:,:,:]\n",
    "# best_cd_pred_vel = cd_pred_vels[lag_idx,:,:,:]\n",
    "# best_fb_pred_vel = fb_pred_vels[lag_idx,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(bump_cdfb_list).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lags,np.mean(r2_comb,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.arange(0,60,dataset.bin_width)\n",
    "dirs = [0.0, 90.0, 180.0, 270.0]\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "\n",
    "start,end = 0,len(all_assist)\n",
    "dict = assist_dict\n",
    "type = 'assist'\n",
    "\n",
    "start,end = len(all_assist), len(all_assist)+len(all_against)\n",
    "dict = against_dict\n",
    "type = 'against'\n",
    "\n",
    "\n",
    "plot_dim = 0\n",
    "\n",
    "var = best_cd_pred_vel[start:end,:,:] + best_fb_pred_vel[start:end,:,:]\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for dir, color in zip(dirs, colors):\n",
    "    j = np.argwhere(plot_dirs == dir)[0,0]\n",
    "    indices = np.arange(dict[j],dict[j+1])\n",
    "    avg_trace = np.mean(np.array(var)[indices,:,plot_dim],axis=0)\n",
    "    plt.plot(x_axis,avg_trace, color=color, linewidth=5)\n",
    "plt.xlabel('Time after bump onset (ms)')\n",
    "plt.ylabel('Velocity (cm/s)')\n",
    "plt.title('Full Prediction - ' + type)\n",
    "plt.ylim([-12, 12])\n",
    "# plt.ylim([-10, 10])\n",
    "\n",
    "var = best_cd_pred_vel[start:end,:,:]\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for dir, color in zip(dirs, colors):\n",
    "    j = np.argwhere(plot_dirs == dir)[0,0]\n",
    "    indices = np.arange(dict[j],dict[j+1])\n",
    "    avg_trace = np.mean(np.array(var)[indices,:,plot_dim],axis=0)\n",
    "    plt.plot(x_axis,avg_trace, color=color, linewidth=5)\n",
    "plt.xlabel('Time after bump onset (ms)')\n",
    "plt.ylabel('Velocity (cm/s)')\n",
    "plt.title('CD Prediction - ' + type)\n",
    "plt.ylim([-12, 12])\n",
    "# plt.ylim([-10, 10])\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_cd_x_bump.pdf',dpi = 'figure')\n",
    "plt.show()\n",
    "\n",
    "var = best_fb_pred_vel[start:end,:,:]\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for dir, color in zip(dirs, colors):\n",
    "    j = np.argwhere(plot_dirs == dir)[0,0]\n",
    "    indices = np.arange(dict[j],dict[j+1])\n",
    "    avg_trace = np.mean(np.array(var)[indices,:,plot_dim],axis=0)\n",
    "    plt.plot(x_axis,avg_trace, color=color, linewidth=5)\n",
    "plt.xlabel('Time after bump onset (ms)')\n",
    "plt.ylabel('Velocity (cm/s)')\n",
    "plt.title('FB Prediction - ' + type)\n",
    "plt.ylim([-12, 12])\n",
    "# plt.ylim([-10, 10])\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_fb_x_bump.pdf',dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assist, against, disturb = 0,1,2\n",
    "\n",
    "signal_x_axis = np.arange(-300,350+10,dataset.bin_width)\n",
    "dirs = [90.0,270.0]\n",
    "colors = ['blue','orange']\n",
    "\n",
    "# dirs = [0.0,180.0]\n",
    "# colors = ['red','green']\n",
    "\n",
    "# dirs = [45.0, 135.0, 225.0, 315.0]\n",
    "# dirs = [90.0, 135.0, 225.0,270.0]\n",
    "\n",
    "# dirs = [0.0, 90.0, 180.0, 270.0]\n",
    "# colors = ['red', 'blue', 'green', 'orange']\n",
    "\n",
    "\n",
    "# trial = assist\n",
    "plot_dim = 1\n",
    "\n",
    "var = bump_cd_list\n",
    "fig, ax = plt.subplots(figsize=(3, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for dir, color in zip(dirs, colors):\n",
    "    j = np.argwhere(plot_dirs == dir)[0,0]\n",
    "\n",
    "    trial=assist\n",
    "    indices = bump_dir_idx[j][trial]\n",
    "    avg_trace = np.mean(np.array(var)[indices,:,1],axis=0)\n",
    "    plt.plot(signal_x_axis,avg_trace, color=color, linewidth=3)\n",
    "    \n",
    "    trial=against\n",
    "    indices = bump_dir_idx[j][trial]\n",
    "    avg_trace = np.mean(np.array(var)[indices,:,1],axis=0)\n",
    "    plt.plot(signal_x_axis,avg_trace, color=color, linestyle = '--',linewidth=3)\n",
    "\n",
    "plt.xlabel('Time after bump onset (ms)')\n",
    "plt.ylabel('CD')\n",
    "# plt.axvline(0,color = 'k', linestyle='--')\n",
    "# plt.axvline(50,color = 'k', linestyle='--')\n",
    "plt.xlim([0,200])\n",
    "# plt.ylim([-.25,0.2])\n",
    "# plt.ylim([-45, 45])\n",
    "# plt.ylim([-20, 20])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_cd_bump.pdf',dpi = 'figure')\n",
    "\n",
    "var = bump_fb_list\n",
    "fig, ax = plt.subplots(figsize=(3, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for dir, color in zip(dirs, colors):\n",
    "    j = np.argwhere(plot_dirs == dir)[0,0]\n",
    "    trial=assist\n",
    "    indices = bump_dir_idx[j][trial]\n",
    "    avg_trace = np.mean(np.array(var)[indices,:,3],axis=0)\n",
    "    plt.plot(signal_x_axis,avg_trace, color=color, linewidth=3)\n",
    "    \n",
    "    trial=against\n",
    "    indices = bump_dir_idx[j][trial]\n",
    "    avg_trace = np.mean(np.array(var)[indices,:,3],axis=0)\n",
    "    plt.plot(signal_x_axis,avg_trace, color=color, linestyle = '--',linewidth=3)\n",
    "\n",
    "plt.xlabel('Time after bump onset (ms)')\n",
    "plt.ylabel('FB')\n",
    "# plt.axvline(0,color = 'k', linestyle='--')\n",
    "# plt.axvline(50,color = 'k', linestyle='--')\n",
    "# plt.ylim([-45, 45])\n",
    "# plt.ylim([-0.5,0.4])\n",
    "plt.xlim([0,200])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_fb_bump.pdf',dpi = 'figure')\n",
    "\n",
    "var = bump_fb_list\n",
    "fig, ax = plt.subplots(figsize=(3, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for dir, color in zip(dirs, colors):\n",
    "    j = np.argwhere(plot_dirs == dir)[0,0]\n",
    "    trial=assist\n",
    "    indices = bump_dir_idx[j][trial]\n",
    "    avg_trace = np.mean(np.array(bump_fb_list)[indices,:,3] - 2*np.array(bump_cd_list)[indices,:,1],axis=0)\n",
    "    plt.plot(signal_x_axis,avg_trace, color=color, linewidth=3)\n",
    "\n",
    "    trial=against\n",
    "    indices = bump_dir_idx[j][trial]\n",
    "    avg_trace = np.mean(np.array(bump_fb_list)[indices,:,3] - 2*np.array(bump_cd_list)[indices,:,1],axis=0)\n",
    "    plt.plot(signal_x_axis,avg_trace, color=color, linestyle = '--',linewidth=3)\n",
    "plt.xlabel('Time after bump onset (ms)')\n",
    "plt.ylabel('FB - CD')\n",
    "# plt.axvline(0,color = 'k', linestyle='--')\n",
    "# plt.axvline(50,color = 'k', linestyle='--')\n",
    "# plt.ylim([-45, 45])\n",
    "# plt.ylim([-0.5,0.4])\n",
    "plt.xlim([0,200])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_fb-cd_bump.pdf',dpi = 'figure')\n",
    "\n",
    "# var = full_vel_list\n",
    "# fig, ax = plt.subplots(figsize=(6, 4))\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# for dir, color in zip(dirs, colors):\n",
    "#     j = np.argwhere(plot_dirs == dir)[0,0]\n",
    "#     indices = bump_dir_idx[j][trial]\n",
    "#     avg_trace = np.mean(np.array(var)[indices,:,plot_dim],axis=0)\n",
    "#     plt.plot(x_axis,avg_trace, color=color, linewidth=5)\n",
    "# plt.xlabel('Time after bump onset (ms)')\n",
    "# plt.ylabel('Velocity (cm/s)')\n",
    "# plt.ylim([-45, 45])\n",
    "# # plt.ylim([-20, 20])\n",
    "\n",
    "# var = bump_vel_list\n",
    "# fig, ax = plt.subplots(figsize=(6, 4))\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# for dir, color in zip(dirs, colors):\n",
    "#     j = np.argwhere(plot_dirs == dir)[0,0]\n",
    "#     indices = bump_dir_idx[j][trial]\n",
    "#     avg_trace = np.mean(np.array(var)[indices,:,plot_dim],axis=0)\n",
    "#     plt.plot(x_axis,avg_trace, color=color, linewidth=5)\n",
    "# plt.xlabel('Time after bump onset (ms)')\n",
    "# plt.ylabel('Velocity (cm/s)')\n",
    "# # plt.ylim([-30, 30])\n",
    "# plt.ylim([-20, 20])\n",
    "\n",
    "# var = best_pred_vel\n",
    "# fig, ax = plt.subplots(figsize=(6, 4))\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# for dir, color in zip(dirs, colors):\n",
    "#     j = np.argwhere(plot_dirs == dir)[0,0]\n",
    "#     indices = bump_dir_idx[j][trial]\n",
    "#     avg_trace = np.mean(np.array(var)[indices,:,plot_dim],axis=0)\n",
    "#     plt.plot(x_axis,avg_trace, color=color, linewidth=5)\n",
    "# plt.xlabel('Time after bump onset (ms)')\n",
    "# plt.ylabel('Velocity (cm/s)')\n",
    "# plt.ylim([-20, 20])\n",
    "# # plt.ylim([-10, 10])\n",
    "\n",
    "\n",
    "# var = best_cd_pred_vel\n",
    "# fig, ax = plt.subplots(figsize=(6, 4))\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# for dir, color in zip(dirs, colors):\n",
    "#     j = np.argwhere(plot_dirs == dir)[0,0]\n",
    "#     indices = bump_dir_idx[j][trial]\n",
    "#     avg_trace = np.mean(np.array(var)[indices,:,plot_dim],axis=0)\n",
    "#     plt.plot(x_axis,avg_trace, color=color, linewidth=5)\n",
    "# plt.xlabel('Time after bump onset (ms)')\n",
    "# plt.ylabel('Velocity (cm/s)')\n",
    "# # plt.ylim([-20, 20])\n",
    "# plt.ylim([-10, 10])\n",
    "\n",
    "# # plt.tight_layout()\n",
    "# # plt.savefig(figDir + monkey + '_cd_x_bump.pdf',dpi = 'figure')\n",
    "# plt.show()\n",
    "\n",
    "# var = best_fb_pred_vel\n",
    "# fig, ax = plt.subplots(figsize=(6, 4))\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# for dir, color in zip(dirs, colors):\n",
    "#     j = np.argwhere(plot_dirs == dir)[0,0]\n",
    "#     indices = bump_dir_idx[j][trial]\n",
    "#     avg_trace = np.mean(np.array(var)[indices,:,plot_dim],axis=0)\n",
    "#     plt.plot(x_axis,avg_trace, color=color, linewidth=5)\n",
    "# plt.xlabel('Time after bump onset (ms)')\n",
    "# plt.ylabel('Velocity (cm/s)')\n",
    "# # plt.ylim([-20, 20])\n",
    "# plt.ylim([-10, 10])\n",
    "\n",
    "# # plt.tight_layout()\n",
    "# # plt.savefig(figDir + monkey + '_fb_x_bump.pdf',dpi = 'figure')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd_r2_comb = r2_comb;  cd_r2_each = r2_each; cd_coefs = best_coef; cd_preds = best_pred_vel\n",
    "# fb_r2_comb = r2_comb; fb_r2_each = r2_each; fb_coefs = best_coef; fb_preds = best_pred_vel\n",
    "# cdfb_r2_comb = r2_comb; cdfb_r2_each = r2_each; cdfb_coefs = best_coef; cdfb_preds = best_pred_vel\n",
    "\n",
    "# cd_r2_comb_sc = r2_comb;  cd_r2_each_sc = r2_each; cd_coefs_sc = best_coef; cd_preds_sc = best_pred_vel\n",
    "# fb_r2_comb_sc = r2_comb; fb_r2_each_sc = r2_each; fb_coefs_sc = best_coef; fb_preds_sc = best_pred_vel\n",
    "cdfb_r2_comb_sc = r2_comb; cdfb_r2_each_sc = r2_each; cdfb_coefs_sc = best_coef; cdfb_preds_sc = best_pred_vel\n",
    "\n",
    "# PC_r2_comb = r2_comb;  PC_r2_each = r2_each; PC_preds = best_pred_vel\n",
    "# spikes_r2_comb = r2_comb;  spikes_r2_each = r2_each; spikes_preds = best_pred_vel\n",
    "\n",
    "\n",
    "# np.savez(monkey+'_MC_zscore_smooth40_spikes_hand_vel_bump_r2s', \\\n",
    "#          cd_r2_comb = cd_r2_comb, cd_r2_each = cd_r2_each, cd_coefs=cd_coefs, cd_preds=cd_preds,\\\n",
    "#          fb_r2_comb = fb_r2_comb, fb_r2_each = fb_r2_each, fb_coefs=fb_coefs, fb_preds=fb_preds,\\\n",
    "#          cdfb_r2_comb = cdfb_r2_comb, cdfb_r2_each = cdfb_r2_each, cdfb_coefs=cdfb_coefs, cdfb_preds=cdfb_preds,\\\n",
    "\n",
    "#          cd_r2_comb_sc = cd_r2_comb_sc, cd_r2_each_sc = cd_r2_each_sc, cd_coefs_sc=cd_coefs_sc, cd_preds_sc=cd_preds_sc,\\\n",
    "#          fb_r2_comb_sc = fb_r2_comb_sc, fb_r2_each_sc = fb_r2_each_sc, fb_coefs_sc=fb_coefs_sc, fb_preds_sc=fb_preds_sc,\\\n",
    "#          cdfb_r2_comb_sc = cdfb_r2_comb_sc, cdfb_r2_each_sc = cdfb_r2_each_sc, cdfb_coefs_sc=cdfb_coefs_sc, cdfb_preds_sc=cdfb_preds_sc,\\\n",
    "\n",
    "#          PC_r2_comb = PC_r2_comb, PC_r2_each = PC_r2_each, PC_preds = PC_preds,\\\n",
    "#          spikes_r2_comb = spikes_r2_comb, spikes_r2_each = spikes_r2_each, spikes_preds = spikes_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey = 'Duncan_20190710'\n",
    "data = np.load(monkey+'_MC_zscore_smooth40_spikes_hand_vel_bump_r2s.npz')\n",
    "lw = 3\n",
    "lag_axis = np.arange(-300, 320, 20)\n",
    "fig, ax = plt.subplots(figsize=(5.5, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# CD only\n",
    "r2_cd = data['cd_r2_comb']  # shape [n_lags, n_splits]\n",
    "mean_cd = np.nanmean(r2_cd, axis=1)\n",
    "std_cd = np.nanstd(r2_cd, axis=1)\n",
    "plt.plot(lag_axis, mean_cd, linewidth=lw, color='green', label='cd',alpha=0.1)\n",
    "plt.fill_between(lag_axis, mean_cd - std_cd, mean_cd + std_cd, color='green', alpha=0.1)\n",
    "\n",
    "print(np.nanmax(mean_cd))\n",
    "print(lag_axis[np.nanargmax(mean_cd)])\n",
    "\n",
    "r2_cd = data['cd_r2_comb_sc']  # shape [n_lags, n_splits]\n",
    "\n",
    "mean_cd = np.nanmean(r2_cd, axis=1)\n",
    "std_cd = np.nanstd(r2_cd, axis=1)\n",
    "plt.plot(lag_axis, mean_cd, linewidth=lw, color='green', label='cd')\n",
    "plt.fill_between(lag_axis, mean_cd - std_cd, mean_cd + std_cd, color='green', alpha=0.3)\n",
    "\n",
    "print(np.nanmax(mean_cd))\n",
    "print(lag_axis[np.nanargmax(mean_cd)])\n",
    "\n",
    "# FB only\n",
    "r2_fb = data['fb_r2_comb']\n",
    "mean_fb = np.nanmean(r2_fb, axis=1)\n",
    "std_fb = np.nanstd(r2_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_fb, linewidth=lw, color='magenta', label='fb',alpha=0.1)\n",
    "plt.fill_between(lag_axis, mean_fb - std_fb, mean_fb + std_fb, color='magenta', alpha=0.1)\n",
    "\n",
    "print(np.nanmax(mean_fb))\n",
    "print(lag_axis[np.nanargmax(mean_fb)])\n",
    "\n",
    "r2_fb = data['fb_r2_comb_sc']\n",
    "mean_fb = np.nanmean(r2_fb, axis=1)\n",
    "std_fb = np.nanstd(r2_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_fb, linewidth=lw, color='magenta', label='fb')\n",
    "plt.fill_between(lag_axis, mean_fb - std_fb, mean_fb + std_fb, color='magenta', alpha=0.3)\n",
    "\n",
    "print(np.nanmax(mean_fb))\n",
    "print(lag_axis[np.nanargmax(mean_fb)])\n",
    "\n",
    "\n",
    "# CD + FB\n",
    "r2_cd_fb = data['cdfb_r2_comb_sc']\n",
    "mean_cd_fb = np.nanmean(r2_cd_fb, axis=1)\n",
    "std_cd_fb = np.nanstd(r2_cd_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_cd_fb, linewidth=lw, color='brown', label='cd+fb',alpha=0.1)\n",
    "plt.fill_between(lag_axis, mean_cd_fb - std_cd_fb, mean_cd_fb + std_cd_fb, color='brown', alpha=0.1)\n",
    "print(np.nanmax(mean_cd_fb))\n",
    "print(lag_axis[np.nanargmax(mean_cd_fb)])\n",
    "\n",
    "r2_cd_fb = data['cdfb_r2_comb']\n",
    "mean_cd_fb = np.nanmean(r2_cd_fb, axis=1)\n",
    "std_cd_fb = np.nanstd(r2_cd_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_cd_fb, linewidth=lw, color='brown', label='cd+fb')\n",
    "plt.fill_between(lag_axis, mean_cd_fb - std_cd_fb, mean_cd_fb + std_cd_fb, color='brown', alpha=0.3)\n",
    "print(np.nanmax(mean_cd_fb))\n",
    "print(lag_axis[np.nanargmax(mean_cd_fb)])\n",
    "\n",
    "# All nrns\n",
    "r2_nrn = data['spikes_r2_comb']\n",
    "mean_nrn = np.nanmean(r2_nrn, axis=1)\n",
    "std_nrn = np.nanstd(r2_nrn, axis=1)\n",
    "plt.plot(lag_axis, mean_nrn, linewidth=lw, linestyle = '--',color='grey', label='neurons', alpha=0.5)\n",
    "plt.fill_between(lag_axis, mean_nrn - std_nrn, mean_nrn + std_nrn, color='grey', alpha=0.5)\n",
    "print(np.nanmax(mean_nrn))\n",
    "print(lag_axis[np.nanargmax(mean_nrn)])\n",
    "\n",
    "# 20 PC\n",
    "r2_pc = data['PC_r2_comb']\n",
    "mean_pc = np.nanmean(r2_pc, axis=1)\n",
    "std_pc = np.nanstd(r2_pc, axis=1)\n",
    "plt.plot(lag_axis, mean_pc, linewidth=lw, linestyle = '--',color='lightgrey', label='PCs', alpha=0.5)\n",
    "plt.fill_between(lag_axis, mean_pc - std_pc, mean_pc + std_pc, color='lightgrey', alpha=0.5)\n",
    "print(np.nanmax(mean_pc))\n",
    "print(lag_axis[np.nanargmax(mean_pc)])\n",
    "plt.axvline(0, color = 'k', linestyle='--')\n",
    "\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R²')\n",
    "# plt.ylim([-0.1, 0.9])\n",
    "plt.ylim([-0.15, 0.7])\n",
    "# plt.legend(fontsize=8)\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(figDir + monkey + '_zscore_bump_r2.pdf',dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw = 3\n",
    "lag_axis = np.arange(-300, 320, 20)\n",
    "fig, ax = plt.subplots(figsize=(5.5, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# CD only\n",
    "# r2_cd = data['cd_r2_comb']  # shape [n_lags, n_splits]\n",
    "# mean_cd = np.nanmean(r2_cd, axis=1)\n",
    "# std_cd = np.nanstd(r2_cd, axis=1)\n",
    "# plt.plot(lag_axis, mean_cd, linewidth=lw, color='green', label='cd',alpha=0.1)\n",
    "# plt.fill_between(lag_axis, mean_cd - std_cd, mean_cd + std_cd, color='green', alpha=0.1)\n",
    "\n",
    "# print(np.nanmax(mean_cd))\n",
    "# print(lag_axis[np.nanargmax(mean_cd)])\n",
    "\n",
    "r2_cd = data['cd_r2_comb_sc']  # shape [n_lags, n_splits]\n",
    "\n",
    "mean_cd = np.nanmean(r2_cd, axis=1)\n",
    "std_cd = np.nanstd(r2_cd, axis=1)\n",
    "plt.plot(lag_axis, mean_cd, linewidth=lw, color='green', label='cd')\n",
    "plt.fill_between(lag_axis, mean_cd - std_cd, mean_cd + std_cd, color='green', alpha=0.3)\n",
    "\n",
    "print(np.nanmax(mean_cd))\n",
    "print(lag_axis[np.nanargmax(mean_cd)])\n",
    "\n",
    "# FB only\n",
    "# r2_fb = data['fb_r2_comb']\n",
    "# mean_fb = np.nanmean(r2_fb, axis=1)\n",
    "# std_fb = np.nanstd(r2_fb, axis=1)\n",
    "# plt.plot(lag_axis, mean_fb, linewidth=lw, color='magenta', label='fb',alpha=0.1)\n",
    "# plt.fill_between(lag_axis, mean_fb - std_fb, mean_fb + std_fb, color='magenta', alpha=0.1)\n",
    "\n",
    "# print(np.nanmax(mean_fb))\n",
    "# print(lag_axis[np.nanargmax(mean_fb)])\n",
    "\n",
    "r2_fb = data['fb_r2_comb_sc']\n",
    "mean_fb = np.nanmean(r2_fb, axis=1)\n",
    "std_fb = np.nanstd(r2_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_fb, linewidth=lw, color='magenta', label='fb')\n",
    "plt.fill_between(lag_axis, mean_fb - std_fb, mean_fb + std_fb, color='magenta', alpha=0.3)\n",
    "\n",
    "print(np.nanmax(mean_fb))\n",
    "print(lag_axis[np.nanargmax(mean_fb)])\n",
    "\n",
    "\n",
    "# CD + FB\n",
    "r2_cd_fb = data['cdfb_r2_comb']\n",
    "mean_cd_fb = np.nanmean(r2_cd_fb, axis=1)\n",
    "std_cd_fb = np.nanstd(r2_cd_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_cd_fb, linewidth=lw, color='brown', label='cd+fb')\n",
    "plt.fill_between(lag_axis, mean_cd_fb - std_cd_fb, mean_cd_fb + std_cd_fb, color='brown', alpha=0.3)\n",
    "print(np.nanmax(mean_cd_fb))\n",
    "print(lag_axis[np.nanargmax(mean_cd_fb)])\n",
    "\n",
    "# r2_cd_fb = data['cdfb_r2_comb_sc']\n",
    "# mean_cd_fb = np.nanmean(r2_cd_fb, axis=1)\n",
    "# std_cd_fb = np.nanstd(r2_cd_fb, axis=1)\n",
    "# plt.plot(lag_axis, mean_cd_fb, linewidth=lw, color='brown', linestyle='--',alpha=0.3,label='cd+fb')\n",
    "# plt.fill_between(lag_axis, mean_cd_fb - std_cd_fb, mean_cd_fb + std_cd_fb, color='brown', alpha=0.3)\n",
    "# print(np.nanmax(mean_cd_fb))\n",
    "# print(lag_axis[np.nanargmax(mean_cd_fb)])\n",
    "\n",
    "plt.axvline(0, color = 'k', linestyle='--')\n",
    "\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R²')\n",
    "# plt.ylim([-0.1, 0.9])\n",
    "plt.ylim([-0.15, 0.55])\n",
    "# plt.legend(fontsize=8)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(figDir + monkey + '_zscore_bump_vsim_r2.pdf',dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(monkey+'_MC_sc_bump_r2s.npz')\n",
    "data.files\n",
    "lw = 3\n",
    "lag_axis = np.arange(-300, 320, 20)\n",
    "fig, ax = plt.subplots(figsize=(5.5, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# CD only\n",
    "r2_cd = data['cd_r2_comb']\n",
    "mean_cd = np.nanmean(r2_cd, axis=1)\n",
    "std_cd = np.nanstd(r2_cd, axis=1)\n",
    "plt.plot(lag_axis, mean_cd, linewidth=lw, color='green', label='cd')\n",
    "plt.fill_between(lag_axis, mean_cd - std_cd, mean_cd + std_cd, color='green', alpha=0.3)\n",
    "\n",
    "print(np.nanmax(mean_cd))\n",
    "print(lag_axis[np.nanargmax(mean_cd)])\n",
    "\n",
    "# FB only\n",
    "r2_fb = data['fb_r2_comb']\n",
    "mean_fb = np.nanmean(r2_fb, axis=1)\n",
    "std_fb = np.nanstd(r2_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_fb, linewidth=lw, color='magenta', label='fb')\n",
    "plt.fill_between(lag_axis, mean_fb - std_fb, mean_fb + std_fb, color='magenta', alpha=0.3)\n",
    "\n",
    "print(np.nanmax(mean_fb))\n",
    "print(lag_axis[np.nanargmax(mean_fb)])\n",
    "\n",
    "# CD + FB\n",
    "r2_cd_fb = data['cdfb_r2_comb']\n",
    "mean_cd_fb = np.nanmean(r2_cd_fb, axis=1)\n",
    "std_cd_fb = np.nanstd(r2_cd_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_cd_fb, linewidth=lw, color='brown', label='cd+fb')\n",
    "plt.fill_between(lag_axis, mean_cd_fb - std_cd_fb, mean_cd_fb + std_cd_fb, color='brown', alpha=0.3)\n",
    "print(np.nanmax(mean_cd_fb))\n",
    "print(lag_axis[np.nanargmax(mean_cd_fb)])\n",
    "\n",
    "# # All nrns\n",
    "# r2_nrn = data['r2_nrn']\n",
    "# mean_nrn = np.nanmean(r2_nrn, axis=1)\n",
    "# std_nrn = np.nanstd(r2_nrn, axis=1)\n",
    "# plt.plot(lag_axis, mean_nrn, linewidth=lw, linestyle = '--',color='grey', label='neurons', alpha=0.3)\n",
    "# plt.fill_between(lag_axis, mean_nrn - std_nrn, mean_nrn + std_nrn, color='grey', alpha=0.3)\n",
    "# print(np.nanmax(mean_nrn))\n",
    "# print(lag_axis[np.nanargmax(mean_nrn)])\n",
    "\n",
    "# # 20 PC\n",
    "# r2_pc = data['r2_pc']\n",
    "# mean_pc = np.nanmean(r2_pc, axis=1)\n",
    "# std_pc = np.nanstd(r2_pc, axis=1)\n",
    "# plt.plot(lag_axis, mean_pc, linewidth=lw, linestyle = '--',color='lightgrey', label='PCs', alpha=0.5)\n",
    "# plt.fill_between(lag_axis, mean_pc - std_pc, mean_pc + std_pc, color='lightgrey', alpha=0.5)\n",
    "# print(np.nanmax(mean_pc))\n",
    "# print(lag_axis[np.nanargmax(mean_pc)])\n",
    "plt.axvline(0, color = 'k', linestyle='--')\n",
    "\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R²')\n",
    "plt.ylim([-0.15, 0.55])\n",
    "# # plt.legend(fontsize=8)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(figDir + monkey + '_bump_r2.pdf',dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5.5, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# CD only\n",
    "r2_cd = data['cd_r2_comb_sc']\n",
    "mean_cd = np.nanmean(r2_cd, axis=1)\n",
    "std_cd = np.nanstd(r2_cd, axis=1)\n",
    "plt.plot(lag_axis, mean_cd, linewidth=lw, color='green', label='cd')\n",
    "plt.fill_between(lag_axis, mean_cd - std_cd, mean_cd + std_cd, color='green', alpha=0.3)\n",
    "\n",
    "print(np.nanmax(mean_cd))\n",
    "print(lag_axis[np.nanargmax(mean_cd)])\n",
    "\n",
    "# FB only\n",
    "r2_fb = data['fb_r2_comb_sc']\n",
    "mean_fb = np.nanmean(r2_fb, axis=1)\n",
    "std_fb = np.nanstd(r2_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_fb, linewidth=lw, color='magenta', label='fb')\n",
    "plt.fill_between(lag_axis, mean_fb - std_fb, mean_fb + std_fb, color='magenta', alpha=0.3)\n",
    "\n",
    "print(np.nanmax(mean_fb))\n",
    "print(lag_axis[np.nanargmax(mean_fb)])\n",
    "\n",
    "# CD + FB\n",
    "r2_cd_fb = data['cdfb_r2_comb_sc']\n",
    "mean_cd_fb = np.nanmean(r2_cd_fb, axis=1)\n",
    "std_cd_fb = np.nanstd(r2_cd_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_cd_fb, linewidth=lw, color='brown', label='cd+fb')\n",
    "plt.fill_between(lag_axis, mean_cd_fb - std_cd_fb, mean_cd_fb + std_cd_fb, color='brown', alpha=0.3)\n",
    "print(np.nanmax(mean_cd_fb))\n",
    "print(lag_axis[np.nanargmax(mean_cd_fb)])\n",
    "\n",
    "# # All nrns\n",
    "# r2_nrn = data['r2_nrn']\n",
    "# mean_nrn = np.nanmean(r2_nrn, axis=1)\n",
    "# std_nrn = np.nanstd(r2_nrn, axis=1)\n",
    "# plt.plot(lag_axis, mean_nrn, linewidth=lw, linestyle = '--',color='grey', label='neurons', alpha=0.3)\n",
    "# plt.fill_between(lag_axis, mean_nrn - std_nrn, mean_nrn + std_nrn, color='grey', alpha=0.3)\n",
    "# print(np.nanmax(mean_nrn))\n",
    "# print(lag_axis[np.nanargmax(mean_nrn)])\n",
    "\n",
    "# # 20 PC\n",
    "# r2_pc = data['r2_pc']\n",
    "# mean_pc = np.nanmean(r2_pc, axis=1)\n",
    "# std_pc = np.nanstd(r2_pc, axis=1)\n",
    "# plt.plot(lag_axis, mean_pc, linewidth=lw, linestyle = '--',color='lightgrey', label='PCs', alpha=0.5)\n",
    "# plt.fill_between(lag_axis, mean_pc - std_pc, mean_pc + std_pc, color='lightgrey', alpha=0.5)\n",
    "# print(np.nanmax(mean_pc))\n",
    "# print(lag_axis[np.nanargmax(mean_pc)])\n",
    "plt.axvline(0, color = 'k', linestyle='--')\n",
    "\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R²')\n",
    "plt.ylim([-0.15, 0.55])\n",
    "# # plt.legend(fontsize=8)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(figDir + monkey + '_sc_bump_r2.pdf',dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = active_df\n",
    "# df = passive_df\n",
    "df = nan_df\n",
    "var = 'hand_vel'\n",
    "# plot_dir = [0.0, 90.0, 180.0, 270.0] \n",
    "# # plot_dir = [45.0, 135.0, 225.0, 315.0] \n",
    "# plot_dim = 0 # plot x velocity\n",
    "# colors = ['red', 'blue', 'green', 'orange']\n",
    "\n",
    "plot_dim = 0 # plot x velocity\n",
    "plot_dir = np.array([0,45,90,135,180,225,270,315]) \n",
    "cmap = plt.get_cmap('coolwarm',len(plot_dir))\n",
    "custom_palette = [mpl.colors.rgb2hex(cmap(i)) for i in range(len(plot_dir))]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,3))\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for trial_dir, color in zip(plot_dir, custom_palette):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir%360 == trial_dir].trial_id\n",
    "    # cond_ids = dataset.trial_info[dataset.trial_info.bump_dir%360 == trial_dir].trial_id\n",
    "    # cond_ids = dataset.trial_info[(dataset.trial_info.cond_dir-dataset.trial_info.bump_dir)%360 == trial_dir].trial_id\n",
    "    for _, trial in df[np.isin(df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        # plt.plot(x_axis, trial[var][plot_dim], color=color, linewidth=0.5)\n",
    "        plt.plot(x_axis, np.array(trial[var])[:,plot_dim], color=color, linewidth=0.3)\n",
    "# plt.xlim([-150, 600])\n",
    "# plt.xlim([-450, 1600])\n",
    "# plt.xlim([-1600, 450])\n",
    "# plt.xlim([-600, 1450])\n",
    "\n",
    "plt.ylim([-55, 55])\n",
    "# plt.ylim([-45, 45])\n",
    "plt.axvline([0],color='k',linestyle='--')\n",
    "# plt.axhline([10])\n",
    "plt.xlabel('Time after bump onset (ms)')\n",
    "plt.ylabel('Velocity (cm/s)')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_reachbump_bump_onset_vel.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dir = [0.0, 90.0, 180.0, 270.0] \n",
    "# plot_dir = [45.0, 135.0, 225.0, 315.0] \n",
    "plot_dim = 1 # plot x velocity\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "fig, ax = plt.subplots(figsize=(10,2))\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir%360 == trial_dir].trial_id\n",
    "    for _, trial in df[np.isin(df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        # plt.plot(x_axis, trial[var][plot_dim], color=color, linewidth=0.5)\n",
    "        plt.plot(x_axis, np.array(trial[var])[:,plot_dim], color=color, linewidth=0.5)\n",
    "# plt.xlim([-200, 200])\n",
    "plt.ylim([-50, 50])\n",
    "plt.axvline([0],color='k',linestyle='--')\n",
    "# plt.axhline([.5])\n",
    "plt.xlabel('Time after bump onset (ms)')\n",
    "plt.ylabel('Hand velocity (cm/s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CD/FB subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_10ms\n",
    "# x_field = 'PCA_40'\n",
    "x_field = 'spikes'\n",
    "# x_field = 'proj_out'\n",
    "dim = dataset.data[x_field].shape[1]\n",
    "\n",
    "align_range = (-100, 1500)\n",
    "align_field = 'move_onset_time'\n",
    "mask = active_mask\n",
    "active_df = dataset.make_trial_data(align_field=align_field, align_range=align_range, ignored_trials=~mask)\n",
    "n_trials = active_n_trials\n",
    "X = active_df[x_field].to_numpy().reshape((-1, dim))\n",
    "print(X.shape)\n",
    "\n",
    "z_score = True\n",
    "if z_score:\n",
    "    mean = np.nanmean(X,axis=0)\n",
    "    std = np.nanstd(X,axis=0)\n",
    "    mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find CD axes\n",
    "align_range = (-100,0)\n",
    "align_field = 'move_onset_time'\n",
    "\n",
    "mask = active_mask\n",
    "active_df = dataset.make_trial_data(align_field=align_field, align_range=align_range, ignored_trials=~mask)\n",
    "cond_dict = active_cond_dict\n",
    "n_trials = active_n_trials\n",
    "X = active_df[x_field].to_numpy().reshape((-1, dim))\n",
    "if z_score:\n",
    "    X = (X - mean)/std\n",
    "act_trial_spikes = X.reshape((n_trials, -1, dim))\n",
    "print(act_trial_spikes.shape)\n",
    "act_trial_mean_activity = np.mean(act_trial_spikes, axis=1)\n",
    "\n",
    "dirs = [dataset.trial_info[dataset.trial_info.trial_id == i]['cond_dir'] for i in active_df.trial_id]\n",
    "cos_x = np.array([round(math.cos(math.radians(i)),3) for i in dirs])\n",
    "sin_y = np.array([round(math.sin(math.radians(i)),3) for i in dirs])\n",
    "cos_sin = np.array([cos_x, sin_y]).T\n",
    "print(cos_sin.shape)\n",
    "\n",
    "# dirs = [dataset.trial_info[dataset.trial_info.trial_id == i]['cond_dir'] for i in active_df.trial_id]\n",
    "# filter = np.array([x%90==0 for x in dirs])\n",
    "# n_trials = len(filter)\n",
    "# act_trial_mean_activity = act_trial_mean_activity[filter[0::act_trial_spikes.shape[1]].squeeze(),:]\n",
    "# print(act_trial_mean_activity.shape)\n",
    "\n",
    "# cos_x = np.array([round(math.cos(math.radians(i)),3) for i in dirs])[filter.squeeze()]\n",
    "# sin_y = np.array([round(math.sin(math.radians(i)),3) for i in dirs])[filter.squeeze()]\n",
    "# cos_sin = np.array([cos_x, sin_y]).T\n",
    "# print(cos_sin.shape)\n",
    "\n",
    "# n_trials = act_trial_mean_activity.shape[0]\n",
    "# cond_dict = cond_dict[filter[0::act_trial_spikes.shape[1]].squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_proj_matrix_sparse(A,reg=1e-10):\n",
    "#     if A.ndim == 1:\n",
    "#         A = A.reshape(-1, 1)\n",
    "#     return A @ np.linalg.pinv(A.T @ A + reg * np.eye(A.shape[1])) @ A.T\n",
    "# def calc_proj_sparse(R, w,reg=1e-10):\n",
    "#     \"\"\" Returns projection of R(ates) onto the space defined by w \"\"\"\n",
    "#     P = calc_proj_matrix_sparse(w,reg=reg)\n",
    "#     return P@R.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "X = act_trial_mean_activity\n",
    "\n",
    "y=np.array(cos_x).reshape((n_trials,-1,1))[:,0,:]\n",
    "X_proc = X\n",
    "axes_list_x = nans([N,y.shape[1],dim])\n",
    "r2_cv_list_x = nans([N])\n",
    "r2_list_x = nans([N])\n",
    "for i in range(N):\n",
    "    reg = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    # reg = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    axes_list_x[i,:,:] = reg.best_estimator_.coef_\n",
    "    r2_list_x[i] = reg.best_estimator_.score(X_proc, y)\n",
    "    weights = reg.best_estimator_.coef_\n",
    "    skf = StratifiedKFold(n_splits=10,shuffle=True,random_state = 42)   \n",
    "    true_concat = nans([n_trials,1])\n",
    "    pred_concat = nans([n_trials,1])\n",
    "    trial_save_idx = 0\n",
    "    for training_set, test_set in skf.split(range(0,n_trials),cond_dict):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test = X_proc[training_set,:],X_proc[test_set,:]\n",
    "        y_train, y_test = y[training_set],y[test_set]\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        # lr = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_test_predicted = lr.predict(X_test)\n",
    "\n",
    "        n = y_test_predicted.shape[0]\n",
    "        true_concat[trial_save_idx:trial_save_idx+n] = y_test\n",
    "        pred_concat[trial_save_idx:trial_save_idx+n] = y_test_predicted.reshape(len(y_test_predicted),1)\n",
    "        trial_save_idx += n\n",
    "    sses =get_sses_pred(true_concat,pred_concat)\n",
    "    sses_mean=get_sses_mean(true_concat)\n",
    "    R2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "    r2_cv_list_x[i] = R2\n",
    "    X_proc = X_proc - calc_proj(X_proc,weights.T).T\n",
    "    # X_proc = X_proc - calc_proj_sparse(X_proc,weights.T).T\n",
    "\n",
    "X_proc = X\n",
    "y = np.array(sin_y).reshape((n_trials,-1,1))[:,0,:]\n",
    "axes_list_y = nans([N,y.shape[1],dim])\n",
    "r2_cv_list_y = nans([N])\n",
    "r2_list_y = nans([N])\n",
    "for i in range(N):\n",
    "    reg = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    # reg = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    axes_list_y[i,:,:] = reg.best_estimator_.coef_\n",
    "    r2_list_y[i] = reg.best_estimator_.score(X_proc, y)\n",
    "    weights = reg.best_estimator_.coef_\n",
    "    skf = StratifiedKFold(n_splits=10,shuffle=True,random_state = 42)   \n",
    "    true_concat = nans([n_trials,1])\n",
    "    pred_concat = nans([n_trials,1])\n",
    "    trial_save_idx = 0\n",
    "    for training_set, test_set in skf.split(range(0,n_trials),cond_dict):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test = X_proc[training_set,:],X_proc[test_set,:]\n",
    "        y_train, y_test = y[training_set],y[test_set]\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        # lr = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_test_predicted = lr.predict(X_test)\n",
    "\n",
    "        n = y_test_predicted.shape[0]\n",
    "        true_concat[trial_save_idx:trial_save_idx+n] = y_test\n",
    "        pred_concat[trial_save_idx:trial_save_idx+n] = y_test_predicted.reshape(len(y_test_predicted),1)\n",
    "        trial_save_idx += n\n",
    "    sses =get_sses_pred(true_concat,pred_concat)\n",
    "    sses_mean=get_sses_mean(true_concat)\n",
    "    R2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "    r2_cv_list_y[i] = R2\n",
    "    X_proc = X_proc - calc_proj(X_proc,weights.T).T\n",
    "    # X_proc = X_proc - calc_proj_sparse(X_proc,weights.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(N)+1,r2_cv_list_x)\n",
    "plt.xlabel('N')\n",
    "plt.title('x-dir')\n",
    "plt.ylabel('R2')\n",
    "print(r2_cv_list_x)\n",
    "plt.show()\n",
    "plt.plot(np.arange(N)+1,r2_cv_list_y)\n",
    "plt.xlabel('N')\n",
    "plt.title('y-dir')\n",
    "plt.ylabel('R2')\n",
    "print(r2_cv_list_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.0\n",
    "print(axes_list_x.squeeze()[r2_cv_list_x>thresh,:].shape)\n",
    "print(axes_list_y.squeeze()[r2_cv_list_y>thresh,:].shape)\n",
    "CD_axes = np.vstack((axes_list_x.squeeze()[r2_cv_list_x>thresh,:],axes_list_y.squeeze()[r2_cv_list_y>thresh,:]))\n",
    "CD_axes.shape\n",
    "\n",
    "all_data = dataset_10ms.data[x_field].to_numpy()\n",
    "proj_data = all_data @ CD_axes.T\n",
    "print(proj_data.shape)\n",
    "\n",
    "dataset_10ms.add_continuous_data(proj_data,'v6_zscore_unsmoothed_CD_proj_'+x_field)\n",
    "\n",
    "# reg = 'Lasso'\n",
    "# dataset_10ms.add_continuous_data(proj_data,'CD_proj_'+x_field+reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find FB axes\n",
    "proj_out_CD = True\n",
    "align_range = (200, 400)\n",
    "align_field = 'move_onset_time'\n",
    "cond_dict = active_cond_dict\n",
    "\n",
    "# proj_out_CD = False\n",
    "# align_range = (-100, 0)\n",
    "# align_field = 'move_offset_time'\n",
    "# cond_dict = active_cond_dict_offset\n",
    "\n",
    "mask = active_mask\n",
    "active_df = dataset.make_trial_data(align_field=align_field, align_range=align_range, ignored_trials=~mask)\n",
    "n_trials = active_df['trial_id'].nunique()\n",
    "X = active_df[x_field].to_numpy().reshape((-1, dim))\n",
    "if z_score:\n",
    "    print('z_scored')\n",
    "    X = (X - mean)/std\n",
    "act_trial_spikes = X.reshape((n_trials, -1, dim))\n",
    "print(act_trial_spikes.shape)\n",
    "act_trial_mean_activity = np.mean(act_trial_spikes, axis=1)\n",
    "\n",
    "dirs = [dataset.trial_info[dataset.trial_info.trial_id == i]['cond_dir'] for i in active_df.trial_id]\n",
    "cos_x = [round(math.cos(math.radians(i)),3) for i in dirs]\n",
    "sin_y = [round(math.sin(math.radians(i)),3) for i in dirs]\n",
    "cos_sin = np.array([cos_x, sin_y]).T\n",
    "print(cos_sin.shape)\n",
    "\n",
    "# dirs = [dataset.trial_info[dataset.trial_info.trial_id == i]['cond_dir'] for i in active_df.trial_id]\n",
    "# filter = np.array([x%90==0 for x in dirs])\n",
    "# n_trials = len(filter)\n",
    "# act_trial_mean_activity = act_trial_mean_activity[filter[0::act_trial_spikes.shape[1]].squeeze(),:]\n",
    "# print(act_trial_mean_activity.shape)\n",
    "\n",
    "# cos_x = np.array([round(math.cos(math.radians(i)),3) for i in dirs])[filter.squeeze()]\n",
    "# sin_y = np.array([round(math.sin(math.radians(i)),3) for i in dirs])[filter.squeeze()]\n",
    "# cos_sin = np.array([cos_x, sin_y]).T\n",
    "# print(cos_sin.shape)\n",
    "\n",
    "# n_trials = act_trial_mean_activity.shape[0]\n",
    "# cond_dict = cond_dict[filter[0::act_trial_spikes.shape[1]].squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "X = act_trial_mean_activity\n",
    "if proj_out_CD:\n",
    "    X =  X - calc_proj(X,CD_axes.T).T\n",
    "\n",
    "y=np.array(cos_x).reshape((n_trials,-1,1))[:,0,:]\n",
    "X_proc = X\n",
    "axes_list_x = nans([N,y.shape[1],dim])\n",
    "r2_cv_list_x = nans([N])\n",
    "r2_list_x = nans([N])\n",
    "for i in range(N):\n",
    "    reg = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    # reg = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    axes_list_x[i,:,:] = reg.best_estimator_.coef_\n",
    "    r2_list_x[i] = reg.best_estimator_.score(X_proc, y)\n",
    "    weights = reg.best_estimator_.coef_\n",
    "    skf = StratifiedKFold(n_splits=10,shuffle=True,random_state = 42)   \n",
    "    true_concat = nans([n_trials,1])\n",
    "    pred_concat = nans([n_trials,1])\n",
    "    trial_save_idx = 0\n",
    "    for training_set, test_set in skf.split(range(0,n_trials),cond_dict):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test = X_proc[training_set,:],X_proc[test_set,:]\n",
    "        y_train, y_test = y[training_set],y[test_set]\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        # lr = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_test_predicted = lr.predict(X_test)\n",
    "\n",
    "        n = y_test_predicted.shape[0]\n",
    "        true_concat[trial_save_idx:trial_save_idx+n] = y_test\n",
    "        pred_concat[trial_save_idx:trial_save_idx+n] = y_test_predicted.reshape(len(y_test_predicted),1)\n",
    "        trial_save_idx += n\n",
    "    sses =get_sses_pred(true_concat,pred_concat)\n",
    "    sses_mean=get_sses_mean(true_concat)\n",
    "    R2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "    r2_cv_list_x[i] = R2\n",
    "    X_proc = X_proc - calc_proj(X_proc,weights.T).T\n",
    "\n",
    "X_proc = X\n",
    "y = np.array(sin_y).reshape((n_trials,-1,1))[:,0,:]\n",
    "axes_list_y = nans([N,y.shape[1],dim])\n",
    "r2_cv_list_y = nans([N])\n",
    "r2_list_y = nans([N])\n",
    "for i in range(N):\n",
    "    reg = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    # reg = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    axes_list_y[i,:,:] = reg.best_estimator_.coef_\n",
    "    r2_list_y[i] = reg.best_estimator_.score(X_proc, y)\n",
    "    weights = reg.best_estimator_.coef_\n",
    "    skf = StratifiedKFold(n_splits=10,shuffle=True,random_state = 42)   \n",
    "    true_concat = nans([n_trials,1])\n",
    "    pred_concat = nans([n_trials,1])\n",
    "    trial_save_idx = 0\n",
    "    for training_set, test_set in skf.split(range(0,n_trials),cond_dict):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test = X_proc[training_set,:],X_proc[test_set,:]\n",
    "        y_train, y_test = y[training_set],y[test_set]\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        # lr = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_test_predicted = lr.predict(X_test)\n",
    "\n",
    "        n = y_test_predicted.shape[0]\n",
    "        true_concat[trial_save_idx:trial_save_idx+n] = y_test\n",
    "        pred_concat[trial_save_idx:trial_save_idx+n] = y_test_predicted.reshape(len(y_test_predicted),1)\n",
    "        trial_save_idx += n\n",
    "    sses =get_sses_pred(true_concat,pred_concat)\n",
    "    sses_mean=get_sses_mean(true_concat)\n",
    "    R2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "    r2_cv_list_y[i] = R2\n",
    "    X_proc = X_proc - calc_proj(X_proc,weights.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(N)+1,r2_cv_list_x)\n",
    "plt.xlabel('N')\n",
    "plt.title('x-dir')\n",
    "plt.ylabel('R2')\n",
    "print(r2_cv_list_x)\n",
    "plt.show()\n",
    "plt.plot(np.arange(N)+1,r2_cv_list_y)\n",
    "plt.xlabel('N')\n",
    "plt.title('y-dir')\n",
    "plt.ylabel('R2')\n",
    "print(r2_cv_list_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.0\n",
    "print(axes_list_x.squeeze()[r2_cv_list_x>thresh,:].shape)\n",
    "print(axes_list_y.squeeze()[r2_cv_list_y>thresh,:].shape)\n",
    "FB_axes = np.vstack((axes_list_x.squeeze()[r2_cv_list_x>thresh,:],axes_list_y.squeeze()[r2_cv_list_y>thresh,:]))\n",
    "FB_axes.shape\n",
    "\n",
    "all_data = dataset_10ms.data[x_field].to_numpy()\n",
    "proj_data = all_data @ FB_axes.T\n",
    "print(proj_data.shape)\n",
    "\n",
    "dataset_10ms.add_continuous_data(proj_data,'v6_zscore_unsmoothed_FB_proj_'+x_field)\n",
    "\n",
    "# reg='Lasso'\n",
    "# dataset_10ms.add_continuous_data(proj_data,'FB_proj_'+x_field+reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find FB? axes\n",
    "# proj_out_CDFB = True\n",
    "# align_range = (200, 400)\n",
    "# align_field = 'move_onset_time'\n",
    "# cond_dict = active_cond_dict\n",
    "\n",
    "# mask = active_mask\n",
    "# active_df = dataset.make_trial_data(align_field=align_field, align_range=align_range, ignored_trials=~mask)\n",
    "# n_trials = active_df['trial_id'].nunique()\n",
    "# X = active_df[x_field].to_numpy().reshape((-1, dim))\n",
    "# if z_score:\n",
    "#     print('z_scored')\n",
    "#     X = (X - mean)/std\n",
    "# act_trial_spikes = X.reshape((n_trials, -1, dim))\n",
    "# print(act_trial_spikes.shape)\n",
    "# act_trial_mean_activity = np.mean(act_trial_spikes, axis=1)\n",
    "\n",
    "# dirs = [dataset.trial_info[dataset.trial_info.trial_id == i]['cond_dir'] for i in active_df.trial_id]\n",
    "# cos_x = [round(math.cos(math.radians(i)),3) for i in dirs]\n",
    "# sin_y = [round(math.sin(math.radians(i)),3) for i in dirs]\n",
    "# cos_sin = np.array([cos_x, sin_y]).T\n",
    "# print(cos_sin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 10\n",
    "# X = act_trial_mean_activity\n",
    "# if proj_out_CDFB:\n",
    "#     CDFB_axes = np.vstack([CD_axes, FB_axes])\n",
    "#     X =  X - calc_proj(X,CDFB_axes.T).T\n",
    "\n",
    "# y=np.array(cos_x).reshape((n_trials,-1,1))[:,0,:]\n",
    "# X_proc = X\n",
    "# axes_list_x = nans([N,y.shape[1],dim])\n",
    "# r2_cv_list_x = nans([N])\n",
    "# r2_list_x = nans([N])\n",
    "# for i in range(N):\n",
    "#     reg = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "#     # reg = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "#     axes_list_x[i,:,:] = reg.best_estimator_.coef_\n",
    "#     r2_list_x[i] = reg.best_estimator_.score(X_proc, y)\n",
    "#     weights = reg.best_estimator_.coef_\n",
    "#     skf = StratifiedKFold(n_splits=10,shuffle=True,random_state = 42)   \n",
    "#     true_concat = nans([n_trials,1])\n",
    "#     pred_concat = nans([n_trials,1])\n",
    "#     trial_save_idx = 0\n",
    "#     for training_set, test_set in skf.split(range(0,n_trials),cond_dict):\n",
    "#         #split training and testing by trials\n",
    "#         X_train, X_test = X_proc[training_set,:],X_proc[test_set,:]\n",
    "#         y_train, y_test = y[training_set],y[test_set]\n",
    "#         lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "#         # lr = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "#         lr.fit(X_train, y_train)\n",
    "#         y_test_predicted = lr.predict(X_test)\n",
    "\n",
    "#         n = y_test_predicted.shape[0]\n",
    "#         true_concat[trial_save_idx:trial_save_idx+n] = y_test\n",
    "#         pred_concat[trial_save_idx:trial_save_idx+n] = y_test_predicted.reshape(len(y_test_predicted),1)\n",
    "#         trial_save_idx += n\n",
    "#     sses =get_sses_pred(true_concat,pred_concat)\n",
    "#     sses_mean=get_sses_mean(true_concat)\n",
    "#     R2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "#     r2_cv_list_x[i] = R2\n",
    "#     X_proc = X_proc - calc_proj(X_proc,weights.T).T\n",
    "\n",
    "# X_proc = X\n",
    "# y = np.array(sin_y).reshape((n_trials,-1,1))[:,0,:]\n",
    "# axes_list_y = nans([N,y.shape[1],dim])\n",
    "# r2_cv_list_y = nans([N])\n",
    "# r2_list_y = nans([N])\n",
    "# for i in range(N):\n",
    "#     reg = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "#     # reg = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "#     axes_list_y[i,:,:] = reg.best_estimator_.coef_\n",
    "#     r2_list_y[i] = reg.best_estimator_.score(X_proc, y)\n",
    "#     weights = reg.best_estimator_.coef_\n",
    "#     skf = StratifiedKFold(n_splits=10,shuffle=True,random_state = 42)   \n",
    "#     true_concat = nans([n_trials,1])\n",
    "#     pred_concat = nans([n_trials,1])\n",
    "#     trial_save_idx = 0\n",
    "#     for training_set, test_set in skf.split(range(0,n_trials),cond_dict):\n",
    "#         #split training and testing by trials\n",
    "#         X_train, X_test = X_proc[training_set,:],X_proc[test_set,:]\n",
    "#         y_train, y_test = y[training_set],y[test_set]\n",
    "#         lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "#         # lr = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "#         lr.fit(X_train, y_train)\n",
    "#         y_test_predicted = lr.predict(X_test)\n",
    "\n",
    "#         n = y_test_predicted.shape[0]\n",
    "#         true_concat[trial_save_idx:trial_save_idx+n] = y_test\n",
    "#         pred_concat[trial_save_idx:trial_save_idx+n] = y_test_predicted.reshape(len(y_test_predicted),1)\n",
    "#         trial_save_idx += n\n",
    "#     sses =get_sses_pred(true_concat,pred_concat)\n",
    "#     sses_mean=get_sses_mean(true_concat)\n",
    "#     R2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "#     r2_cv_list_y[i] = R2\n",
    "#     X_proc = X_proc - calc_proj(X_proc,weights.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.arange(N)+1,r2_cv_list_x)\n",
    "# plt.xlabel('N')\n",
    "# plt.title('x-dir')\n",
    "# plt.ylabel('R2')\n",
    "# print(r2_cv_list_x)\n",
    "# plt.show()\n",
    "# plt.plot(np.arange(N)+1,r2_cv_list_y)\n",
    "# plt.xlabel('N')\n",
    "# plt.title('y-dir')\n",
    "# plt.ylabel('R2')\n",
    "# print(r2_cv_list_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresh = 0.0\n",
    "# print(axes_list_x.squeeze()[r2_cv_list_x>thresh,:].shape)\n",
    "# print(axes_list_y.squeeze()[r2_cv_list_y>thresh,:].shape)\n",
    "# FBq_axes = np.vstack((axes_list_x.squeeze()[r2_cv_list_x>thresh,:],axes_list_y.squeeze()[r2_cv_list_y>thresh,:]))\n",
    "# FBq_axes.shape\n",
    "\n",
    "\n",
    "# all_data = dataset_10ms.data[x_field].to_numpy()\n",
    "# proj_data = all_data @ FBq_axes.T\n",
    "# print(proj_data.shape)\n",
    "\n",
    "# dataset_10ms.add_continuous_data(proj_data,'v6_alt_zscore_unsmoothed100_FB?_proj_'+x_field)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.data.keys().unique(0))\n",
    "CD_proj = np.array(dataset_10ms.data['v6_zscore_unsmoothed_CD_proj_'+x_field])\n",
    "print(CD_proj.shape)\n",
    "FB_proj = np.array(dataset_10ms.data['v6_zscore_unsmoothed_FB_proj_'+x_field])\n",
    "print(FB_proj.shape)\n",
    "CD_FB_proj  = np.hstack([CD_proj,FB_proj])\n",
    "print(CD_FB_proj.shape)\n",
    "dataset_10ms.add_continuous_data(CD_FB_proj,'v6_zscore_unsmoothed_CD_FB_proj_'+x_field)\n",
    "\n",
    "print(dataset.data.keys().unique(0))\n",
    "\n",
    "# # all_data = dataset_10ms.data[x_field].to_numpy()\n",
    "# # CD_proj_out_data = all_data - calc_proj(all_data,CD_axes.T).T\n",
    "# # print(CD_proj_out_data.shape)\n",
    "# # dataset_10ms.add_continuous_data(CD_proj_out_data,'CD_proj_out_'+x_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CD_axes_Lasso = CD_axes\n",
    "# FB_axes_Lasso = FB_axes\n",
    "\n",
    "# CD_axes_Ridge = CD_axes\n",
    "# FB_axes_Ridge = FB_axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(monkey+'_v6_zscore_unsmoothed_cdfb_weights_'+x_field, CD_axes = CD_axes, FB_axes = FB_axes) \n",
    "np.savez(monkey+'_v6_zscore_unsmoothed_cdfb_data_'+x_field, \\\n",
    "        CD_FB_proj = dataset.data['v6_zscore_unsmoothed_CD_FB_proj_'+x_field].to_numpy(), \\\n",
    "        FB_proj = dataset.data['v6_zscore_unsmoothed_FB_proj_'+x_field].to_numpy(),\n",
    "        CD_proj = dataset.data['v6_zscore_unsmoothed_CD_proj_'+x_field].to_numpy())\n",
    "\n",
    "# np.savez(monkey+'_v6_qsignal_zscore_unsmoothed_cdfb_weights_'+x_field, CD_axes = CD_axes, FB_axes = FB_axes, FBq_axes = FBq_axes) \n",
    "# np.savez(monkey+'_v6_qsignal_zscore_unsmoothed_cdfb_data_'+x_field, \\\n",
    "#         FBq_proj = dataset.data['v6_alt_zscore_unsmoothed100_FB?_proj_'+x_field].to_numpy(), \\\n",
    "#         FB_proj = dataset.data['v6_alt_zscore_unsmoothed_FB_proj_'+x_field].to_numpy(),\n",
    "#         CD_proj = dataset.data['v6_alt_zscore_unsmoothed_CD_proj_'+x_field].to_numpy(),\n",
    "#         CD_FB_proj = dataset.data['v6_alt_zscore_unsmoothed_CD_FB_proj_'+x_field].to_numpy())\n",
    "\n",
    "# np.savez(monkey+'_cdfb_weights_'+x_field, CD_axes_Lasso = CD_axes_Lasso, FB_axes_Lasso = FB_axes_Lasso,\\\n",
    "#          CD_axes_Ridge = CD_axes_Ridge, FB_axes_Ridge = FB_axes_Ridge) \n",
    "# np.savez(monkey+'_cdfb_data_'+x_field, \\\n",
    "#         CD_FB_proj_Lasso = dataset.data['CD_FB_proj_'+x_field+'Lasso'].to_numpy(), \\\n",
    "#         FB_proj_Lasso = dataset.data['FB_proj_'+x_field+'Lasso'].to_numpy(),\\\n",
    "#         CD_proj_Lasso = dataset.data['CD_proj_'+x_field+'Lasso'].to_numpy(),\\\n",
    "#         CD_FB_proj_Ridge = dataset.data['CD_FB_proj_'+x_field+'Ridge'].to_numpy(), \\\n",
    "#         FB_proj_Ridge = dataset.data['FB_proj_'+x_field+'Ridge'].to_numpy(),\n",
    "#         CD_proj_Ridge = dataset.data['CD_proj_'+x_field+'Ridge'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = dataset_10ms.data['spikes_smth_40'].to_numpy()\n",
    "# CD_proj = all_data @ CD_axes.T\n",
    "# print(CD_proj.shape)\n",
    "# dataset_10ms.add_continuous_data(CD_proj,'v6_smoothed100_CD_proj_'+x_field)\n",
    "\n",
    "# FB_proj = all_data @ FB_axes.T\n",
    "# print(FB_proj.shape)\n",
    "# dataset_10ms.add_continuous_data(FB_proj,'v6_smoothed100_FB_proj_'+x_field)\n",
    "\n",
    "# FBq_proj = all_data @ FBq_axes.T\n",
    "# print(FBq_proj.shape)\n",
    "# dataset_10ms.add_continuous_data(FBq_proj,'v6_smoothed100_FB?_proj_'+x_field)\n",
    "\n",
    "# CD_FB_proj  = np.hstack([CD_proj,FB_proj])\n",
    "# print(CD_FB_proj.shape)\n",
    "# dataset_10ms.add_continuous_data(CD_FB_proj,'v6_smoothed100_CD_FB_proj_'+x_field)\n",
    "\n",
    "# np.savez(monkey+'_v6_qsignal_smoothed100_cdfb_data_'+x_field, \\\n",
    "#         CD_FB_proj = dataset_10ms.data['v6_smoothed100_CD_FB_proj_'+x_field].to_numpy(), \\\n",
    "#         FB_proj = dataset_10ms.data['v6_smoothed100_FB_proj_'+x_field].to_numpy(),\n",
    "#         CD_proj = dataset_10ms.data['v6_smoothed100_CD_proj_'+x_field].to_numpy(),\n",
    "#         FBq_proj = dataset_10ms.data['v6_smoothed100_FB?_proj_'+x_field].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(monkey+'_v6_alt_zscore_unsmoothed100_cdfb_weights_'+x_field+'.npz')\n",
    "data.files\n",
    "angles = principal_angles(data['CD_axes'].T, data['FB_axes'].T)\n",
    "print(\"Principal angles\", np.degrees(angles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey = 'Duncan_20190710'\n",
    "x_field = 'spikes'\n",
    "data1 = np.load(monkey+'_v6_alt_zscore_unsmoothed_cdfb_weights_'+x_field+'.npz')\n",
    "data2 = np.load(monkey+'_v6_zscore_unsmoothed_cdfb_weights_'+x_field+'.npz')\n",
    "angles = principal_angles(data1['FB_axes'].T, data2['FB_axes'].T)\n",
    "print(np.degrees(principal_angles(data1['CD_axes'].T, data1['FB_axes'].T)))\n",
    "print(np.degrees(principal_angles(data2['CD_axes'].T, data2['FB_axes'].T)))\n",
    "print(np.degrees(principal_angles(data1['CD_axes'].T, data2['CD_axes'].T)))\n",
    "\n",
    "print(\"Principal angles\", np.degrees(angles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey = \"Han_20171207\"\n",
    "x_field = 'spikes'\n",
    "data1 = np.load(monkey+'_v6_alt_zscore_unsmoothed_cdfb_weights_'+x_field+'.npz')\n",
    "data2 = np.load(monkey+'_v6_zscore_unsmoothed_cdfb_weights_'+x_field+'.npz')\n",
    "print(np.degrees(principal_angles(data1['CD_axes'].T, data1['FB_axes'].T)))\n",
    "print(np.degrees(principal_angles(data2['CD_axes'].T, data2['FB_axes'].T)))\n",
    "print(np.degrees(principal_angles(data1['CD_axes'].T, data2['CD_axes'].T)))\n",
    "\n",
    "angles = principal_angles(data1['FB_axes'].T, data2['FB_axes'].T)\n",
    "print(\"Principal angles\", np.degrees(angles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FB_axes[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['FB_axes'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['FB_axes'][2].T,color='red')\n",
    "plt.plot(FB_axes[2].T,color='k')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['CD_axes'].T,color='red')\n",
    "plt.plot(CD_axes.T,color='k')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = CD_axes.T\n",
    "Y = FB_axes.T\n",
    "angles = principal_angles(X, Y)\n",
    "print(\"Principal angles\", np.degrees(angles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FB_axes_plot = FB_axes\n",
    "angDist_array = nans([len(CD_axes),len(FB_axes_plot)])\n",
    "for i in range(len(CD_axes)):\n",
    "    for j in range(len(FB_axes_plot)):\n",
    "        angDist_array[i,j] = math.degrees(angle_between(CD_axes[i,:],FB_axes_plot[j,:]))\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "im = ax.imshow(angDist_array)\n",
    "ax.set_xlabel('Feedback axes')\n",
    "ax.set_ylabel('CD axes')\n",
    "ax.set_xticks(np.arange(len(FB_axes_plot)))\n",
    "ax.set_yticks(np.arange(len(CD_axes)))\n",
    "\n",
    "for i in range(len(CD_axes)):\n",
    "    for j in range(len(FB_axes_plot)):\n",
    "        text = ax.text(j, i, str(int(angDist_array[i, j])),\n",
    "                        ha=\"center\", va=\"center\", color=\"w\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_cdfb_degrees_pc.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_10ms\n",
    "x_field = 'spikes'\n",
    "dim = dataset.data[x_field].shape[1]\n",
    "align_range = (-100, 1500)\n",
    "align_field = 'move_onset_time'\n",
    "mask = active_mask\n",
    "active_df = dataset.make_trial_data(align_field=align_field, align_range=align_range, ignored_trials=~mask)\n",
    "n_trials = active_n_trials\n",
    "X = active_df[x_field].to_numpy().reshape((-1, dim))\n",
    "print(X.shape)\n",
    "mean = np.nanmean(X,axis=0)\n",
    "std = np.nanstd(X,axis=0)\n",
    "mean.shape\n",
    "#CD\n",
    "align_range = (-100,0)\n",
    "align_field = 'move_onset_time'\n",
    "mask = active_mask\n",
    "active_df = dataset.make_trial_data(align_field=align_field, align_range=align_range, ignored_trials=~mask)\n",
    "cond_dict = active_cond_dict\n",
    "n_trials = active_n_trials\n",
    "X = active_df[x_field].to_numpy().reshape((-1, dim))\n",
    "X = (X - mean)/std\n",
    "act_trial_spikes = X.reshape((n_trials, -1, dim))\n",
    "print(act_trial_spikes.shape)\n",
    "act_trial_mean_activity = np.mean(act_trial_spikes, axis=1)\n",
    "\n",
    "\n",
    "dirs = [dataset.trial_info[dataset.trial_info.trial_id == i]['cond_dir'] for i in active_df.trial_id]\n",
    "cos_x = np.array([round(math.cos(math.radians(i)),3) for i in dirs])\n",
    "sin_y = np.array([round(math.sin(math.radians(i)),3) for i in dirs])\n",
    "cos_sin = np.array([cos_x, sin_y]).T\n",
    "print(cos_sin.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "X = act_trial_mean_activity\n",
    "\n",
    "y=np.array(cos_x).reshape((n_trials,-1,1))[:,0,:]\n",
    "X_proc = X\n",
    "axes_list_x = nans([N,y.shape[1],dim])\n",
    "r2_cv_list_x = nans([N])\n",
    "r2_list_x = nans([N])\n",
    "for i in range(N):\n",
    "    reg = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    # reg = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    axes_list_x[i,:,:] = reg.best_estimator_.coef_\n",
    "    r2_list_x[i] = reg.best_estimator_.score(X_proc, y)\n",
    "    weights = reg.best_estimator_.coef_\n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True,random_state = 42)   \n",
    "    true_concat = nans([n_trials,1])\n",
    "    pred_concat = nans([n_trials,1])\n",
    "    trial_save_idx = 0\n",
    "    for training_set, test_set in skf.split(range(0,n_trials),cond_dict):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test = X_proc[training_set,:],X_proc[test_set,:]\n",
    "        y_train, y_test = y[training_set],y[test_set]\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        # lr = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_test_predicted = lr.predict(X_test)\n",
    "\n",
    "        n = y_test_predicted.shape[0]\n",
    "        true_concat[trial_save_idx:trial_save_idx+n] = y_test\n",
    "        pred_concat[trial_save_idx:trial_save_idx+n] = y_test_predicted.reshape(len(y_test_predicted),1)\n",
    "        trial_save_idx += n\n",
    "    sses =get_sses_pred(true_concat,pred_concat)\n",
    "    sses_mean=get_sses_mean(true_concat)\n",
    "    R2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "    r2_cv_list_x[i] = R2\n",
    "    X_proc = X_proc - calc_proj(X_proc,weights.T).T\n",
    "    # X_proc = X_proc - calc_proj_sparse(X_proc,weights.T).T\n",
    "\n",
    "X_proc = X\n",
    "y = np.array(sin_y).reshape((n_trials,-1,1))[:,0,:]\n",
    "axes_list_y = nans([N,y.shape[1],dim])\n",
    "r2_cv_list_y = nans([N])\n",
    "r2_list_y = nans([N])\n",
    "for i in range(N):\n",
    "    reg = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    # reg = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    axes_list_y[i,:,:] = reg.best_estimator_.coef_\n",
    "    r2_list_y[i] = reg.best_estimator_.score(X_proc, y)\n",
    "    weights = reg.best_estimator_.coef_\n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True,random_state = 42)   \n",
    "    true_concat = nans([n_trials,1])\n",
    "    pred_concat = nans([n_trials,1])\n",
    "    trial_save_idx = 0\n",
    "    for training_set, test_set in skf.split(range(0,n_trials),cond_dict):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test = X_proc[training_set,:],X_proc[test_set,:]\n",
    "        y_train, y_test = y[training_set],y[test_set]\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        # lr = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_test_predicted = lr.predict(X_test)\n",
    "\n",
    "        n = y_test_predicted.shape[0]\n",
    "        true_concat[trial_save_idx:trial_save_idx+n] = y_test\n",
    "        pred_concat[trial_save_idx:trial_save_idx+n] = y_test_predicted.reshape(len(y_test_predicted),1)\n",
    "        trial_save_idx += n\n",
    "    sses =get_sses_pred(true_concat,pred_concat)\n",
    "    sses_mean=get_sses_mean(true_concat)\n",
    "    R2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "    r2_cv_list_y[i] = R2\n",
    "    X_proc = X_proc - calc_proj(X_proc,weights.T).T\n",
    "thresh = 0.0\n",
    "print(axes_list_x.squeeze()[r2_cv_list_x>thresh,:].shape)\n",
    "print(axes_list_y.squeeze()[r2_cv_list_y>thresh,:].shape)\n",
    "CD_axes = np.vstack((axes_list_x.squeeze()[r2_cv_list_x>thresh,:],axes_list_y.squeeze()[r2_cv_list_y>thresh,:]))\n",
    "CD_axes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FB_axes_list = []\n",
    "\n",
    "# align_range_start = np.arange(-100, 1500,100)\n",
    "# align_range_end = np.arange(0, 1600,100)\n",
    "\n",
    "align_range_start = np.arange(-1600, 0, 100)\n",
    "align_range_end = np.arange(-1500, 100,100)\n",
    "\n",
    "\n",
    "for i in range(len(align_range_start)):\n",
    "    align_field = 'move_offset_time'\n",
    "    mask = active_mask\n",
    "    align_range = (align_range_start[i], align_range_end[i])\n",
    "    active_df = dataset.make_trial_data(align_field=align_field, align_range=align_range, ignored_trials=~mask)\n",
    "    cond_dict = active_cond_dict\n",
    "    n_trials = active_df['trial_id'].nunique()\n",
    "    X = active_df[x_field].to_numpy().reshape((-1, dim))\n",
    "    X = (X - mean)/std\n",
    "    act_trial_spikes = X.reshape((n_trials, -1, dim))\n",
    "    act_trial_mean_activity = np.mean(act_trial_spikes, axis=1)\n",
    "\n",
    "    dirs = [dataset.trial_info[dataset.trial_info.trial_id == i]['cond_dir'] for i in active_df.trial_id]\n",
    "    cos_x = [round(math.cos(math.radians(i)),3) for i in dirs]\n",
    "    sin_y = [round(math.sin(math.radians(i)),3) for i in dirs]\n",
    "    cos_sin = np.array([cos_x, sin_y]).T\n",
    "    print(cos_sin.shape)\n",
    "\n",
    "    N = 10\n",
    "    X = act_trial_mean_activity\n",
    "    y=np.array(cos_x).reshape((n_trials,-1,1))[:,0,:]\n",
    "    X_proc = X\n",
    "    axes_list_x = nans([N,y.shape[1],dim])\n",
    "    r2_cv_list_x = nans([N])\n",
    "    r2_list_x = nans([N])\n",
    "    for i in range(N):\n",
    "        reg = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "        # reg = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "        axes_list_x[i,:,:] = reg.best_estimator_.coef_\n",
    "        r2_list_x[i] = reg.best_estimator_.score(X_proc, y)\n",
    "        weights = reg.best_estimator_.coef_\n",
    "        skf = StratifiedKFold(n_splits=3,shuffle=True,random_state = 42)   \n",
    "        true_concat = nans([n_trials,1])\n",
    "        pred_concat = nans([n_trials,1])\n",
    "        trial_save_idx = 0\n",
    "        for training_set, test_set in skf.split(range(0,n_trials),cond_dict):\n",
    "            #split training and testing by trials\n",
    "            X_train, X_test = X_proc[training_set,:],X_proc[test_set,:]\n",
    "            y_train, y_test = y[training_set],y[test_set]\n",
    "            lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "            # lr = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "            lr.fit(X_train, y_train)\n",
    "            y_test_predicted = lr.predict(X_test)\n",
    "\n",
    "            n = y_test_predicted.shape[0]\n",
    "            true_concat[trial_save_idx:trial_save_idx+n] = y_test\n",
    "            pred_concat[trial_save_idx:trial_save_idx+n] = y_test_predicted.reshape(len(y_test_predicted),1)\n",
    "            trial_save_idx += n\n",
    "        sses =get_sses_pred(true_concat,pred_concat)\n",
    "        sses_mean=get_sses_mean(true_concat)\n",
    "        R2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "        r2_cv_list_x[i] = R2\n",
    "        X_proc = X_proc - calc_proj(X_proc,weights.T).T\n",
    "\n",
    "    X_proc = X\n",
    "    y = np.array(sin_y).reshape((n_trials,-1,1))[:,0,:]\n",
    "    axes_list_y = nans([N,y.shape[1],dim])\n",
    "    r2_cv_list_y = nans([N])\n",
    "    r2_list_y = nans([N])\n",
    "    for i in range(N):\n",
    "        reg = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "        # reg = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "        axes_list_y[i,:,:] = reg.best_estimator_.coef_\n",
    "        r2_list_y[i] = reg.best_estimator_.score(X_proc, y)\n",
    "        weights = reg.best_estimator_.coef_\n",
    "        skf = StratifiedKFold(n_splits=3,shuffle=True,random_state = 42)   \n",
    "        true_concat = nans([n_trials,1])\n",
    "        pred_concat = nans([n_trials,1])\n",
    "        trial_save_idx = 0\n",
    "        for training_set, test_set in skf.split(range(0,n_trials),cond_dict):\n",
    "            #split training and testing by trials\n",
    "            X_train, X_test = X_proc[training_set,:],X_proc[test_set,:]\n",
    "            y_train, y_test = y[training_set],y[test_set]\n",
    "            lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "            # lr = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "            lr.fit(X_train, y_train)\n",
    "            y_test_predicted = lr.predict(X_test)\n",
    "\n",
    "            n = y_test_predicted.shape[0]\n",
    "            true_concat[trial_save_idx:trial_save_idx+n] = y_test\n",
    "            pred_concat[trial_save_idx:trial_save_idx+n] = y_test_predicted.reshape(len(y_test_predicted),1)\n",
    "            trial_save_idx += n\n",
    "        sses =get_sses_pred(true_concat,pred_concat)\n",
    "        sses_mean=get_sses_mean(true_concat)\n",
    "        R2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "        r2_cv_list_y[i] = R2\n",
    "        X_proc = X_proc - calc_proj(X_proc,weights.T).T\n",
    "    thresh = 0.0\n",
    "    print(axes_list_x.squeeze()[r2_cv_list_x>thresh,:].shape)\n",
    "    print(axes_list_y.squeeze()[r2_cv_list_y>thresh,:].shape)\n",
    "    FB_axes = np.vstack((axes_list_x.squeeze()[r2_cv_list_x>thresh,:],axes_list_y.squeeze()[r2_cv_list_y>thresh,:]))\n",
    "    print(FB_axes.shape)\n",
    "    FB_axes_list.append(FB_axes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_angles_list = []\n",
    "for i in range(len(align_range_start)):\n",
    "    FB_axes = FB_axes_list[i]\n",
    "    X = CD_axes.T\n",
    "    Y = FB_axes.T\n",
    "    angles = principal_angles(X, Y)\n",
    "    print(\"Principal angles\", np.degrees(angles))\n",
    "    principal_angles_list.append(np.degrees(angles)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(align_range_start, principal_angles_list,\"-o\",color = 'k')\n",
    "plt.xlim([-200, 1600])\n",
    "# plt.xlim([-1700, 0])\n",
    "plt.ylim(([-5, 80]))\n",
    "plt.axhline([75.37946756],linestyle='--')\n",
    "plt.xlabel('Time after move onset (ms)')\n",
    "plt.ylabel('Principal angle (deg)')\n",
    "plt.title('z-score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FB_axes_plot = FB_axes\n",
    "# angDist_array = nans([len(FB_axes_plot),len(FB_axes_plot)])\n",
    "# for i in range(len(FB_axes_plot)):\n",
    "#     for j in range(len(FB_axes_plot)):\n",
    "#         angDist_array[i,j] = math.degrees(angle_between(FB_axes_plot[i,:],FB_axes_plot[j,:]))\n",
    "# fig, ax = plt.subplots(figsize=(6, 6))\n",
    "# im = ax.imshow(angDist_array)\n",
    "# ax.set_xlabel('FB axes')\n",
    "# ax.set_ylabel('FB axes')\n",
    "# ax.set_xticks(np.arange(len(FB_axes_plot)))\n",
    "# ax.set_yticks(np.arange(len(FB_axes_plot)))\n",
    "# for i in range(len(FB_axes_plot)):\n",
    "#     for j in range(len(FB_axes_plot)):\n",
    "#         text = ax.text(j, i, str(int(angDist_array[i, j])),\n",
    "#                         ha=\"center\", va=\"center\", color=\"w\", fontsize=14)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# angDist_array = nans([len(CD_axes),len(CD_axes)])\n",
    "# for i in range(len(CD_axes)):\n",
    "#     for j in range(len(CD_axes)):\n",
    "#         angDist_array[i,j] = math.degrees(angle_between(CD_axes[i,:],CD_axes[j,:]))\n",
    "# fig, ax = plt.subplots(figsize=(4, 4))\n",
    "# im = ax.imshow(angDist_array)\n",
    "# ax.set_xlabel('CD axes')\n",
    "# ax.set_ylabel('CD axes')\n",
    "# ax.set_xticks(np.arange(len(CD_axes)))\n",
    "# ax.set_yticks(np.arange(len(CD_axes)))\n",
    "# for i in range(len(CD_axes)):\n",
    "#     for j in range(len(CD_axes)):\n",
    "#         text = ax.text(j, i, str(int(angDist_array[i, j])),\n",
    "#                         ha=\"center\", va=\"center\", color=\"w\", fontsize=14)\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_field = 'spikes'\n",
    "# data = np.load(monkey+'_v6_unsmoothed100_cdfb_weights_'+x_field+'.npz')\n",
    "# data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = dataset_10ms.data['spikes_smth_40'].to_numpy()\n",
    "# CD_proj = all_data @ data['CD_axes'].T\n",
    "# # CD_proj = all_data @ CD_axes.T\n",
    "# print(CD_proj.shape)\n",
    "# dataset_10ms.add_continuous_data(CD_proj,'v6_smoothed100_CD_proj_'+x_field)\n",
    "\n",
    "# FB_proj = all_data @ data['FB_axes'].T\n",
    "# print(FB_proj.shape)\n",
    "# dataset_10ms.add_continuous_data(FB_proj,'v6_smoothed100_FB_proj_'+x_field)\n",
    "\n",
    "# CD_FB_proj  = np.hstack([CD_proj,FB_proj])\n",
    "# print(CD_FB_proj.shape)\n",
    "# dataset_10ms.add_continuous_data(CD_FB_proj,'v6_smoothed100_CD_FB_proj_'+x_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez(monkey+'_v6_smoothed100_cdfb_data_'+x_field, \\\n",
    "#         CD_FB_proj = dataset_10ms.data['v6_smoothed100_CD_FB_proj_'+x_field].to_numpy(), \\\n",
    "#         FB_proj = dataset_10ms.data['v6_smoothed100_FB_proj_'+x_field].to_numpy(),\n",
    "#         CD_proj = dataset_10ms.data['v6_smoothed100_CD_proj_'+x_field].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CD_proj = dataset_10ms.data['v6_zscore_unsmoothed_CD_proj_spikes'].to_numpy()\n",
    "gaussian_kernel_width = 40 #in ms\n",
    "sigma = int(gaussian_kernel_width/bin_width)\n",
    "data_smoothed = gaussian_filter1d_twoside(CD_proj.astype(np.float64),sigma,axis=0)\n",
    "dataset_10ms.add_continuous_data(data_smoothed,'smoothed_CD_proj_'+x_field)\n",
    "\n",
    "FB_proj = dataset_10ms.data['v6_zscore_unsmoothed_FB_proj_spikes'].to_numpy()\n",
    "data_smoothed = gaussian_filter1d_twoside(FB_proj.astype(np.float64),sigma,axis=0)\n",
    "dataset_10ms.add_continuous_data(data_smoothed,'smoothed_FB_proj_'+x_field)\n",
    "\n",
    "CD_FB_proj = dataset_10ms.data['v6_zscore_unsmoothed_CD_FB_proj_spikes'].to_numpy()\n",
    "data_smoothed = gaussian_filter1d_twoside(CD_FB_proj.astype(np.float64),sigma,axis=0)\n",
    "dataset_10ms.add_continuous_data(data_smoothed,'smoothed_CD_FB_proj_'+x_field)\n",
    "\n",
    "# FBq_proj = dataset_10ms.data['v6_alt_zscore_unsmoothed100_FB?_proj_spikes'].to_numpy()\n",
    "# data_smoothed = gaussian_filter1d_twoside(FBq_proj.astype(np.float64),sigma,axis=0)\n",
    "# dataset_10ms.add_continuous_data(data_smoothed,'smoothed_FBq_proj_'+x_field)\n",
    "\n",
    "print(dataset.data.keys().unique(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez(monkey+'_v6_zscore_smoothed_cdfb_data_'+x_field, \\\n",
    "#         CD_FB_proj = dataset_10ms.data['zscore_smoothed_CD_FB_proj_spikes'].to_numpy(), \\\n",
    "#         FB_proj = dataset_10ms.data['zscore_smoothed_FB_proj_spikes'].to_numpy(),\n",
    "#         CD_proj = dataset_10ms.data['zscore_smoothed_CD_proj_spikes'].to_numpy())\n",
    "\n",
    "np.savez(monkey+'_v6_zscore_smoothed_cdfb_data_'+x_field, \\\n",
    "        CD_FB_proj = dataset_10ms.data['smoothed_CD_FB_proj_spikes'].to_numpy(), \\\n",
    "        FB_proj = dataset_10ms.data['smoothed_FB_proj_spikes'].to_numpy(),\\\n",
    "        CD_proj = dataset_10ms.data['smoothed_CD_proj_spikes'].to_numpy())\n",
    "        # FBq_proj = dataset_10ms.data['smoothed_FBq_proj_spikes'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_10ms\n",
    "x_field = 'spikes'\n",
    "data = np.load(monkey+'_v6_zscore_smoothed_cdfb_data_'+x_field+'.npz')\n",
    "data.files\n",
    "dataset.add_continuous_data(data['CD_FB_proj'],'CD_FB_proj')\n",
    "dataset.add_continuous_data(data['CD_proj'],'CD_proj')\n",
    "dataset.add_continuous_data(data['FB_proj'],'FB_proj')\n",
    "# dataset.add_continuous_data(data['FBq_proj'],'FBq_proj')\n",
    "print(dataset.data.keys().unique(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_field = 'spikes'\n",
    "# data = np.load(monkey+'_v6_smoothed100_cdfb_data_'+x_field+'.npz')\n",
    "# data.files\n",
    "# dataset.add_continuous_data(data['CD_FB_proj'],'smooth_CD_FB_proj')\n",
    "# dataset.add_continuous_data(data['CD_proj'],'smooth_CD_proj')\n",
    "# dataset.add_continuous_data(data['FB_proj'],'smooth_FB_proj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_field = 'spikes'\n",
    "# data = np.load(monkey+'_v6_alt_unsmoothed100_cdfb_data_'+x_field+'.npz')\n",
    "# data.files\n",
    "# dataset.add_continuous_data(data['CD_FB_proj'],'alt_unsmooth_CD_FB_proj')\n",
    "# dataset.add_continuous_data(data['CD_proj'],'alt_unsmooth_CD_proj')\n",
    "# dataset.add_continuous_data(data['FB_proj'],'alt_unsmooth_FB_proj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA projections over trial, for different reaching directions\n",
    "# plot_dir = np.array([0,45,90,135,180,225,270,315]) \n",
    "# directions = np.array([0,45,90,135,180,225,270,315])\n",
    "plot_dir = np.array([0,90,180,270]) \n",
    "directions = np.array([0,90,180,270])\n",
    "cmap = plt.get_cmap('coolwarm',len(plot_dir))\n",
    "custom_palette = [mpl.colors.rgb2hex(cmap(i)) for i in range(len(plot_dir))]\n",
    "plot_field = 'CD_FB_proj'\n",
    "# plot_field = 'zscore_smoothed_CD_FB_proj_spikes'\n",
    "N = dataset_10ms.data[plot_field].shape[1]\n",
    "order = range(N)\n",
    "\n",
    "pred_range = (-200, 1100)\n",
    "trial_mask = active_mask\n",
    "cond_dict = active_cond_dict\n",
    "n_timepoints = int((pred_range[1] - pred_range[0])/dataset_10ms.bin_width)\n",
    "data = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=pred_range, ignored_trials=~trial_mask, allow_overlap=True)\n",
    "n_trials = data['trial_id'].nunique()\n",
    "trials_pca = nans([n_trials,n_timepoints,N])\n",
    "i = 0\n",
    "for idx, trial in data.groupby('trial_id'):\n",
    "    trials_pca[i,:,:]=trial[plot_field].to_numpy()\n",
    "    i+=1\n",
    "print(trials_pca.shape)\n",
    "\n",
    "x_axis = np.arange(pred_range[0], pred_range[1], dataset_10ms.bin_width)\n",
    "\n",
    "# define some useful time points\n",
    "move_idx=0\n",
    "ret_idx = 500\n",
    "\n",
    "plot_dims = N\n",
    "\n",
    "fig,ax=plt.subplots(plot_dims,1,figsize=(10,N+4))\n",
    "for i in range(plot_dims):\n",
    "    for j in range(len(plot_dir)):\n",
    "        color = custom_palette[j]\n",
    "        dir_idx = np.argwhere(directions == plot_dir[j])[0]\n",
    "        cond_mean_proj = np.mean(trials_pca[np.argwhere(cond_dict==dir_idx).flatten(),:,:], axis = 0)[:,order[i]] \n",
    "        pca_mean = np.mean(data[plot_field].to_numpy(),axis = 0)[order[i]] \n",
    "        ax[i].plot(x_axis,cond_mean_proj - pca_mean,linewidth=2.25,color = color,label = plot_dir[j])\n",
    "        \n",
    "        ax[i].axvline(move_idx, color='k',linewidth = 1)\n",
    "        ax[i].axvline(ret_idx, color='k',linewidth = 1)\n",
    "        \n",
    "        ax[i].set_xlim([-200,1000])\n",
    "        # ax[i].set_ylim([-.5, .5])\n",
    "        ax[i].axhline(0,color ='k',ls = '--')\n",
    "        if i<plot_dims-1:\n",
    "            ax[i].set_xticks([])\n",
    "        else:\n",
    "            ax[i].set_xlabel('Time after movement onset (ms)')\n",
    "            \n",
    "        ax[i].set_yticks([])\n",
    "        ax[i].set_ylabel('Dim. '+str(i+1))\n",
    "\n",
    "    ax[0].set_title('Active trials')\n",
    "     \n",
    "plt.legend(bbox_to_anchor = (1, 1), loc = 'upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_zscore_cdfb_active_smooth.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA projections over trial, for different reaching directions\n",
    "pred_range = (-100, 600)\n",
    "trial_mask = passive_mask\n",
    "cond_dict = passive_cond_dict\n",
    "n_timepoints = int((pred_range[1] - pred_range[0])/dataset_10ms.bin_width)\n",
    "data = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=pred_range, ignored_trials=~trial_mask, allow_overlap=True)\n",
    "n_trials = data['trial_id'].nunique()\n",
    "trials_pca = nans([n_trials,n_timepoints,N])\n",
    "i = 0\n",
    "for idx, trial in data.groupby('trial_id'):\n",
    "    trials_pca[i,:,:]=trial[plot_field].to_numpy()\n",
    "    i+=1\n",
    "print(trials_pca.shape)\n",
    "\n",
    "x_axis = np.arange(pred_range[0], pred_range[1], dataset_10ms.bin_width)\n",
    "\n",
    "# define some useful time points\n",
    "move_idx=0\n",
    "ret_idx = 120\n",
    "\n",
    "plot_dims = N\n",
    "\n",
    "fig,ax=plt.subplots(plot_dims,1,figsize=(10,N+4))\n",
    "for i in range(plot_dims):\n",
    "    for j in range(len(plot_dir)):\n",
    "        color = custom_palette[j]\n",
    "        dir_idx = np.argwhere(directions == plot_dir[j])[0]\n",
    "        cond_mean_proj = np.mean(trials_pca[np.argwhere(cond_dict==dir_idx).flatten(),:,:], axis = 0)[:,order[i]] \n",
    "        pca_mean = np.mean(data[plot_field].to_numpy(),axis = 0)[order[i]]\n",
    "        ax[i].plot(x_axis,cond_mean_proj - pca_mean,linewidth=2.25,color = color,label = plot_dir[j])\n",
    "        \n",
    "        ax[i].axvline(move_idx, color='k',linewidth = 1)\n",
    "        # ax[i].axvline(120, color='k',linewidth = 1)\n",
    "        ax[i].axvline(ret_idx, color='k',linewidth = 1)\n",
    "        ax[i].set_xlim([-100,500])\n",
    "        # ax[i].set_ylim([-.5, .5])\n",
    "        ax[i].axhline(0,color ='k',ls = '--')\n",
    "        if i<plot_dims-1:\n",
    "            ax[i].set_xticks([])\n",
    "        else:\n",
    "            ax[i].set_xlabel('Time after movement onset (ms)')\n",
    "\n",
    "        ax[i].set_yticks([])\n",
    "        ax[i].set_ylabel('Dim. '+str(i+1))\n",
    "\n",
    "    ax[0].set_title('Passive trials')\n",
    "\n",
    "plt.legend(bbox_to_anchor = (1, 1), loc = 'upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_zscore_cdfb_passive_smooth.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_cond_dict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA projections over trial, for different reaching directions\n",
    "# pred_range = (-200, 1100)\n",
    "pred_range = (-500, 500)\n",
    "trial_mask = nan_disturb_mask\n",
    "cond_dict = nan_disturb_bump_cond_dict\n",
    "# trial_mask = nan_mask\n",
    "# cond_dict = nan_bump_cond_dict\n",
    "# cond_dict = nan_cond_dict\n",
    "n_timepoints = int((pred_range[1] - pred_range[0])/dataset_10ms.bin_width)\n",
    "data = dataset_10ms.make_trial_data(align_field='bump_time', align_range=pred_range, ignored_trials=~trial_mask, allow_overlap=True)\n",
    "n_trials = data['trial_id'].nunique()\n",
    "trials_pca = nans([n_trials,n_timepoints,N])\n",
    "i = 0\n",
    "for idx, trial in data.groupby('trial_id'):\n",
    "    trials_pca[i,:,:]=trial[plot_field].to_numpy()\n",
    "    i+=1\n",
    "print(trials_pca.shape)\n",
    "\n",
    "x_axis = np.arange(pred_range[0], pred_range[1], dataset_10ms.bin_width)\n",
    "\n",
    "# define some useful time points\n",
    "move_idx=0\n",
    "ret_idx = 200\n",
    "\n",
    "plot_dims = N\n",
    "\n",
    "fig,ax=plt.subplots(plot_dims,1,figsize=(10,N+4))\n",
    "for i in range(plot_dims):\n",
    "    for j in range(len(plot_dir)):\n",
    "        color = custom_palette[j]\n",
    "        dir_idx = np.argwhere(directions == plot_dir[j])[0]\n",
    "        cond_mean_proj = np.mean(trials_pca[np.argwhere(cond_dict==dir_idx).flatten(),:,:], axis = 0)[:,order[i]] \n",
    "        pca_mean = np.mean(data[plot_field].to_numpy(),axis = 0)[order[i]]\n",
    "        ax[i].plot(x_axis,cond_mean_proj - pca_mean,linewidth=2.25,color = color,label = plot_dir[j])\n",
    "        \n",
    "        ax[i].axvline(move_idx, color='k',linewidth = .5)\n",
    "        ax[i].axvline(ret_idx, color='k',linewidth = .5)\n",
    "        ax[i].set_xlim([-500,500])\n",
    "        ax[i].set_ylim([-.5, .5])\n",
    "        ax[i].axhline(0,color ='k',ls = '--')\n",
    "        if i<plot_dims-1:\n",
    "            ax[i].set_xticks([])\n",
    "        else:\n",
    "            ax[i].set_xlabel('Time after bump onset (ms)')\n",
    "            \n",
    "        ax[i].set_yticks([])\n",
    "        ax[i].set_ylabel('Dim. '+str(i+1))\n",
    "\n",
    "    ax[0].set_title('Reach-bump disturb trials')\n",
    "    \n",
    "plt.legend(bbox_to_anchor = (1, 1), loc = 'upper left')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_cdfb_passive_pc.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_field = 'CD_proj_'+x_field\n",
    "x_name = '0000'\n",
    "y_name = '0001'\n",
    "\n",
    "# Active, 2D plot\n",
    "unique_conditions = [(False, 0.0), (False, 45.0), (False, 90.0), (False, 135.0),\n",
    "                     (False, 180.0), (False, 225.0), (False, 270.0), (False, 315.0)]\n",
    "# unique_conditions = [(False, 0.0),  (False, 90.0), (False, 180.0), (False, 270.0)]\n",
    "\n",
    "# Initialize figure\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "ax_0 = fig.add_subplot(1,5,1)\n",
    "ax_1 = fig.add_subplot(1, 5,2)\n",
    "ax_2 = fig.add_subplot(1, 5,3)\n",
    "ax_3 = fig.add_subplot(1, 5,4)\n",
    "ax_4 = fig.add_subplot(1, 5,5)\n",
    "# xlim = [-1.5, 1.5]\n",
    "# ylim = [-1.5, 1.5]\n",
    "\n",
    "for cond in unique_conditions:\n",
    "    # Filter out invalid trials (labeled 'none') and trials in other conditions\n",
    "    cond_mask = (dataset.trial_info['ctr_hold_bump']==cond[0]) & \\\n",
    "                (dataset.trial_info['cond_dir']%360==cond[1]) & \\\n",
    "                (dataset.trial_info.split != 'none')\n",
    "# cond_mask = (np.isnan(dataset.trial_info['ctr_hold_bump'])) & \\\n",
    "\n",
    "    # Extract relevant portion of selected trials\n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(-200, 0), ignored_trials=~cond_mask)\n",
    "    ax_0.plot(cond_data.groupby('align_time').mean()[plot_field][x_name].to_numpy(),cond_data.groupby('align_time').mean()[plot_field][y_name].to_numpy(),color=plt.cm.hsv(cond[1] / 360))\n",
    "    # for idx, trial in cond_data.groupby('trial_id'):\n",
    "    #     ax_0.plot(trial[plot_field][x_name], trial[plot_field][y_name], color=plt.cm.hsv(cond[1] / 360), linewidth=0.5)\n",
    "        # ax_0.set_xlim(xlim)\n",
    "        # ax_0.set_ylim(ylim)\n",
    "\n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(0, 500), ignored_trials=~cond_mask)\n",
    "    ax_1.plot(cond_data.groupby('align_time').mean()[plot_field][x_name].to_numpy(),cond_data.groupby('align_time').mean()[plot_field][y_name].to_numpy(),color=plt.cm.hsv(cond[1] / 360))\n",
    "    # for idx, trial in cond_data.groupby('trial_id'):\n",
    "    #     ax_1.plot(trial[plot_field][x_name], trial[plot_field][y_name], color=plt.cm.hsv(cond[1] / 360), linewidth=0.5)\n",
    "        # ax_1.set_xlim(xlim)\n",
    "        # ax_1.set_ylim(ylim)\n",
    "    \n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(500, 1000), ignored_trials=~cond_mask)\n",
    "    ax_2.plot(cond_data.groupby('align_time').mean()[plot_field][x_name].to_numpy(),cond_data.groupby('align_time').mean()[plot_field][y_name].to_numpy(),color=plt.cm.hsv(cond[1] / 360))\n",
    "    # for idx, trial in cond_data.groupby('trial_id'):\n",
    "    #     ax_2.plot(trial[plot_field][x_name], trial[plot_field][y_name], color=plt.cm.hsv(cond[1] / 360), linewidth=0.5)\n",
    "        # ax_2.set_xlim(xlim)\n",
    "        # ax_2.set_ylim(ylim)\n",
    "            \n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(1000, 1500), ignored_trials=~cond_mask)\n",
    "    ax_3.plot(cond_data.groupby('align_time').mean()[plot_field][x_name].to_numpy(),cond_data.groupby('align_time').mean()[plot_field][y_name].to_numpy(),color=plt.cm.hsv(cond[1] / 360))\n",
    "    # for idx, trial in cond_data.groupby('trial_id'):\n",
    "    #     ax_3.plot(trial[plot_field][x_name], trial[plot_field][y_name], color=plt.cm.hsv(cond[1] / 360), linewidth=0.5)\n",
    "        # ax_3.set_xlim(xlim)\n",
    "        # ax_3.set_ylim(ylim)\n",
    "\n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(1500, 2000), ignored_trials=~cond_mask)\n",
    "    ax_4.plot(cond_data.groupby('align_time').mean()[plot_field][x_name].to_numpy(),cond_data.groupby('align_time').mean()[plot_field][y_name].to_numpy(),color=plt.cm.hsv(cond[1] / 360))\n",
    "    # for idx, trial in cond_data.groupby('trial_id'):\n",
    "    #     ax_4.plot(trial[plot_field][x_name], trial[plot_field][y_name], color=plt.cm.hsv(cond[1] / 360), linewidth=0.5)\n",
    "        # ax_4.set_xlim(xlim)\n",
    "        # ax_4.set_ylim(ylim)\n",
    "            \n",
    "# Add labels\n",
    "ax_0.set_title('-200 to 0')\n",
    "ax_1.set_title('0 to 500')\n",
    "ax_2.set_title('500 to 1000')\n",
    "ax_3.set_title('1000 to 1500')\n",
    "ax_4.set_title('1500 to 2000')\n",
    "\n",
    "# ax_0.axis(\"off\")\n",
    "# ax_1.axis(\"off\")\n",
    "# ax_2.axis(\"off\")\n",
    "# ax_3.axis(\"off\")\n",
    "# ax_4.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_active_traj.pdf',dpi = 'figure')\n",
    "# plt.suptitle('Active Reach Trajectories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passive\n",
    "\n",
    "unique_conditions = [(True, 0.0), (True, 45.0), (True, 90.0), (True, 135.0),\n",
    "                     (True, 180.0), (True, 225.0), (True, 270.0), (True, 315.0)]\n",
    "# unique_conditions = [(True, 0.0),  (True, 90.0), (True, 180.0), (True, 270.0)]\n",
    "\n",
    "# Initialize figure\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax_0 = fig.add_subplot(1,3,1)\n",
    "ax_1 = fig.add_subplot(1, 3,2)\n",
    "ax_2 = fig.add_subplot(1, 3,3)\n",
    "# xlim = [-1.5, 1.5]\n",
    "# ylim = [-1.5, 1.5]\n",
    "\n",
    "for cond in unique_conditions:\n",
    "    # Filter out invalid trials (labeled 'none') and trials in other conditions\n",
    "    cond_mask = (dataset.trial_info['ctr_hold_bump']==cond[0]) & \\\n",
    "                (dataset.trial_info['cond_dir']%360==cond[1]) & \\\n",
    "                (dataset.trial_info.split != 'none')\n",
    "# cond_mask = (np.isnan(dataset.trial_info['ctr_hold_bump'])) & \\\n",
    "\n",
    "    # Extract relevant portion of selected trials\n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(-200, 0), ignored_trials=~cond_mask)\n",
    "    ax_0.plot(cond_data.groupby('align_time').mean()[plot_field][x_name].to_numpy(),cond_data.groupby('align_time').mean()[plot_field][y_name].to_numpy(),color=plt.cm.hsv(cond[1] / 360))\n",
    "    # for idx, trial in cond_data.groupby('trial_id'):\n",
    "    #     ax_0.plot(trial[plot_field][x_name], trial[plot_field][y_name], color=plt.cm.hsv(cond[1] / 360), linewidth=0.5)\n",
    "        # ax_0.set_xlim(xlim)\n",
    "        # ax_0.set_ylim(ylim)\n",
    "    \n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(0, 200), ignored_trials=~cond_mask)\n",
    "    ax_1.plot(cond_data.groupby('align_time').mean()[plot_field][x_name].to_numpy(),cond_data.groupby('align_time').mean()[plot_field][y_name].to_numpy(),color=plt.cm.hsv(cond[1] / 360))\n",
    "    # for idx, trial in cond_data.groupby('trial_id'):\n",
    "    #     ax_1.plot(trial[plot_field][x_name], trial[plot_field][y_name], color=plt.cm.hsv(cond[1] / 360), linewidth=0.5)\n",
    "        # ax_1.set_xlim(xlim)\n",
    "        # ax_1.set_ylim(ylim)\n",
    "\n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(200, 500), ignored_trials=~cond_mask)\n",
    "    ax_2.plot(cond_data.groupby('align_time').mean()[plot_field][x_name].to_numpy(),cond_data.groupby('align_time').mean()[plot_field][y_name].to_numpy(),color=plt.cm.hsv(cond[1] / 360))\n",
    "    # for idx, trial in cond_data.groupby('trial_id'):\n",
    "    #     ax_2.plot(trial[plot_field][x_name], trial[plot_field][y_name], color=plt.cm.hsv(cond[1] / 360), linewidth=0.5)\n",
    "        # ax_2.set_xlim(xlim)\n",
    "        # ax_2.set_ylim(ylim)\n",
    "            \n",
    "# Add labels\n",
    "ax_0.set_title('-200 to 0')\n",
    "ax_1.set_title('0 to 200')\n",
    "ax_2.set_title('200 to 500')\n",
    "\n",
    "# ax_0.axis(\"off\")\n",
    "# ax_1.axis(\"off\")\n",
    "# ax_2.axis(\"off\")\n",
    "# ax_3.axis(\"off\")\n",
    "# ax_4.axis(\"off\")\n",
    "\n",
    "figDir = '/Users/sherryan/area2_population_analysis/figures_plus/'\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_active_traj.pdf',dpi = 'figure')\n",
    "# plt.suptitle('Active Reach Trajectories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reaching directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldername = \"~/area2_population_analysis/s1-kinematics/actpas_NWB/\"\n",
    "# monkey = \"Han_20171207\"\n",
    "# filename = foldername + monkey + \"_COactpas_TD_offset6.nwb\"\n",
    "\n",
    "# monkey = \"Chips_20170913\"\n",
    "# filename = foldername + monkey + \"_COactpas_TD.nwb\"\n",
    "\n",
    "monkey = 'Duncan_20190710'\n",
    "filename = foldername + monkey + \"_COactpas_offset6.nwb\"\n",
    "\n",
    "dataset_50ms = NWBDataset(filename, split_heldout=False)\n",
    "dataset_50ms.resample(10)\n",
    "bin_width = dataset_50ms.bin_width\n",
    "print(bin_width)\n",
    "\n",
    "# dataset_50ms.resample(10)\n",
    "# bin_width = dataset_50ms.bin_width\n",
    "# print(bin_width)\n",
    "\n",
    "# filename = '/Users/sherryan/area2_population_analysis/s1-kinematics/'+monkey+'_COactpas_with_emg_TD.mat'\n",
    "# mat = scipy.io.loadmat(filename)\n",
    "# EMG = mat['trial_data']['emg'][0,0]\n",
    "# dataset_50ms.add_continuous_data(EMG,'EMG')\n",
    "\n",
    "# dataset_50ms.add_continuous_data(dataset_10ms.data.PCA_40.to_numpy(),'PCA_40')\n",
    "# dataset_50ms.add_continuous_data(dataset_10ms.data.spikes_smth_40_oneside.to_numpy(),'spikes_smth_40_oneside')\n",
    "# dataset_50ms.add_continuous_data(dataset_10ms.data.muscle_PCA.to_numpy(),'muscle_PCA')\n",
    "# dataset_50ms.add_continuous_data(dataset_10ms.data.joint_PCA.to_numpy(),'joint_PCA')\n",
    "\n",
    "# x_field = 'spikes'\n",
    "# data = np.load(monkey+'_unsmoothed50_cdfb_data_'+x_field+'.npz')\n",
    "# data.files\n",
    "# dataset_50ms.add_continuous_data(data['CD_FB_proj'],'CD_FB_proj')\n",
    "# dataset_50ms.add_continuous_data(data['CD_proj'],'CD_proj')\n",
    "# dataset_50ms.add_continuous_data(data['FB_proj'],'FB_proj')\n",
    "# dataset_50ms.resample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'spikes'\n",
    "data = np.load(monkey+'_v6_zscore_unsmoothed_cdfb_data_'+x_field+'.npz')\n",
    "data.files\n",
    "dataset_50ms.add_continuous_data(data['CD_FB_proj'],'CD_FB_proj')\n",
    "dataset_50ms.add_continuous_data(data['CD_proj'],'CD_proj')\n",
    "dataset_50ms.add_continuous_data(data['FB_proj'],'FB_proj')\n",
    "dataset_50ms.resample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procX_train_test(X,y,training_set,test_set):\n",
    "    X_train = X[training_set,:]\n",
    "    X_test = X[test_set,:]\n",
    "    y_train = y[training_set,:]\n",
    "    y_test = y[test_set,:]    \n",
    "    \n",
    "    X_train_mean=np.nanmean(X_train,axis=0)\n",
    "    X_train_std=np.nanstd(X_train,axis=0)  \n",
    "    X_train_std[X_train_std==0] = 1\n",
    "\n",
    "\n",
    "    X_train=(X_train-X_train_mean)/X_train_std\n",
    "    X_test=(X_test-X_train_mean)/X_train_std\n",
    " \n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial_mask = active_mask\n",
    "n_trials = dataset_10ms.trial_info.shape[0]\n",
    "print(n_trials,'total trials')\n",
    "n_neurons = dataset_10ms.data.spikes.shape[1]\n",
    "print(n_neurons,'neurons')\n",
    "\n",
    "#make dictionary for trial condition (reaching directions) for Stratified CV\n",
    "dataset = dataset_10ms\n",
    "active_mask = (dataset.trial_info.ctr_hold_bump==0) & (dataset.trial_info['split'] != 'none')\n",
    "passive_mask = (dataset.trial_info.ctr_hold_bump==1) & (dataset.trial_info['split'] != 'none')\n",
    "nan_mask = (np.isnan(dataset.trial_info.ctr_hold_bump)) & (dataset.trial_info['split'] != 'none')\n",
    "nan_against_mask = (np.isnan(dataset.trial_info.ctr_hold_bump)) & (dataset.trial_info['bump_dir']%360 == (dataset.trial_info['cond_dir']+180)%360) & (dataset.trial_info['split'] != 'none')\n",
    "nan_assist_mask = (np.isnan(dataset.trial_info.ctr_hold_bump)) & (dataset.trial_info['bump_dir']%360 == dataset.trial_info['cond_dir']%360) & (dataset.trial_info['split'] != 'none')\n",
    "nan_disturb_mask = (np.isnan(dataset.trial_info.ctr_hold_bump)) & (dataset.trial_info['bump_dir']%360 != dataset.trial_info['cond_dir']%360) & (dataset.trial_info['bump_dir']%360 != (dataset.trial_info['cond_dir']+180)%360) & (dataset.trial_info['split'] != 'none')\n",
    "nan_against_assist_mask = (np.isnan(dataset.trial_info.ctr_hold_bump)) & ((dataset.trial_info['bump_dir']%360 == (dataset.trial_info['cond_dir']+180)%360) | (dataset.trial_info['bump_dir']%360 == dataset.trial_info['cond_dir']%360)) & (dataset.trial_info['split'] != 'none')\n",
    "\n",
    "all_mask = (dataset.trial_info['split'] != 'none')\n",
    "\n",
    "trial_mask = all_mask\n",
    "valid_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(valid_n_trials,'valid trials')\n",
    "\n",
    "trial_mask = active_mask\n",
    "active_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "active_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(active_n_trials,'active trials')\n",
    "\n",
    "trial_mask = passive_mask\n",
    "passive_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "passive_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(passive_n_trials,'passive trials')\n",
    "\n",
    "trial_mask = nan_mask\n",
    "nan_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "nan_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(nan_n_trials,'reach bump trials')\n",
    "\n",
    "trial_mask = nan_against_mask\n",
    "nan_against_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "nan_against_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(nan_against_n_trials,'reach bump against trials')\n",
    "\n",
    "trial_mask = nan_assist_mask\n",
    "nan_assist_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "nan_assist_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(nan_assist_n_trials,'reach bump assist trials')\n",
    "\n",
    "trial_mask = nan_disturb_mask\n",
    "nan_disturb_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "nan_disturb_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(nan_disturb_n_trials,'reach bump disturb trials')\n",
    "\n",
    "active_cond_dir_idx = []\n",
    "passive_cond_dir_idx = []\n",
    "nan_cond_dir_idx = []\n",
    "nan_bump_cond_dir_idx = []\n",
    "\n",
    "for direction in [0,45,90,135,180,225,270,315]:\n",
    "# for direction in [0,90,180,270]:\n",
    "    active_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & (dataset.trial_info['ctr_hold_bump'] == 0) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "    passive_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & (dataset.trial_info['ctr_hold_bump'] == 1) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "    nan_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & (np.isnan(dataset.trial_info.ctr_hold_bump)) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "    nan_bump_cond_dir_idx.append(np.where((dataset.trial_info['bump_dir']%360 == direction) & (np.isnan(dataset.trial_info.ctr_hold_bump)) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "\n",
    "active_cond_dict = nans([active_n_trials])\n",
    "i = 0\n",
    "for idx in active_trials_idx:\n",
    "    for cond in range(0,len(active_cond_dir_idx)):\n",
    "        if idx in active_cond_dir_idx[cond]:\n",
    "            active_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(active_cond_dict)\n",
    "print(len(active_cond_dict))\n",
    "\n",
    "passive_cond_dict = nans([passive_n_trials])\n",
    "i = 0\n",
    "for idx in passive_trials_idx:\n",
    "    for cond in range(0,len(passive_cond_dir_idx)):\n",
    "        if idx in passive_cond_dir_idx[cond]:\n",
    "            passive_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(passive_cond_dict)\n",
    "print(len(passive_cond_dict))\n",
    "\n",
    "nan_cond_dict = nans([nan_n_trials])\n",
    "i = 0\n",
    "for idx in nan_trials_idx:\n",
    "    for cond in range(0,len(nan_cond_dir_idx)):\n",
    "        if idx in nan_cond_dir_idx[cond]:\n",
    "            nan_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(nan_cond_dict)\n",
    "print(len(nan_cond_dict))\n",
    "\n",
    "nan_bump_cond_dict = nans([nan_n_trials])\n",
    "i = 0\n",
    "for idx in nan_trials_idx:\n",
    "    for cond in range(0,len(nan_bump_cond_dir_idx)):\n",
    "        if idx in nan_bump_cond_dir_idx[cond]:\n",
    "            nan_bump_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(nan_bump_cond_dict)\n",
    "print(len(nan_bump_cond_dict))\n",
    "\n",
    "\n",
    "if monkey == 'Duncan_20190710':\n",
    "    active_df = dataset.make_trial_data(align_field='move_onset_time', align_range = (-100,100), ignored_trials = ~active_mask)\n",
    "    del_indices = list(set(active_trials_idx) - set(active_df['trial_id'].unique()))\n",
    "    print('was',active_n_trials,'active trials')\n",
    "    active_n_trials = active_n_trials - len(list(set(active_trials_idx) - set(active_df['trial_id'].unique())))\n",
    "    active_cond_dict_onset = np.delete(active_cond_dict,np.where(np.isin(active_trials_idx, del_indices)))\n",
    "    print('now',active_n_trials,'active trials')\n",
    "    print(len(active_cond_dict_onset))\n",
    "\n",
    "    passive_df = dataset.make_trial_data(align_field='move_onset_time', align_range = (-100,100), ignored_trials = ~passive_mask)\n",
    "    del_indices = list(set(passive_trials_idx) - set(passive_df['trial_id'].unique()))\n",
    "    print('was',passive_n_trials,'passive trials')\n",
    "    passive_n_trials = passive_n_trials - len(list(set(passive_trials_idx) - set(passive_df['trial_id'].unique())))\n",
    "    passive_cond_dict = np.delete(passive_cond_dict,np.where(np.isin(passive_trials_idx, del_indices)))\n",
    "    print('now',passive_n_trials,'passive trials')\n",
    "\n",
    "    print(len(passive_cond_dict))\n",
    "\n",
    "    # nan_df = dataset.make_trial_data(align_field='move_onset_time', align_range = (-100,100), ignored_trials = ~nan_mask)\n",
    "    # del_indices = list(set(nan_trials_idx) - set(nan_df['trial_id'].unique()))\n",
    "    # print('was',nan_n_trials,'nan trials')\n",
    "    # nan_n_trials = nan_n_trials - len(list(set(nan_trials_idx) - set(nan_df['trial_id'].unique())))\n",
    "    # nan_cond_dict = np.delete(nan_cond_dict,np.where(np.isin(nan_trials_idx, del_indices)))\n",
    "    # nan_bump_cond_dict = np.delete(nan_bump_cond_dict,np.where(np.isin(nan_trials_idx, del_indices)))\n",
    "    # print('now',nan_n_trials,'nan trials')\n",
    "    # print(len(nan_bump_cond_dict))\n",
    "\n",
    "active_df = dataset.make_trial_data(align_field='move_offset_time', align_range = (-100,0), ignored_trials = ~active_mask)\n",
    "del_indices = list(set(active_trials_idx) - set(active_df['trial_id'].unique()))\n",
    "print('was',active_n_trials,'active trials')\n",
    "active_cond_dict_offset = np.delete(active_cond_dict, np.where(np.isin(active_trials_idx, del_indices))[0])\n",
    "print('now')\n",
    "print(len(active_cond_dict_offset))\n",
    "if monkey == 'Duncan_20190710':\n",
    "    active_cond_dict = active_cond_dict_onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dims = 20\n",
    "all_data = np.array(dataset_50ms.data.spikes)\n",
    "print(all_data.shape)\n",
    "if not np.isnan(all_data).any():\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(all_data)\n",
    "    pca = PCA(n_components=n_dims,random_state = 42)\n",
    "    PCA_data = pca.fit_transform(X)\n",
    "print(PCA_data.shape)\n",
    "dataset_50ms.add_continuous_data(PCA_data,'20PC')\n",
    "print('PCA total var explained:',sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reach-bump trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bump align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA projections over trial, for different reaching directions\n",
    "plot_dir = np.array([0,45,90,135,180,225,270,315]) \n",
    "directions = np.array([0,45,90,135,180,225,270,315])\n",
    "\n",
    "cmap = plt.get_cmap('coolwarm',len(plot_dir))\n",
    "custom_palette = [mpl.colors.rgb2hex(cmap(i)) for i in range(len(plot_dir))]\n",
    "plot_field = 'smooth_CD_FB_proj'\n",
    "N = dataset_50ms.data[plot_field].shape[1]\n",
    "order = range(N)\n",
    "\n",
    "pred_range = (-300, 1000)\n",
    "\n",
    "# mask = nan_mask\n",
    "# n_trials = nan_n_trials\n",
    "# cond_dict = nan_cond_dict\n",
    "\n",
    "mask = nan_assist_mask\n",
    "n_trials = nan_assist_n_trials\n",
    "cond_dict = nan_assist_cond_dict\n",
    "\n",
    "# mask = nan_against_mask\n",
    "# n_trials = nan_against_n_trials\n",
    "# cond_dict = nan_against_cond_dict\n",
    "\n",
    "# mask = nan_disturb_mask\n",
    "# n_trials = nan_disturb_n_trials\n",
    "# cond_dict = nan_disturb_cond_dict\n",
    "# cond_dict = nan_disturb_bump_cond_dict\n",
    "\n",
    "n_timepoints = int((pred_range[1] - pred_range[0])/dataset_50ms.bin_width)\n",
    "data = dataset_50ms.make_trial_data(align_field='bump_time', align_range=pred_range, ignored_trials=~mask)\n",
    "n_trials = data['trial_id'].nunique()\n",
    "trials_pca = nans([n_trials,n_timepoints,N])\n",
    "i = 0\n",
    "for idx, trial in data.groupby('trial_id'):\n",
    "    trials_pca[i,:,:]=trial[plot_field].to_numpy()\n",
    "    i+=1\n",
    "print(trials_pca.shape)\n",
    "\n",
    "x_axis = np.arange(pred_range[0], pred_range[1], dataset_50ms.bin_width)\n",
    "\n",
    "# define some useful time points\n",
    "move_idx=0\n",
    "ret_idx = 500\n",
    "\n",
    "plot_dims = N\n",
    "\n",
    "fig,ax=plt.subplots(plot_dims,1,figsize=(10,N+4))\n",
    "for i in range(plot_dims):\n",
    "    for j in range(len(plot_dir)):\n",
    "        color = custom_palette[j]\n",
    "        dir_idx = np.argwhere(directions == plot_dir[j])[0]\n",
    "        cond_mean_proj = np.mean(trials_pca[np.argwhere(cond_dict==dir_idx).flatten(),:,:], axis = 0)[:,order[i]] \n",
    "        pca_mean = np.mean(data[plot_field].to_numpy(),axis = 0)[order[i]] \n",
    "        ax[i].plot(x_axis,cond_mean_proj - pca_mean,linewidth=2.25,color = color,label = plot_dir[j])\n",
    "        \n",
    "        ax[i].axvline(move_idx, color='k',linewidth = 1)\n",
    "        # ax[i].axvline(ret_idx, color='k',linewidth = 1)\n",
    "        \n",
    "        ax[i].set_xlim([-200,1000])\n",
    "        ax[i].set_ylim([-1, 1])\n",
    "        ax[i].axhline(0,color ='k',ls = '--')\n",
    "        if i<plot_dims-1:\n",
    "            ax[i].set_xticks([])\n",
    "        else:\n",
    "            ax[i].set_xlabel('Time after bump onset (ms)')\n",
    "            \n",
    "        ax[i].set_yticks([])\n",
    "        ax[i].set_ylabel('Dim. '+str(i+1))\n",
    "\n",
    "    ax[0].set_title('Assist trials (bump sorted)')\n",
    "     \n",
    "plt.legend(bbox_to_anchor = (1, 1), loc = 'upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_cdfb_reachbump_assist_smooth.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldername = \"~/area2_population_analysis/s1-kinematics/actpas_NWB/\"\n",
    "# monkey = \"Han_20171207\"\n",
    "# filename = foldername + monkey + \"_COactpas_TD_offset6.nwb\"\n",
    "\n",
    "# monkey = \"Chips_20170913\"\n",
    "# filename = foldername + monkey + \"_COactpas_TD.nwb\"\n",
    "\n",
    "monkey = 'Duncan_20190710'\n",
    "filename = foldername + monkey + \"_COactpas_offset6.nwb\"\n",
    "\n",
    "dataset_10ms = NWBDataset(filename, split_heldout=False)\n",
    "\n",
    "dataset_10ms.resample(10) #in 10-ms bin, has to resample first for Duncan\n",
    "bin_width = dataset_10ms.bin_width\n",
    "print(bin_width)\n",
    "\n",
    "# xyz_force = np.array([dataset_5ms.data['force']['x'].to_numpy(), dataset_5ms.data['force']['y'].to_numpy(), dataset_5ms.data['force']['z'].to_numpy()]).T\n",
    "# dataset_10ms.add_continuous_data(xyz_force,'manip_force',chan_names = ['x','y','z'])\n",
    "\n",
    "dataset_10ms.smooth_spk(40, name='smth_40')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset_10ms\n",
    "# x_field = 'spikes'\n",
    "# data = np.load(monkey+'_v6_zscore_smoothed_cdfb_data_'+x_field+'.npz')\n",
    "# data.files\n",
    "# dataset.add_continuous_data(data['CD_FB_proj'],'CD_FB_proj')\n",
    "# dataset.add_continuous_data(data['CD_proj'],'CD_proj')\n",
    "# dataset.add_continuous_data(data['FB_proj'],'FB_proj')\n",
    "# dataset.data.keys().unique(0)\n",
    "\n",
    "# x_field = 'spikes'\n",
    "# data = np.load(monkey+'_v6_zscore_unsmoothed_cdfb_data_'+x_field+'.npz')\n",
    "# data.files\n",
    "# dataset.add_continuous_data(data['CD_FB_proj'],'unsmooth_CD_FB_proj')\n",
    "# dataset.add_continuous_data(data['CD_proj'],'unsmooth_CD_proj')\n",
    "# dataset.add_continuous_data(data['FB_proj'],'unsmooth_FB_proj')\n",
    "# dataset.data.keys().unique(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_10ms\n",
    "x_field = 'spikes'\n",
    "data = np.load(monkey+'_v6_zscore_unsmoothed_cdfb_data_'+x_field+'.npz')\n",
    "data.files\n",
    "dataset.add_continuous_data(data['CD_FB_proj'],'CD+FB')\n",
    "dataset.add_continuous_data(data['CD_proj'],'CD')\n",
    "dataset.add_continuous_data(data['FB_proj'],'FB')\n",
    "dataset.data.keys().unique(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial_mask = active_mask\n",
    "n_trials = dataset_10ms.trial_info.shape[0]\n",
    "print(n_trials,'total trials')\n",
    "n_neurons = dataset_10ms.data.spikes.shape[1]\n",
    "print(n_neurons,'neurons')\n",
    "\n",
    "#make dictionary for trial condition (reaching directions) for Stratified CV\n",
    "dataset = dataset_10ms\n",
    "active_mask = (dataset.trial_info.ctr_hold_bump==0) & (dataset.trial_info['split'] != 'none')\n",
    "passive_mask = (dataset.trial_info.ctr_hold_bump==1) & (dataset.trial_info['split'] != 'none')\n",
    "nan_mask = (np.isnan(dataset.trial_info.ctr_hold_bump)) & (dataset.trial_info['split'] != 'none')\n",
    "nan_against_mask = (np.isnan(dataset.trial_info.ctr_hold_bump)) & (dataset.trial_info['bump_dir']%360 == (dataset.trial_info['cond_dir']+180)%360) & (dataset.trial_info['split'] != 'none')\n",
    "nan_assist_mask = (np.isnan(dataset.trial_info.ctr_hold_bump)) & (dataset.trial_info['bump_dir']%360 == dataset.trial_info['cond_dir']%360) & (dataset.trial_info['split'] != 'none')\n",
    "nan_disturb_mask = (np.isnan(dataset.trial_info.ctr_hold_bump)) & (dataset.trial_info['bump_dir']%360 != dataset.trial_info['cond_dir']%360) & (dataset.trial_info['bump_dir']%360 != (dataset.trial_info['cond_dir']+180)%360) & (dataset.trial_info['split'] != 'none')\n",
    "nan_against_assist_mask = (np.isnan(dataset.trial_info.ctr_hold_bump)) & ((dataset.trial_info['bump_dir']%360 == (dataset.trial_info['cond_dir']+180)%360) | (dataset.trial_info['bump_dir']%360 == dataset.trial_info['cond_dir']%360)) & (dataset.trial_info['split'] != 'none')\n",
    "\n",
    "all_mask = (dataset.trial_info['split'] != 'none')\n",
    "\n",
    "trial_mask = all_mask\n",
    "valid_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(valid_n_trials,'valid trials')\n",
    "\n",
    "trial_mask = active_mask\n",
    "active_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "active_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(active_n_trials,'active trials')\n",
    "\n",
    "trial_mask = passive_mask\n",
    "passive_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "passive_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(passive_n_trials,'passive trials')\n",
    "\n",
    "trial_mask = nan_mask\n",
    "nan_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "nan_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(nan_n_trials,'reach bump trials')\n",
    "\n",
    "trial_mask = nan_against_mask\n",
    "nan_against_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "nan_against_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(nan_against_n_trials,'reach bump against trials')\n",
    "\n",
    "trial_mask = nan_assist_mask\n",
    "nan_assist_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "nan_assist_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(nan_assist_n_trials,'reach bump assist trials')\n",
    "\n",
    "trial_mask = nan_disturb_mask\n",
    "nan_disturb_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "nan_disturb_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(nan_disturb_n_trials,'reach bump disturb trials')\n",
    "\n",
    "active_cond_dir_idx = []\n",
    "passive_cond_dir_idx = []\n",
    "nan_cond_dir_idx = []\n",
    "nan_bump_cond_dir_idx = []\n",
    "\n",
    "for direction in [0,45,90,135,180,225,270,315]:\n",
    "# for direction in [0,90,180,270]:\n",
    "    active_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & (dataset.trial_info['ctr_hold_bump'] == 0) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "    passive_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & (dataset.trial_info['ctr_hold_bump'] == 1) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "    nan_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & (np.isnan(dataset.trial_info.ctr_hold_bump)) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "    nan_bump_cond_dir_idx.append(np.where((dataset.trial_info['bump_dir']%360 == direction) & (np.isnan(dataset.trial_info.ctr_hold_bump)) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "\n",
    "active_cond_dict = nans([active_n_trials])\n",
    "i = 0\n",
    "for idx in active_trials_idx:\n",
    "    for cond in range(0,len(active_cond_dir_idx)):\n",
    "        if idx in active_cond_dir_idx[cond]:\n",
    "            active_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(active_cond_dict)\n",
    "print(len(active_cond_dict))\n",
    "\n",
    "passive_cond_dict = nans([passive_n_trials])\n",
    "i = 0\n",
    "for idx in passive_trials_idx:\n",
    "    for cond in range(0,len(passive_cond_dir_idx)):\n",
    "        if idx in passive_cond_dir_idx[cond]:\n",
    "            passive_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(passive_cond_dict)\n",
    "print(len(passive_cond_dict))\n",
    "\n",
    "nan_cond_dict = nans([nan_n_trials])\n",
    "i = 0\n",
    "for idx in nan_trials_idx:\n",
    "    for cond in range(0,len(nan_cond_dir_idx)):\n",
    "        if idx in nan_cond_dir_idx[cond]:\n",
    "            nan_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(nan_cond_dict)\n",
    "print(len(nan_cond_dict))\n",
    "\n",
    "nan_bump_cond_dict = nans([nan_n_trials])\n",
    "i = 0\n",
    "for idx in nan_trials_idx:\n",
    "    for cond in range(0,len(nan_bump_cond_dir_idx)):\n",
    "        if idx in nan_bump_cond_dir_idx[cond]:\n",
    "            nan_bump_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(nan_bump_cond_dict)\n",
    "print(len(nan_bump_cond_dict))\n",
    "\n",
    "\n",
    "if monkey == 'Duncan_20190710':\n",
    "    active_df = dataset.make_trial_data(align_field='move_onset_time', align_range = (-100,100), ignored_trials = ~active_mask)\n",
    "    del_indices = list(set(active_trials_idx) - set(active_df['trial_id'].unique()))\n",
    "    print('was',active_n_trials,'active trials')\n",
    "    active_n_trials = active_n_trials - len(list(set(active_trials_idx) - set(active_df['trial_id'].unique())))\n",
    "    active_cond_dict_onset = np.delete(active_cond_dict,np.where(np.isin(active_trials_idx, del_indices)))\n",
    "    print('now',active_n_trials,'active trials')\n",
    "    print(len(active_cond_dict_onset))\n",
    "\n",
    "    passive_df = dataset.make_trial_data(align_field='move_onset_time', align_range = (-100,100), ignored_trials = ~passive_mask)\n",
    "    del_indices = list(set(passive_trials_idx) - set(passive_df['trial_id'].unique()))\n",
    "    print('was',passive_n_trials,'passive trials')\n",
    "    passive_n_trials = passive_n_trials - len(list(set(passive_trials_idx) - set(passive_df['trial_id'].unique())))\n",
    "    passive_cond_dict = np.delete(passive_cond_dict,np.where(np.isin(passive_trials_idx, del_indices)))\n",
    "    print('now',passive_n_trials,'passive trials')\n",
    "\n",
    "    print(len(passive_cond_dict))\n",
    "\n",
    "    # nan_df = dataset.make_trial_data(align_field='move_onset_time', align_range = (-100,100), ignored_trials = ~nan_mask)\n",
    "    # del_indices = list(set(nan_trials_idx) - set(nan_df['trial_id'].unique()))\n",
    "    # print('was',nan_n_trials,'nan trials')\n",
    "    # nan_n_trials = nan_n_trials - len(list(set(nan_trials_idx) - set(nan_df['trial_id'].unique())))\n",
    "    # nan_cond_dict = np.delete(nan_cond_dict,np.where(np.isin(nan_trials_idx, del_indices)))\n",
    "    # nan_bump_cond_dict = np.delete(nan_bump_cond_dict,np.where(np.isin(nan_trials_idx, del_indices)))\n",
    "    # print('now',nan_n_trials,'nan trials')\n",
    "    # print(len(nan_bump_cond_dict))\n",
    "\n",
    "active_df = dataset.make_trial_data(align_field='move_offset_time', align_range = (-100,0), ignored_trials = ~active_mask)\n",
    "del_indices = list(set(active_trials_idx) - set(active_df['trial_id'].unique()))\n",
    "print('was',active_n_trials,'active trials')\n",
    "active_cond_dict_offset = np.delete(active_cond_dict, np.where(np.isin(active_trials_idx, del_indices))[0])\n",
    "print('now')\n",
    "print(len(active_cond_dict_offset))\n",
    "if monkey == 'Duncan_20190710':\n",
    "    active_cond_dict = active_cond_dict_onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "dataset = dataset_10ms\n",
    "n_splits = 100\n",
    "pred_range = (25,75)  # first 100 ms after bump\n",
    "x_fields = ['CD', 'FB', 'CD+FB']\n",
    "\n",
    "pos_labels_all = [0, 45, 90, 135]  # only these directions have both assist and against trials\n",
    "decoder_results = {pos_label: {x_field: [] for x_field in x_fields} for pos_label in pos_labels_all}\n",
    "all_results = {x_field: [] for x_field in x_fields}\n",
    "pred_range = (25,75)  # first 100 ms after bump\n",
    "\n",
    "for pos_label in pos_labels_all:\n",
    "    # Select trials (reach dir 90/270 & assist/against)\n",
    "    neg_label = pos_label+180\n",
    "    reach_mask = dataset.trial_info['cond_dir'] % 360\n",
    "    reach_mask = reach_mask.isin([pos_label, neg_label])\n",
    "    trial_mask = reach_mask & (nan_assist_mask | nan_against_mask)\n",
    "    selected_trials_idx = dataset.trial_info.loc[trial_mask, 'trial_id'].values\n",
    "    n_trials = len(selected_trials_idx)\n",
    "    print(n_trials, \"trials selected\")\n",
    "\n",
    "    # Build 4-class labels: 0=left assist, 1=left against, 2=right assist, 3=right against\n",
    "    labels = []\n",
    "    for tid in selected_trials_idx:\n",
    "        row = dataset.trial_info.loc[dataset.trial_info.trial_id == tid].iloc[0]\n",
    "        cond, bump = row['cond_dir'] % 360, row['bump_dir'] % 360\n",
    "        if cond == pos_label:  # upward (90°)\n",
    "            if bump == cond: label = 2  # right assist\n",
    "            else: label = 3  # right against\n",
    "        elif cond == neg_label:  # downward (270°)\n",
    "            if bump == cond: label = 0  # left assist\n",
    "            else: label = 1  # left against\n",
    "        else: raise ValueError(f\"Unexpected cond_dir {cond} for trial {tid}\")\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels)\n",
    "    print(labels)\n",
    "\n",
    "    # Balance across 4 groups\n",
    "    min_count = min(Counter(labels).values())\n",
    "    print(min_count)\n",
    "\n",
    "    # i.e., 1 = upward (90°), 0 = downward (270°)\n",
    "    y_dir = np.array([dataset.trial_info.loc[dataset.trial_info.trial_id == tid, 'bump_dir'].values[0] for tid in selected_trials_idx])\n",
    "    y = (y_dir%360 == pos_label).astype(int)\n",
    "\n",
    "    trial_df = dataset.make_trial_data(\n",
    "        align_field='bump_time', \n",
    "        align_range=pred_range, \n",
    "        ignored_trials=~trial_mask\n",
    "    )\n",
    "\n",
    "    for x_field in x_fields:\n",
    "        # Extract trial data\n",
    "        dim = dataset.data[x_field].shape[1]\n",
    "        # Convert to (trials, bins, features)\n",
    "        X_trials = trial_df[x_field].to_numpy().reshape(n_trials, -1, dim)\n",
    "        X_avg = np.mean(X_trials, axis=1)  # average over first 100 ms\n",
    "\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(StandardScaler().fit_transform(X_avg), labels)\n",
    "        # if x_field == 'CD+FB':\n",
    "        #     # Store weights and scalers for later inspection\n",
    "        #     all_results.setdefault('combined_weights', []).append(clf.coef_.ravel())\n",
    "        # Classification with splits\n",
    "        sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.5, random_state=42)\n",
    "        split_acc = []\n",
    "        for j, (train_idx, test_idx) in enumerate(sss.split(X_avg, labels)):\n",
    "            # # Standardize based on training set\n",
    "            # scaler = StandardScaler()\n",
    "            # X_train = scaler.fit_transform(X_avg[train_idx])\n",
    "            # X_test = scaler.transform(X_avg[test_idx])\n",
    "            # # X_train = X_avg[train_idx]\n",
    "            # # X_test = X_avg[test_idx]\n",
    "                # Balance *training set*\n",
    "            train_labels = labels[train_idx]  # use 4-class labels\n",
    "            min_count = min(Counter(train_labels).values())\n",
    "            balanced_train_idx = np.hstack([\n",
    "                np.random.choice(np.where(train_labels == c)[0], min_count, replace=False)\n",
    "                for c in np.unique(labels)\n",
    "            ])\n",
    "            np.random.shuffle(balanced_train_idx)\n",
    "\n",
    "            # Map back to absolute indices\n",
    "            train_idx_bal = train_idx[balanced_train_idx]\n",
    "\n",
    "            # Same for test set if you want strict balancing there too\n",
    "            test_labels = labels[test_idx]\n",
    "            min_count_test = min(Counter(test_labels).values())\n",
    "            balanced_test_idx = np.hstack([\n",
    "                np.random.choice(np.where(test_labels == c)[0], min_count_test, replace=False)\n",
    "                for c in np.unique(labels)\n",
    "            ])\n",
    "            np.random.shuffle(balanced_test_idx)\n",
    "            test_idx_bal = test_idx[balanced_test_idx]\n",
    "\n",
    "            # Build X and y for classifier\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_avg[train_idx_bal])\n",
    "            X_test = scaler.transform(X_avg[test_idx_bal])\n",
    "            y_train = np.isin(labels[train_idx_bal], [1,2]).astype(int)\n",
    "            y_test = np.isin(labels[test_idx_bal], [1,2]).astype(int)\n",
    "\n",
    "            # print(len(y_train), len(y_test))            \n",
    "\n",
    "\n",
    "            clf = LogisticRegression()\n",
    "            clf.fit(X_train, y_train)\n",
    "            if x_field == 'CD+FB':\n",
    "                w = clf.coef_.ravel()\n",
    "                b = clf.intercept_[0]\n",
    "                # Split weights into CD and FB components\n",
    "                n_cd = 3\n",
    "                w_cd = w[:n_cd]\n",
    "                w_fb = w[n_cd:]\n",
    "\n",
    "                # Compute component contributions on *test set*\n",
    "                X_cd_test = X_test[:, :n_cd]\n",
    "                X_fb_test = X_test[:, n_cd:]\n",
    "\n",
    "                z_cd = X_cd_test @ w_cd\n",
    "                z_fb = X_fb_test @ w_fb\n",
    "                z_total = z_cd + z_fb + b  # == clf.decision_function(X_test)\n",
    "                one_label_mask = (y_test == 1)\n",
    "                z_cd_1 = z_cd[one_label_mask]\n",
    "                z_fb_1 = z_fb[one_label_mask]\n",
    "                z_total_1 = z_total[one_label_mask]\n",
    "                # Store everything for later analysis\n",
    "                decoder_results[pos_label].setdefault('combined_components', []).append({\n",
    "                    'z_cd': z_cd_1,\n",
    "                    'z_fb': z_fb_1,\n",
    "                    'z_total': z_total_1\n",
    "                })\n",
    "            split_acc.append(clf.score(X_test, y_test))\n",
    "\n",
    "        # Store all split accuracies (not pooled predictions)\n",
    "        all_results[x_field].extend(split_acc)\n",
    "\n",
    "# Final summary across all pos_labels\n",
    "for x_field in x_fields:\n",
    "    mean_acc = np.mean(all_results[x_field])\n",
    "    std_acc  = np.std(all_results[x_field])\n",
    "    print(f\"{x_field}: mean accuracy = {mean_acc:.3f}, std = {std_acc:.3f}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "colors = ['green', 'magenta', 'brown']  # CD_proj, FB_proj, CD_FB_proj\n",
    "# x_fields = ['CD', 'FB', 'CD+FB']\n",
    "for i, x_field in enumerate(x_fields):\n",
    "    plt.errorbar(x_field, np.mean(all_results[x_field]), yerr=np.std(all_results[x_field]), fmt='o',\n",
    "                 capsize=5, markersize=8, color=colors[i], label=x_field)\n",
    "\n",
    "plt.ylabel('Classification Accuracy',fontsize=15)\n",
    "# plt.xlabel('Variable', fontsize=10)\n",
    "plt.xticks()\n",
    "plt.yticks()\n",
    "plt.ylim(0.3,1.05)\n",
    "# plt.legend(fontsize=9)\n",
    "# chance_level = max(np.sum(y == 0), np.sum(y == 1)) / len(y)\n",
    "chance_level = .5\n",
    "plt.axhline(chance_level, color='gray', linestyle='--', label='Chance level')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + \"_real_bump_class3.pdf\", dpi = 'figure')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_means = []\n",
    "fb_means = []\n",
    "\n",
    "for pos_label in pos_labels_all:\n",
    "    z_cd_all = np.concatenate([d['z_cd'] for d in decoder_results[pos_label]['combined_components']])\n",
    "    z_fb_all = np.concatenate([d['z_fb'] for d in decoder_results[pos_label]['combined_components']])\n",
    "\n",
    "    # Compute *mean signed contribution* per decoder\n",
    "    cd_means.append(np.mean(z_cd_all))\n",
    "    fb_means.append(np.mean(z_fb_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_means = np.array(cd_means)\n",
    "fb_means = np.array(fb_means)\n",
    "x = np.arange(len(pos_labels_all))  # [0,1,2,3]\n",
    "width = 0.2\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "plt.scatter(x - width/2, cd_means, color='green', s=60, label='CD')\n",
    "plt.scatter(x + width/2, fb_means, color='magenta', s=60, label='FB')\n",
    "\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "plt.xticks(x, [f\"{p}°\" for p in pos_labels_all])\n",
    "plt.xlabel('Reach direction (pos_label)')\n",
    "plt.ylabel('Mean logit contribution (w·x)')\n",
    "plt.title('CD and FB contributions per decoder')\n",
    "# plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_means.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.corrcoef(z_cd, z_fb)[0,1]\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results['combined_components']['z_cd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "z_cd_all = np.concatenate([d['z_cd'] for d in all_results['combined_components']])\n",
    "z_fb_all = np.concatenate([d['z_fb'] for d in all_results['combined_components']])\n",
    "y_all = np.concatenate([d['y_test'] for d in all_results['combined_components']])\n",
    "\n",
    "r = np.corrcoef(z_cd_all, z_fb_all)[0, 1]\n",
    "print(f\"CD–FB correlation across all test trials: {r:.3f}\")\n",
    "\n",
    "# Optional visualization\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(z_cd_all, z_fb_all, alpha=0.3)\n",
    "plt.xlabel('z_CD (CD contribution)')\n",
    "plt.ylabel('z_FB (FB contribution)')\n",
    "plt.title(f'CD vs FB contributions (r={r:.2f})')\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.axvline(0, color='gray', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack(all_results['combined_weights']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.vstack(all_results['combined_weights'])   # (400, 9)\n",
    "weights_avg_per_decoder = np.mean(\n",
    "    weights.reshape(4, n_splits, -1),\n",
    "    axis=1\n",
    ")\n",
    "cd_dim = dataset.data['CD'].shape[1]\n",
    "fb_dim = dataset.data['FB'].shape[1]\n",
    "\n",
    "# Compute mean CD and FB weights per decoder (signed means)\n",
    "cd_means = weights_avg_per_decoder[:, :cd_dim]\n",
    "fb_means = weights_avg_per_decoder[:, cd_dim:cd_dim+fb_dim]\n",
    "\n",
    "print(\"CD means:\", cd_means)\n",
    "print(\"FB means:\", fb_means)\n",
    "\n",
    "# weights = np.vstack(all_results['cd_only_weights'])   # (400, 3)\n",
    "# weights_avg_per_decoder = np.mean(\n",
    "#     weights.reshape(4, n_splits, -1),\n",
    "#     axis=1\n",
    "# )\n",
    "# cd_only_means = weights_avg_per_decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(pos_labels_all))  # [0,1,2,3]\n",
    "width = 0.2  # offset for visibility\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "# plt.scatter([x - width]*3, cd_only_means[x,:], color='green', label='CD',facecolors='none', edgecolors='green', s=60)\n",
    "plt.scatter([x - width/2]*1, cd_means[x,0], color='green', edgecolors='red',label='CD', s=40)\n",
    "# plt.scatter([x - width/2]*1, np.mean(cd_means[x,1:],axis=1), color='green', label='CD', s=60)\n",
    "plt.scatter([x - width/2]*1, cd_means[x,1], color='green', label='CD', s=60)\n",
    "\n",
    "\n",
    "# plt.scatter([x + width/2]*1, np.mean(fb_means[x,0:3],axis=1), color='magenta', edgecolors='red',label='FB', s=40)\n",
    "# plt.scatter([x + width/2]*1, np.mean(fb_means[x,3:],axis=1), color='magenta', label='FB', s=60)\n",
    "plt.scatter([x + width/2]*1, fb_means[x,0], color='magenta', edgecolors='red',label='FB', s=40)\n",
    "plt.scatter([x + width/2]*1, fb_means[x,3], color='magenta', label='FB', s=60)\n",
    "\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "plt.xticks(x, [f\"{p}°\" for p in pos_labels_all])\n",
    "plt.xlabel('Reach direction (pos_label)')\n",
    "plt.ylabel('decoder weight')\n",
    "plt.title('CD and FB weights per combined decoder')\n",
    "# plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "colors = ['green', 'magenta', 'brown']  # CD_proj, FB_proj, CD_FB_proj\n",
    "x_fields = ['CD', 'FB', 'CD+FB']\n",
    "for i, x_field in enumerate(x_fields):\n",
    "    plt.errorbar(x_field, np.mean(all_results[x_field]), yerr=np.std(all_results[x_field]), fmt='o',\n",
    "                 capsize=5, markersize=8, color=colors[i], label=x_field)\n",
    "\n",
    "plt.ylabel('Classification Accuracy',fontsize=15)\n",
    "# plt.xlabel('Variable', fontsize=10)\n",
    "plt.xticks()\n",
    "plt.yticks()\n",
    "plt.ylim(0.3,1.05)\n",
    "# plt.legend(fontsize=9)\n",
    "# chance_level = max(np.sum(y == 0), np.sum(y == 1)) / len(y)\n",
    "chance_level = .5\n",
    "plt.axhline(chance_level, color='gray', linestyle='--', label='Chance level')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + \"_real_bump_class3.pdf\", dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 16 conditions, in the format (ctr_hold_bump, cond_dir)\n",
    "unique_conditions = [(False, 0.0), (False, 45.0), (False, 90.0), (False, 135.0),\n",
    "                     (False, 180.0), (False, 225.0), (False, 270.0), (False, 315.0)]\n",
    "# unique_conditions = [(True, 0.0), (True, 45.0), (True, 90.0), (True, 135.0),\n",
    "#                      (True, 180.0), (True, 225.0), (True, 270.0), (True, 315.0)]\n",
    "\n",
    "# unique_conditions = [(np.nan, 0.0), (np.nan, 45.0), (np.nan, 90.0), (np.nan, 135.0),\n",
    "#                      (np.nan, 180.0), (np.nan, 225.0), (np.nan, 270.0), (np.nan, 315.0)]\n",
    "\n",
    "# unique_conditions = [(False, 0.0), (False, 90.0), \n",
    "#                      (False, 180.0), (False, 270.0)]\n",
    "# # unique_conditions = [(True, 0.0),(True, 90.0), \n",
    "#                      (True, 180.0), (True, 270.0)]\n",
    "\n",
    "# Initialize figure\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax_0 = fig.add_subplot(1,2,1)\n",
    "ax_1 = fig.add_subplot(1,2,2)\n",
    "dataset=dataset_10ms\n",
    "cmap = plt.get_cmap('coolwarm',len(unique_conditions))\n",
    "custom_palette = [matplotlib.colors.rgb2hex(cmap(i)) for i in range(len(unique_conditions))]\n",
    "j=0\n",
    "for cond in unique_conditions:\n",
    "    # Filter out invalid trials (labeled 'none') and trials in other conditions\n",
    "    cond_mask = (dataset.trial_info['ctr_hold_bump']==cond[0]) & \\\n",
    "                (dataset.trial_info['cond_dir']%360==cond[1]) & \\\n",
    "                (dataset.trial_info.split != 'none')\n",
    "# cond_mask = (np.isnan(dataset.trial_info['ctr_hold_bump'])) & \\\n",
    "\n",
    "    # Extract relevant portion of selected trials\n",
    "\n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(0, 1500), ignored_trials=~cond_mask)\n",
    "    # Plot reaches on appropriate subplot\n",
    "    for idx, trial in cond_data.groupby('trial_id'):\n",
    "        ax_0.plot(trial.hand_pos.x, trial.hand_pos.y, color=custom_palette[j], linewidth=0.5)\n",
    "    j+=1\n",
    "\n",
    "unique_conditions = [(True, 0.0), (True, 45.0), (True, 90.0), (True, 135.0),\n",
    "                     (True, 180.0), (True, 225.0), (True, 270.0), (True, 315.0)]\n",
    "j=0\n",
    "for cond in unique_conditions:\n",
    "    cond_mask = (np.isnan(dataset.trial_info['ctr_hold_bump'])) & \\\n",
    "            (dataset.trial_info['cond_dir']%360==cond[1]) & \\\n",
    "            (dataset.trial_info.split != 'none')\n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(0, 1500), ignored_trials=~cond_mask)\n",
    "    # Plot reaches on appropriate subplot\n",
    "    for idx, trial in cond_data.groupby('trial_id'):\n",
    "        ax_1.plot(trial.hand_pos.x, trial.hand_pos.y, color=custom_palette[j], linewidth=0.5)\n",
    "        ax_1.set_xlim([-11,13])\n",
    "        ax_1.set_ylim([-9,15])\n",
    "    j+=1\n",
    "\n",
    "    \n",
    "            \n",
    "# Add labels\n",
    "ax_0.set_title('active')\n",
    "ax_1.set_title('reach-bump')\n",
    "\n",
    "# ax_0.axis(\"off\")\n",
    "# ax_1.axis(\"off\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_traj.pdf',dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,4))\n",
    "\n",
    "colors = ['green', 'magenta', 'brown']  # CD_proj, FB_proj, CD_FB_proj\n",
    "for i, x_field in enumerate(x_fields):\n",
    "    y_true = np.array(all_results[x_field][\"y_true\"])\n",
    "    y_pred = np.array(all_results[x_field][\"y_pred\"])\n",
    "    acc = (y_true == y_pred).astype(float)    \n",
    "    plt.errorbar(x_field, np.mean(acc), yerr=np.std(acc), fmt='o',\n",
    "                 capsize=5, markersize=8, color=colors[i], label=x_field)\n",
    "\n",
    "plt.ylabel('Classification Accuracy', fontsize=10)\n",
    "plt.xlabel('Variable', fontsize=10)\n",
    "plt.xticks(fontsize=9)\n",
    "plt.yticks(fontsize=9)\n",
    "# plt.ylim(0.2,1.05)\n",
    "plt.title('Bump Classification Accuracy', fontsize=11)\n",
    "# plt.legend(fontsize=9)\n",
    "# chance_level = max(np.sum(y == 0), np.sum(y == 1)) / len(y)\n",
    "chance_level = .5\n",
    "plt.axhline(chance_level, color='gray', linestyle='--', label='Chance level')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fields = ['CD_proj', 'FB_proj']\n",
    "colors = {'up': 'blue', 'down': 'gold'}  # upward / downward\n",
    "linestyles = {'assist': '-', 'against': ':'}\n",
    "\n",
    "pred_range = (0, 100)  # ms after bump\n",
    "trial_df_all = dataset_10ms.make_trial_data(align_field='bump_time', align_range=pred_range, ignored_trials=~trial_mask)\n",
    "n_trials = len(selected_trials_idx)\n",
    "bin_width = dataset_10ms.bin_width  # to convert bins to ms\n",
    "time_axis = np.arange(pred_range[0], pred_range[1], bin_width)\n",
    "\n",
    "# Define trial groups\n",
    "groups = {\n",
    "    'up_assist': np.array([tid for tid in selected_trials_idx if (dataset.trial_info.loc[dataset.trial_info.trial_id==tid,'bump_dir'].values[0]==90 and tid in nan_assist_trials_idx)]),\n",
    "    'up_against': np.array([tid for tid in selected_trials_idx if (dataset.trial_info.loc[dataset.trial_info.trial_id==tid,'bump_dir'].values[0]==90 and tid in nan_against_trials_idx)]),\n",
    "    'down_assist': np.array([tid for tid in selected_trials_idx if (dataset.trial_info.loc[dataset.trial_info.trial_id==tid,'bump_dir'].values[0]==270 and tid in nan_assist_trials_idx)]),\n",
    "    'down_against': np.array([tid for tid in selected_trials_idx if (dataset.trial_info.loc[dataset.trial_info.trial_id==tid,'bump_dir'].values[0]==270 and tid in nan_against_trials_idx)]),\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# Plot per projection\n",
    "# -------------------------------\n",
    "for x_field in x_fields:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    X_trials = trial_df_all[x_field].to_numpy().reshape(n_trials, -1, dataset_10ms.data[x_field].shape[1])\n",
    "    \n",
    "    for group_name, trial_ids in groups.items():\n",
    "        if len(trial_ids) == 0:\n",
    "            continue\n",
    "        # Get indices of these trials in X_trials\n",
    "        trial_idx = [i for i, tid in enumerate(selected_trials_idx) if tid in trial_ids]\n",
    "        group_data = X_trials[trial_idx, :, :]  # shape: (trials, bins, features)\n",
    "        # Average over neurons\n",
    "        group_mean = np.mean(group_data, axis=(0,2))  # average over trials & features\n",
    "        group_sem = np.std(group_data, axis=(0,2)) / np.sqrt(len(trial_idx))\n",
    "        \n",
    "        # Color / linestyle\n",
    "        color = colors['up'] if 'up' in group_name else colors['down']\n",
    "        linestyle = linestyles['assist'] if 'assist' in group_name else linestyles['against']\n",
    "        \n",
    "        plt.plot(time_axis, group_mean, color=color, linestyle=linestyle, label=group_name.replace('_',' ').title())\n",
    "        # plt.fill_between(time_axis, group_mean-group_sem, group_mean+group_sem, color=color, alpha=0.2)\n",
    "    \n",
    "    plt.xlabel('Time (ms)', fontsize=10)\n",
    "    plt.ylabel(f'{x_field} signal', fontsize=10)\n",
    "    plt.title(f'{x_field} traces around bump', fontsize=11)\n",
    "    plt.legend(fontsize=9)\n",
    "    plt.xticks(fontsize=9)\n",
    "    plt.yticks(fontsize=9)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 20\n",
    "pred_range = (-300, 1000)\n",
    "x_field = 'CD_FB_proj'\n",
    "pos_bool = False\n",
    "# mask = nan_mask\n",
    "# n_trials = nan_n_trials\n",
    "# cond_dict = nan_cond_dict\n",
    "\n",
    "mask = nan_against_assist_mask\n",
    "n_trials = np.sum(mask)\n",
    "\n",
    "dim = dataset_50ms.data[x_field].shape[1]\n",
    "nan_df = dataset_50ms.make_trial_data(align_field='bump_time', align_range=pred_range, ignored_trials=~mask)\n",
    "\n",
    "dirs = [dataset_50ms.trial_info[dataset_50ms.trial_info.trial_id == i]['bump_dir'] for i in nan_df.trial_id]\n",
    "# dirs = [dataset_50ms.trial_info[dataset_50ms.trial_info.trial_id == i]['cond_dir'] for i in nan_df.trial_id]\n",
    "# dirs = [(dataset_50ms.trial_info[dataset_50ms.trial_info.trial_id == i]['cond_dir']-dataset_50ms.trial_info[dataset_50ms.trial_info.trial_id == i]['bump_dir']) for i in nan_df.trial_id]\n",
    "\n",
    "cos_x = [round(math.cos(math.radians(i)),3) for i in dirs]\n",
    "sin_y = [round(math.sin(math.radians(i)),3) for i in dirs]\n",
    "cos_sin = np.array([cos_x, sin_y]).T\n",
    "nan_trial_PCA = nan_df[x_field].to_numpy().reshape((n_trials, -1, dim))\n",
    "print(nan_trial_PCA.shape)\n",
    "nan_trial_ang = cos_sin.reshape((n_trials, -1, 2))\n",
    "# act_trial_ang = np.array(cos_ x).reshape((n_trials,-1,1))\n",
    "# act_trial_ang = np.array(sin_y).reshape((n_trials,-1,1))\n",
    "print(nan_trial_ang.shape)\n",
    "n_bins = nan_trial_PCA.shape[1]\n",
    "\n",
    "n_cd_dims = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_dir_dict = (\n",
    "    dataset_50ms.trial_info.cond_dir\n",
    ").to_dict()\n",
    "\n",
    "bump_dir_dict = (\n",
    "    dataset_50ms.trial_info.bump_dir\n",
    ").to_dict()\n",
    "\n",
    "assist_mask = np.array([cond_dir_dict[i]%360==bump_dir_dict[i]%360 for i in nan_df.trial_id.unique()])\n",
    "against_mask = np.array([(cond_dir_dict[i]+180)%360==bump_dir_dict[i]%360 for i in nan_df.trial_id.unique()])\n",
    "disturb_mask = ~(assist_mask | against_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_dict = assist_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bump_r2_arr = nans([n_bins, n_splits])\n",
    "cd_preds = nans([n_bins, n_trials, 2])\n",
    "fb_preds = nans([n_bins, n_trials, 2])\n",
    "\n",
    "# bump_coefs_arr = nans([n_bins, 2, dim])\n",
    "for i in range(n_bins):\n",
    "    X = nan_trial_PCA[:,i,:]\n",
    "    Y = nan_trial_ang[:,i,:]\n",
    "    std=np.nanstd(X,axis=0)  \n",
    "    std[std==0] = 1\n",
    "    X_proc = (X - np.nanmean(X,axis=0))/std\n",
    "    lr_all = GridSearchCV(Ridge(positive=pos_bool), {'alpha': np.logspace(-3, 3, 7)})\n",
    "    lr_all.fit(X_proc, Y)\n",
    "    coef = lr_all.best_estimator_.coef_\n",
    "    cd_preds[i,:,:] = X_proc[:,:n_cd_dims] @ coef.T[:n_cd_dims,:]\n",
    "    fb_preds[i,:,:] = X_proc[:,n_cd_dims:] @ coef.T[n_cd_dims:,:]\n",
    "    # bump_coefs_arr[i,:,:] = coef\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits,test_size=0.2)\n",
    "    for j, (training_set, test_set) in enumerate(sss.split(range(0,n_trials),cond_dict)):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test, y_train, y_test = procX_train_test(X,Y,training_set,test_set)\n",
    "        lr = GridSearchCV(Ridge(positive=pos_bool), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_test)\n",
    "        # Combined R² over both components\n",
    "        ss_res_combined = np.sum((y_test - y_pred) ** 2)\n",
    "        ss_tot_combined = np.sum((y_test - np.mean(y_test, axis=0)) ** 2)\n",
    "        r2_combined = 1 - ss_res_combined / ss_tot_combined\n",
    "        bump_r2_arr[i,j] = r2_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [0.0, 90.0, 180.0, 270.0]\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "plot_dim=1\n",
    "type = 'against'\n",
    "var_name = 'FB'\n",
    "var = fb_preds\n",
    "lag_axis = np.arange(-300, 1000, 50) + 25\n",
    "\n",
    "t_idx = np.argwhere(lag_axis==25)[0,0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for dir, color in zip(dirs, colors):\n",
    "    if type=='assist':\n",
    "        mask = np.array([cond_dir_dict[i]%360==bump_dir_dict[i]%360==dir for i in nan_df.trial_id.unique()])\n",
    "    elif type == 'against':\n",
    "        mask = np.array([(cond_dir_dict[i]+180)%360==bump_dir_dict[i]%360==dir for i in nan_df.trial_id.unique()])\n",
    "    avg_trace = np.mean(var[:,mask,plot_dim],axis=1)\n",
    "    plt.scatter(0,avg_trace[t_idx],color=color)\n",
    "    # plt.plot(lag_axis,avg_trace, color=color, linewidth=5)\n",
    "# plt.ylim([-0.6,0.6])\n",
    "plt.xlabel('Time after bump onset (ms)')\n",
    "plt.ylabel('Direction')\n",
    "plt.title(var_name+' Prediction - ' + type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lag_axis,cd_preds[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_axis = np.arange(-300, 1000, 50) + 25\n",
    "plt.plot(lag_axis,bump_r2_arr)\n",
    "plt.axvline(25,color='k',linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.shuffle(nan_trial_ang)\n",
    "# shuffle_r2_arr = nans([n_bins, n_splits])\n",
    "# for i in range(n_bins):\n",
    "#     X = nan_trial_PCA[:,i,:]\n",
    "#     Y = nan_trial_ang[:,i,:]\n",
    "#     std=np.nanstd(X,axis=0)  \n",
    "#     std[std==0] = 1\n",
    "#     X_proc = (X - np.nanmean(X,axis=0))/std\n",
    "#     lr_all = GridSearchCV(Ridge(positive=pos_bool), {'alpha': np.logspace(-3, 3, 7)})\n",
    "#     lr_all.fit(X_proc, Y)\n",
    "#     sss = StratifiedShuffleSplit(n_splits=n_splits,test_size=0.2)\n",
    "#     for j, (training_set, test_set) in enumerate(sss.split(range(0,n_trials),cond_dict)):\n",
    "#         #split training and testing by trials\n",
    "#         X_train, X_test, y_train, y_test = procX_train_test(X,Y,training_set,test_set)\n",
    "#         lr = GridSearchCV(Ridge(positive=pos_bool), {'alpha': np.logspace(-3, 3, 7)})\n",
    "#         lr.fit(X_train, y_train)\n",
    "#         y_pred = lr.predict(X_test)\n",
    "#         # Combined R² over both components\n",
    "#         ss_res_combined = np.sum((y_test - y_pred) ** 2)\n",
    "#         ss_tot_combined = np.sum((y_test - np.mean(y_test, axis=0)) ** 2)\n",
    "#         r2_combined = 1 - ss_res_combined / ss_tot_combined\n",
    "#         shuffle_r2_arr[i,j] = r2_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd_bump_r2 = bump_r2_arr\n",
    "# fb_bump_r2 = bump_r2_arr\n",
    "# cd_fb_bump_r2 = bump_r2_arr\n",
    "# SC_cd_bump_r2 = bump_r2_arr\n",
    "# SC_fb_bump_r2 = bump_r2_arr\n",
    "SC_cd_fb_bump_r2 = bump_r2_arr\n",
    "\n",
    "# neural_pc_bump_r2 = bump_r2_arr\n",
    "# neural_spikes_bump_r2 = bump_r2_arr\n",
    "# bump_shuffle_r2_arr = shuffle_r2_arr\n",
    "\n",
    "np.savez(monkey+'_v6_zscore_cdfb_reachbump_diffdir_r2_unsmoothed', \n",
    "         cd_bump_r2 = cd_bump_r2,\\\n",
    "         fb_bump_r2 = fb_bump_r2, cd_fb_bump_r2 = cd_fb_bump_r2,\\\n",
    "         SC_cd_bump_r2 = SC_cd_bump_r2, SC_fb_bump_r2 = SC_fb_bump_r2, SC_cd_fb_bump_r2 = SC_cd_fb_bump_r2,\\\n",
    "         neural_pc_bump_r2 = neural_pc_bump_r2,\\\n",
    "         neural_spikes_bump_r2 = neural_spikes_bump_r2,\\\n",
    "         bump_shuffle_r2_arr=bump_shuffle_r2_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey = 'Duncan_20190710'\n",
    "data = np.load(monkey+'_v6_zscore_cdfb_reachbump_diffdir_r2_unsmoothed.npz')\n",
    "data.files\n",
    "cd_bump_r2 = data['cd_bump_r2'] \n",
    "fb_bump_r2=data['fb_bump_r2'] \n",
    "cd_fb_bump_r2=data['cd_fb_bump_r2'] \n",
    "SC_cd_bump_r2 = data['SC_cd_bump_r2'] \n",
    "SC_fb_bump_r2=data['SC_fb_bump_r2'] \n",
    "SC_cd_fb_bump_r2=data['SC_cd_fb_bump_r2'] \n",
    "\n",
    "neural_pc_bump_r2 = data['neural_pc_bump_r2']\n",
    "neural_spikes_bump_r2 = data['neural_spikes_bump_r2']\n",
    "bump_shuffle_r2_arr=data['bump_shuffle_r2_arr'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_axis = np.arange(-300, 1000, 50) + 25\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# Plot function with mean and shaded std\n",
    "def plot_with_shade(x, data, color, label, alpha = 1, alpha_shade=0.3, linestyle='-'):\n",
    "    mean = np.mean(data, axis=1)\n",
    "    std = np.std(data, axis=1)\n",
    "    ax.plot(x, mean, linestyle + 'o', color=color,label=label, alpha= alpha)\n",
    "    ax.fill_between(x, mean - std, mean + std, color=color, alpha=alpha_shade)\n",
    "\n",
    "# Plotting each dataset\n",
    "# plot_with_shade(lag_axis, cd_bump_r2, 'green', 'CD',alpha = .1, alpha_shade=.1)\n",
    "# plot_with_shade(lag_axis, fb_bump_r2, 'magenta', 'FB',alpha = .1, alpha_shade=.1)\n",
    "# plot_with_shade(lag_axis, SC_cd_fb_bump_r2, 'brown', 'CD_FB',alpha = .1, alpha_shade=.1)\n",
    "\n",
    "\n",
    "plot_with_shade(lag_axis, cd_bump_r2, 'green', 'CD')\n",
    "plot_with_shade(lag_axis, fb_bump_r2, 'magenta', 'FB')\n",
    "\n",
    "plot_with_shade(lag_axis, cd_fb_bump_r2, 'brown', 'CD_FB')\n",
    "plot_with_shade(lag_axis, neural_spikes_bump_r2, 'grey', 'neurons', alpha = .5, alpha_shade=.5,linestyle='--')\n",
    "plot_with_shade(lag_axis, neural_pc_bump_r2, 'lightgrey', 'PCs', alpha = .5, alpha_shade=.5,linestyle='--')\n",
    "plot_with_shade(lag_axis, bump_shuffle_r2_arr, 'k', 'shuffle', alpha_shade=.1,linestyle='--')\n",
    "\n",
    "\n",
    "# Axes labels and aesthetics\n",
    "plt.xlabel('Time after bump onset (ms)')\n",
    "plt.ylabel('R²')\n",
    "# plt.legend(fontsize=8)\n",
    "plt.xlim([-210, 610])\n",
    "plt.ylim([-0.1, 1.0])\n",
    "plt.axvline(0, color='k', linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show\n",
    "plt.savefig(figDir + monkey + '_cdfb_diffdir_r2_reachbump.pdf', dpi='figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(lag_axis,bump_coefs_arr[:,0,3:7],label='FB-x',color='magenta')\n",
    "ax.plot(lag_axis,bump_coefs_arr[:,0,0:2],label='CD-x',color='green')\n",
    "ax.plot(lag_axis,bump_coefs_arr[:,0,7:11],label='FB-y',color='grey')\n",
    "ax.plot(lag_axis,bump_coefs_arr[:,0,2],label='CD-y',color='k')\n",
    "# plt.legend()\n",
    "plt.title('bump x-dir decoder weights')\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "# Create custom legend handles\n",
    "custom_lines = [\n",
    "    Line2D([0], [0], color='magenta', lw=2, label='FB-x'),\n",
    "    Line2D([0], [0], color='green', lw=2, label='CD-x'),\n",
    "    Line2D([0], [0], color='grey', lw=2, label='FB-y'),\n",
    "    Line2D([0], [0], color='k', lw=2, label='CD-y'),\n",
    "]\n",
    "plt.axvline(0, color='k', linestyle='--')\n",
    "# Add custom legend\n",
    "ax.legend(handles=custom_lines, fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(lag_axis,bump_coefs_arr[:,1,3:7],label='FB-x',color='grey')\n",
    "ax.plot(lag_axis,bump_coefs_arr[:,1,0:2],label='CD-x',color='k')\n",
    "ax.plot(lag_axis,bump_coefs_arr[:,1,7:11],label='FB-y',color='magenta')\n",
    "ax.plot(lag_axis,bump_coefs_arr[:,1,2],label='CD-y',color='green')\n",
    "# plt.legend()\n",
    "plt.title('bump y-dir decoder weights')\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "# Create custom legend handles\n",
    "custom_lines = [\n",
    "    Line2D([0], [0], color='grey', lw=2, label='FB-x'),\n",
    "    Line2D([0], [0], color='k', lw=2, label='CD-x'),\n",
    "    Line2D([0], [0], color='magenta', lw=2, label='FB-y'),\n",
    "    Line2D([0], [0], color='green', lw=2, label='CD-y'),\n",
    "]\n",
    "plt.axvline(0, color='k', linestyle='--')\n",
    "# Add custom legend\n",
    "ax.legend(handles=custom_lines, fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### move align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 20\n",
    "pred_range = (-300, 1000)\n",
    "x_field = 'spikes'\n",
    "mask = nan_mask\n",
    "n_trials = nan_n_trials\n",
    "cond_dict = nan_cond_dict\n",
    "dim = dataset.data[x_field].shape[1]\n",
    "nan_df = dataset_50ms.make_trial_data(align_field='move_onset_time', align_range=pred_range, ignored_trials=~mask)\n",
    "\n",
    "dirs = [dataset_50ms.trial_info[dataset_50ms.trial_info.trial_id == i]['cond_dir'] for i in nan_df.trial_id]\n",
    "cos_x = [round(math.cos(math.radians(i)),3) for i in dirs]\n",
    "sin_y = [round(math.sin(math.radians(i)),3) for i in dirs]\n",
    "cos_sin = np.array([cos_x, sin_y]).T\n",
    "nan_trial_PCA = nan_df[x_field].to_numpy().reshape((n_trials, -1, dim))\n",
    "print(nan_trial_PCA.shape)\n",
    "nan_trial_ang = cos_sin.reshape((n_trials, -1, 2))\n",
    "# act_trial_ang = np.array(cos_x).reshape((n_trials,-1,1))\n",
    "# act_trial_ang = np.array(sin_y).reshape((n_trials,-1,1))\n",
    "print(nan_trial_ang.shape)\n",
    "n_bins = nan_trial_PCA.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_cond_r2_arr = nans([n_bins, n_splits])\n",
    "for i in range(n_bins):\n",
    "    X = nan_trial_PCA[:,i,:]\n",
    "    Y = nan_trial_ang[:,i,:]\n",
    "    std=np.nanstd(X,axis=0)  \n",
    "    std[std==0] = 1\n",
    "    X_proc = (X - np.nanmean(X,axis=0))/std\n",
    "    lr_all = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "    lr_all.fit(X_proc, Y)\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits)\n",
    "    for j, (training_set, test_set) in enumerate(sss.split(range(0,n_trials),cond_dict)):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test, y_train, y_test = procX_train_test(X,Y,training_set,test_set)\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_test)\n",
    "        # Combined R² over both components\n",
    "        ss_res_combined = np.sum((y_test - y_pred) ** 2)\n",
    "        ss_tot_combined = np.sum((y_test - np.mean(y_test, axis=0)) ** 2)\n",
    "        r2_combined = 1 - ss_res_combined / ss_tot_combined\n",
    "        nan_cond_r2_arr[i,j] = r2_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(nan_trial_ang)\n",
    "shuffle_r2_arr = nans([n_bins, n_splits])\n",
    "for i in range(n_bins):\n",
    "    X = nan_trial_PCA[:,i,:]\n",
    "    Y = nan_trial_ang[:,i,:]\n",
    "    std=np.nanstd(X,axis=0)  \n",
    "    std[std==0] = 1\n",
    "    X_proc = (X - np.nanmean(X,axis=0))/std\n",
    "    lr_all = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "    lr_all.fit(X_proc, Y)\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits)\n",
    "    for j, (training_set, test_set) in enumerate(sss.split(range(0,n_trials),cond_dict)):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test, y_train, y_test = procX_train_test(X,Y,training_set,test_set)\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_test)\n",
    "        # Combined R² over both components\n",
    "        ss_res_combined = np.sum((y_test - y_pred) ** 2)\n",
    "        ss_tot_combined = np.sum((y_test - np.mean(y_test, axis=0)) ** 2)\n",
    "        r2_combined = 1 - ss_res_combined / ss_tot_combined\n",
    "        shuffle_r2_arr[i,j] = r2_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_dict = nan_bump_cond_dict\n",
    "dirs = [dataset_50ms.trial_info[dataset_50ms.trial_info.trial_id == i]['bump_dir'] for i in nan_df.trial_id]\n",
    "cos_x = [round(math.cos(math.radians(i)),3) for i in dirs]\n",
    "sin_y = [round(math.sin(math.radians(i)),3) for i in dirs]\n",
    "cos_sin = np.array([cos_x, sin_y]).T\n",
    "nan_trial_ang = cos_sin.reshape((n_trials, -1, 2))\n",
    "print(nan_trial_ang.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_bump_r2_arr = nans([n_bins,n_splits])\n",
    "for i in range(n_bins):\n",
    "    X = nan_trial_PCA[:,i,:]\n",
    "    Y = nan_trial_ang[:,i,:]\n",
    "    std=np.nanstd(X,axis=0)  \n",
    "    std[std==0] = 1\n",
    "    X_proc = (X - np.nanmean(X,axis=0))/std\n",
    "    lr_all = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "    lr_all.fit(X_proc, Y)\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits)\n",
    "    for j, (training_set, test_set) in enumerate(sss.split(range(0,n_trials),cond_dict)):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test, y_train, y_test = procX_train_test(X,Y,training_set,test_set)\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_test)\n",
    "        # Combined R² over both components\n",
    "        ss_res_combined = np.sum((y_test - y_pred) ** 2)\n",
    "        ss_tot_combined = np.sum((y_test - np.mean(y_test, axis=0)) ** 2)\n",
    "        r2_combined = 1 - ss_res_combined / ss_tot_combined\n",
    "        nan_bump_r2_arr[i,j] = r2_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd_nan_cond_r2 = nan_cond_r2_arr\n",
    "# cd_nan_bump_r2 = nan_bump_r2_arr\n",
    "# fb_nan_cond_r2 = nan_cond_r2_arr\n",
    "# fb_nan_bump_r2 = nan_bump_r2_arr\n",
    "# cd_fb_nan_cond_r2 = nan_cond_r2_arr\n",
    "# cd_fb_nan_bump_r2 = nan_bump_r2_arr\n",
    "# neural_pc_nan_cond_r2 = nan_cond_r2_arr\n",
    "# neural_pc_nan_bump_r2 = nan_bump_r2_arr\n",
    "neural_nan_cond_r2 = nan_cond_r2_arr\n",
    "neural_nan_bump_r2 = nan_bump_r2_arr\n",
    "shuffle_r2 = shuffle_r2_arr\n",
    "\n",
    "np.savez(monkey+'_v6_cdfb_reachbump_dir_r2_unsmoothed100', \n",
    "         cd_nan_cond_r2 = cd_nan_cond_r2, cd_nan_bump_r2 = cd_nan_bump_r2, \\\n",
    "         fb_nan_cond_r2 = fb_nan_cond_r2,fb_nan_bump_r2 = fb_nan_bump_r2, \\\n",
    "         cd_fb_nan_cond_r2 = cd_fb_nan_cond_r2,cd_fb_nan_bump_r2 = cd_fb_nan_bump_r2,\\\n",
    "         neural_pc_nan_cond_r2 = neural_pc_nan_cond_r2, neural_pc_nan_bump_r2 = neural_pc_nan_bump_r2,\\\n",
    "         neural_nan_cond_r2 = neural_nan_cond_r2, neural_nan_bump_r2 = neural_nan_bump_r2,\\\n",
    "         shuffle_r2=shuffle_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey = 'Duncan_20190710'\n",
    "data = np.load(monkey+'_v6_cdfb_reachbump_dir_r2_unsmoothed100.npz')\n",
    "data.files\n",
    "cd_act_r2 = data['cd_nan_cond_r2'] \n",
    "cd_pas_r2=data['cd_nan_bump_r2'] \n",
    "fb_act_r2=data['fb_nan_cond_r2'] \n",
    "fb_pas_r2=data['fb_nan_bump_r2'] \n",
    "cd_fb_act_r2=data['cd_fb_nan_cond_r2'] \n",
    "cd_fb_pas_r2=data['cd_fb_nan_bump_r2'] \n",
    "neural_spikes_act_r2=data['neural_nan_cond_r2'] \n",
    "neural_spikes_pas_r2=data['neural_nan_bump_r2'] \n",
    "shuffle_r2 = data['shuffle_r2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_axis = np.arange(-300, 1000, 50) + 25\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# Plot function with mean and shaded std\n",
    "def plot_with_shade(x, data, color, label, alpha=1, linestyle='-'):\n",
    "    mean = np.mean(data, axis=1)\n",
    "    std = np.std(data, axis=1)\n",
    "    ax.plot(x, mean, linestyle + 'o', color=color, alpha = alpha, label=label)\n",
    "    ax.fill_between(x, mean - std, mean + std, color=color, alpha=0.3)\n",
    "\n",
    "# Plotting each dataset\n",
    "plot_with_shade(lag_axis, cd_act_r2, 'green', 'CD')\n",
    "plot_with_shade(lag_axis, fb_act_r2, 'magenta', 'FB')\n",
    "plot_with_shade(lag_axis, cd_fb_act_r2, 'brown', 'CD_FB')\n",
    "plot_with_shade(lag_axis, neural_spikes_act_r2, 'grey', 'Neural', alpha=0.3,linestyle='--')\n",
    "plot_with_shade(lag_axis, shuffle_r2, 'k', 'shuffle', alpha=0.1,linestyle='--')\n",
    "\n",
    "\n",
    "# Axes labels and aesthetics\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "plt.ylabel('R²')\n",
    "plt.legend(fontsize=8)\n",
    "plt.xlim([-210, 1010])\n",
    "plt.ylim([-0.1, 1.0])\n",
    "plt.axvline(0, color='k', linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show\n",
    "plt.savefig(figDir + monkey + '_cdfb_dir_r2_reachbump_cond.pdf', dpi='figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_axis = np.arange(-300, 1000, 50) + 25\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# Plot function with mean and shaded std\n",
    "def plot_with_shade(x, data, color, label, alpha=1, linestyle='-'):\n",
    "    mean = np.mean(data, axis=1)\n",
    "    std = np.std(data, axis=1)\n",
    "    ax.plot(x, mean, linestyle + 'o', color=color, alpha = alpha, label=label)\n",
    "    ax.fill_between(x, mean - std, mean + std, color=color, alpha=0.3)\n",
    "\n",
    "# Plotting each dataset\n",
    "plot_with_shade(lag_axis, cd_pas_r2, 'green', 'CD')\n",
    "plot_with_shade(lag_axis, fb_pas_r2, 'magenta', 'FB')\n",
    "plot_with_shade(lag_axis, cd_fb_pas_r2, 'brown', 'CD_FB')\n",
    "plot_with_shade(lag_axis, neural_spikes_pas_r2, 'grey', 'Neural', alpha=0.3,linestyle='--')\n",
    "plot_with_shade(lag_axis, shuffle_r2, 'k', 'shuffle', alpha=0.1,linestyle='--')\n",
    "\n",
    "\n",
    "# Axes labels and aesthetics\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "plt.ylabel('R²')\n",
    "plt.legend(fontsize=8)\n",
    "plt.xlim([-210, 1010])\n",
    "plt.ylim([-0.1, 1.0])\n",
    "plt.axvline(0, color='k', linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show\n",
    "plt.savefig(figDir + monkey + '_cdfb_dir_r2_reachbump_bump.pdf', dpi='figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active and Passive trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_range = (-300, 1000)\n",
    "x_field = 'CD_FB_proj'\n",
    "pos_bool = True\n",
    "mask = active_mask\n",
    "n_trials = active_n_trials\n",
    "cond_dict = active_cond_dict\n",
    "dim = dataset_50ms.data[x_field].shape[1]\n",
    "active_df = dataset_50ms.make_trial_data(align_field='move_onset_time', align_range=pred_range, ignored_trials=~mask)\n",
    "dirs = [dataset_50ms.trial_info[dataset_50ms.trial_info.trial_id == i]['cond_dir'] for i in active_df.trial_id]\n",
    "cos_x = [round(math.cos(math.radians(i)),3) for i in dirs]\n",
    "sin_y = [round(math.sin(math.radians(i)),3) for i in dirs]\n",
    "cos_sin = np.array([cos_x, sin_y]).T\n",
    "act_trial_PCA = active_df[x_field].to_numpy().reshape((n_trials, -1, dim))\n",
    "print(act_trial_PCA.shape)\n",
    "act_trial_ang = cos_sin.reshape((n_trials, -1, 2))\n",
    "# act_trial_ang = np.array(cos_x).reshape((n_trials,-1,1))\n",
    "# act_trial_ang = np.array(sin_y).reshape((n_trials,-1,1))\n",
    "\n",
    "print(act_trial_ang.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = act_trial_PCA.shape[1]\n",
    "n_splits = 20\n",
    "act_coefs_arr = nans([n_bins, 2, dim])\n",
    "act_offset_arr = nans([n_bins, 2])\n",
    "act_r2_arr = nans([n_bins, n_splits])\n",
    "act_r2_xy_arr = nans([n_bins,2, n_splits])\n",
    "\n",
    "for i in range(n_bins):\n",
    "    X = act_trial_PCA[:,i,:]\n",
    "    Y = act_trial_ang[:,i,:]\n",
    "    std=np.nanstd(X,axis=0)  \n",
    "    std[std==0] = 1\n",
    "    X_proc = (X - np.nanmean(X,axis=0))/std\n",
    "    lr_all = GridSearchCV(Ridge(positive=pos_bool), {'alpha': np.logspace(-3, 3, 7)})\n",
    "    lr_all.fit(X_proc, Y)\n",
    "    act_coefs_arr[i,:,:] = lr_all.best_estimator_.coef_\n",
    "    act_offset_arr[i,:] = lr_all.best_estimator_.intercept_\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits)\n",
    "    for j, (training_set, test_set) in enumerate(sss.split(range(0,n_trials),cond_dict)):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test, y_train, y_test = procX_train_test(X,Y,training_set,test_set)\n",
    "        lr = GridSearchCV(Ridge(positive=pos_bool), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_test)\n",
    "        # Separate R² for each dimension (x and y)\n",
    "        r2_x = 1 - np.sum((y_test[:, 0] - y_pred[:, 0]) ** 2) / np.sum((y_test[:, 0] - np.mean(y_test[:, 0])) ** 2)\n",
    "        r2_y = 1 - np.sum((y_test[:, 1] - y_pred[:, 1]) ** 2) / np.sum((y_test[:, 1] - np.mean(y_test[:, 1])) ** 2)\n",
    "        act_r2_xy_arr[i,:,j] = [r2_x, r2_y]\n",
    "\n",
    "        # Combined R² over both components\n",
    "        ss_res_combined = np.sum((y_test - y_pred) ** 2)\n",
    "        ss_tot_combined = np.sum((y_test - np.mean(y_test, axis=0)) ** 2)\n",
    "        r2_combined = 1 - ss_res_combined / ss_tot_combined\n",
    "        act_r2_arr[i,j] = r2_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_bins = act_trial_PCA.shape[1]\n",
    "# np.random.shuffle(act_trial_ang)\n",
    "# act_shuffle_r2_arr = nans([n_bins, n_splits])\n",
    "# for i in range(n_bins):\n",
    "#     X = act_trial_PCA[:,i,:]\n",
    "#     Y = act_trial_ang[:,i,:]\n",
    "#     std=np.nanstd(X,axis=0)  \n",
    "#     std[std==0] = 1\n",
    "#     X_proc = (X - np.nanmean(X,axis=0))/std\n",
    "#     lr_all = GridSearchCV(Ridge(positive=pos_bool), {'alpha': np.logspace(-3, 3, 7)})\n",
    "#     lr_all.fit(X_proc, Y)\n",
    "#     act_coefs_arr[i,:,:] = lr_all.best_estimator_.coef_\n",
    "#     act_offset_arr[i,:] = lr_all.best_estimator_.intercept_\n",
    "#     sss = StratifiedShuffleSplit(n_splits=n_splits)\n",
    "#     for j, (training_set, test_set) in enumerate(sss.split(range(0,n_trials),cond_dict)):\n",
    "#         #split training and testing by trials\n",
    "#         X_train, X_test, y_train, y_test = procX_train_test(X,Y,training_set,test_set)\n",
    "#         lr = GridSearchCV(Ridge(positive=pos_bool), {'alpha': np.logspace(-3, 3, 7)})\n",
    "#         lr.fit(X_train, y_train)\n",
    "#         y_pred = lr.predict(X_test)\n",
    "#         # Combined R² over both components\n",
    "#         ss_res_combined = np.sum((y_test - y_pred) ** 2)\n",
    "#         ss_tot_combined = np.sum((y_test - np.mean(y_test, axis=0)) ** 2)\n",
    "#         r2_combined = 1 - ss_res_combined / ss_tot_combined\n",
    "#         act_shuffle_r2_arr[i,j] = r2_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_range = (-300, 1000)\n",
    "mask = passive_mask\n",
    "n_trials = passive_n_trials\n",
    "cond_dict = passive_cond_dict\n",
    "passive_df = dataset_50ms.make_trial_data(align_field='move_onset_time', align_range=pred_range, ignored_trials=~mask)\n",
    "dirs = [dataset_50ms.trial_info[dataset_50ms.trial_info.trial_id == i]['cond_dir'] for i in passive_df.trial_id]\n",
    "cos_x = [round(math.cos(math.radians(i)),3) for i in dirs]\n",
    "sin_y = [round(math.sin(math.radians(i)),3) for i in dirs]\n",
    "cos_sin = np.array([cos_x, sin_y]).T\n",
    "pas_trial_PCA = passive_df[x_field].to_numpy().reshape((n_trials, -1, dim))\n",
    "print(pas_trial_PCA.shape)\n",
    "pas_trial_ang = cos_sin.reshape((n_trials, -1, 2))\n",
    "# pas_trial_ang = np.array(cos_x).reshape((n_trials,-1,1))\n",
    "# pas_trial_ang = np.array(sin_y).reshape((n_trials,-1,1))\n",
    "\n",
    "print(pas_trial_ang.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas_coefs_arr = nans([n_bins, 2, dim])\n",
    "pas_offset_arr = nans([n_bins, 2])\n",
    "pas_r2_arr = nans([n_bins,n_splits])\n",
    "pas_r2_xy_arr = nans([n_bins,2,n_splits])\n",
    "\n",
    "for i in range(n_bins):\n",
    "    X = pas_trial_PCA[:,i,:]\n",
    "    Y = pas_trial_ang[:,i,:]\n",
    "    std=np.nanstd(X,axis=0)  \n",
    "    std[std==0] = 1\n",
    "    X_proc = (X - np.nanmean(X,axis=0))/std\n",
    "    lr_all = GridSearchCV(Ridge(positive=pos_bool), {'alpha': np.logspace(-3, 3, 7)})\n",
    "    lr_all.fit(X_proc, Y)\n",
    "    pas_coefs_arr[i,:,:] = lr_all.best_estimator_.coef_\n",
    "    pas_offset_arr[i,:] = lr_all.best_estimator_.intercept_\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits)\n",
    "    for j, (training_set, test_set) in enumerate(sss.split(range(0,n_trials),cond_dict)):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test, y_train, y_test = procX_train_test(X,Y,training_set,test_set)\n",
    "        lr = GridSearchCV(Ridge(positive=pos_bool), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_test)\n",
    "        # Separate R² for each dimension (x and y)\n",
    "        r2_x = 1 - np.sum((y_test[:, 0] - y_pred[:, 0]) ** 2) / np.sum((y_test[:, 0] - np.mean(y_test[:, 0])) ** 2)\n",
    "        r2_y = 1 - np.sum((y_test[:, 1] - y_pred[:, 1]) ** 2) / np.sum((y_test[:, 1] - np.mean(y_test[:, 1])) ** 2)\n",
    "        pas_r2_xy_arr[i,:,j] = [r2_x, r2_y]\n",
    "        # Combined R² over both components\n",
    "        ss_res_combined = np.sum((y_test - y_pred) ** 2)\n",
    "        ss_tot_combined = np.sum((y_test - np.mean(y_test, axis=0)) ** 2)\n",
    "        r2_combined = 1 - ss_res_combined / ss_tot_combined\n",
    "        pas_r2_arr[i,j] = r2_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.shuffle(pas_trial_ang)\n",
    "# pas_shuffle_r2_arr = nans([n_bins, n_splits])\n",
    "# for i in range(n_bins):\n",
    "#     X = pas_trial_PCA[:,i,:]\n",
    "#     Y = pas_trial_ang[:,i,:]\n",
    "#     std=np.nanstd(X,axis=0)  \n",
    "#     std[std==0] = 1\n",
    "#     X_proc = (X - np.nanmean(X,axis=0))/std\n",
    "#     lr_all = GridSearchCV(Ridge(positive=pos_bool), {'alpha': np.logspace(-3, 3, 7)})\n",
    "#     lr_all.fit(X_proc, Y)\n",
    "#     sss = StratifiedShuffleSplit(n_splits=n_splits)\n",
    "#     for j, (training_set, test_set) in enumerate(sss.split(range(0,n_trials),cond_dict)):\n",
    "#         #split training and testing by trials\n",
    "#         X_train, X_test, y_train, y_test = procX_train_test(X,Y,training_set,test_set)\n",
    "#         lr = GridSearchCV(Ridge(positive=pos_bool), {'alpha': np.logspace(-3, 3, 7)})\n",
    "#         lr.fit(X_train, y_train)\n",
    "#         y_pred = lr.predict(X_test)\n",
    "#         # Combined R² over both components\n",
    "#         ss_res_combined = np.sum((y_test - y_pred) ** 2)\n",
    "#         ss_tot_combined = np.sum((y_test - np.mean(y_test, axis=0)) ** 2)\n",
    "#         r2_combined = 1 - ss_res_combined / ss_tot_combined\n",
    "#         pas_shuffle_r2_arr[i,j] = r2_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd_act_r2 = act_r2_arr\n",
    "# cd_pas_r2 = pas_r2_arr\n",
    "# fb_act_r2 = act_r2_arr\n",
    "# fb_pas_r2 = pas_r2_arr\n",
    "# cd_fb_act_r2 = act_r2_arr\n",
    "# cd_fb_pas_r2 = pas_r2_arr\n",
    "# SC_cd_act_r2 = act_r2_arr\n",
    "# SC_cd_pas_r2 = pas_r2_arr\n",
    "# SC_fb_act_r2 = act_r2_arr\n",
    "# SC_fb_pas_r2 = pas_r2_arr\n",
    "SC_cd_fb_act_r2 = act_r2_arr\n",
    "SC_cd_fb_pas_r2 = pas_r2_arr\n",
    "\n",
    "# neural_pc_act_r2 = act_r2_arr\n",
    "# neural_pc_pas_r2 = pas_r2_arr\n",
    "# neural_spikes_act_r2 = act_r2_arr\n",
    "# neural_spikes_pas_r2 = pas_r2_arr\n",
    "# act_shuffle_r2_arr = act_shuffle_r2_arr\n",
    "# pas_shuffle_r2_arr = pas_shuffle_r2_arr\n",
    "\n",
    "\n",
    "# emg_act_r2 = act_r2_arr\n",
    "# emg_pas_r2 = pas_r2_arr\n",
    "# muscle_pc_act_r2 = act_r2_arr\n",
    "# muscle_pc_pas_r2 = pas_r2_arr\n",
    "# joint_pc_act_r2 = act_r2_arr\n",
    "# joint_pc_pas_r2 = pas_r2_arr\n",
    "\n",
    "\n",
    "np.savez(monkey+'_v6_zscore_cdfb_dir_r2_unsmoothed', \n",
    "         cd_act_r2 = cd_act_r2, cd_pas_r2 = cd_pas_r2, \\\n",
    "         fb_act_r2 = fb_act_r2,fb_pas_r2 = fb_pas_r2, \\\n",
    "         cd_fb_act_r2 = cd_fb_act_r2,cd_fb_pas_r2 = cd_fb_pas_r2,\\\n",
    "         SC_cd_act_r2 = SC_cd_act_r2, SC_cd_pas_r2 = SC_cd_pas_r2, \\\n",
    "         SC_fb_act_r2 = SC_fb_act_r2,SC_fb_pas_r2 = SC_fb_pas_r2, \\\n",
    "         SC_cd_fb_act_r2 = SC_cd_fb_act_r2,SC_cd_fb_pas_r2 = SC_cd_fb_pas_r2,\\\n",
    "         neural_pc_act_r2 = neural_pc_act_r2, neural_pc_pas_r2 = neural_pc_pas_r2,\\\n",
    "         neural_spikes_act_r2 = neural_spikes_act_r2, neural_spikes_pas_r2 = neural_spikes_pas_r2,\\\n",
    "         act_shuffle_r2_arr = act_shuffle_r2_arr, pas_shuffle_r2_arr = pas_shuffle_r2_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monkey = \"Han_20171207\"\n",
    "monkey = 'Duncan_20190710'\n",
    "\n",
    "data = np.load(monkey+'_v6_zscore_cdfb_dir_r2_unsmoothed.npz')\n",
    "data.files\n",
    "cd_act_r2 = data['cd_act_r2'] \n",
    "cd_pas_r2=data['cd_pas_r2'] \n",
    "fb_act_r2=data['fb_act_r2'] \n",
    "fb_pas_r2=data['fb_pas_r2'] \n",
    "cd_fb_act_r2=data['cd_fb_act_r2'] \n",
    "cd_fb_pas_r2=data['cd_fb_pas_r2'] \n",
    "SC_cd_act_r2 = data['SC_cd_act_r2'] \n",
    "SC_cd_pas_r2=data['SC_cd_pas_r2'] \n",
    "SC_fb_act_r2=data['SC_fb_act_r2'] \n",
    "SC_fb_pas_r2=data['SC_fb_pas_r2'] \n",
    "SC_cd_fb_act_r2=data['SC_cd_fb_act_r2'] \n",
    "SC_cd_fb_pas_r2=data['SC_cd_fb_pas_r2'] \n",
    "neural_pc_act_r2 = data['neural_pc_act_r2']\n",
    "neural_pc_pas_r2 = data['neural_pc_pas_r2']\n",
    "neural_spikes_act_r2=data['neural_spikes_act_r2'] \n",
    "neural_spikes_pas_r2=data['neural_spikes_pas_r2'] \n",
    "act_shuffle_r2_arr = data['act_shuffle_r2_arr']\n",
    "pas_shuffle_r2_arr = data['pas_shuffle_r2_arr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_axis = np.arange(-300, 1000, 50) + 25\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# Plot function with mean and shaded std\n",
    "def plot_with_shade(x, data, color, label, alpha = 1, alpha_shade=0.3, linestyle='-'):\n",
    "    mean = np.mean(data, axis=1)\n",
    "    std = np.std(data, axis=1)\n",
    "    ax.plot(x, mean, linestyle + 'o', color=color,label=label, alpha= alpha)\n",
    "    ax.fill_between(x, mean - std, mean + std, color=color, alpha=alpha_shade)\n",
    "\n",
    "# Plotting each dataset\n",
    "# plot_with_shade(lag_axis, cd_act_r2, 'green', 'CD',alpha = .1, alpha_shade=.1)\n",
    "# plot_with_shade(lag_axis, fb_act_r2, 'magenta', 'FB',alpha = .1, alpha_shade=.1)\n",
    "# plot_with_shade(lag_axis, SC_cd_fb_act_r2, 'brown', 'CD_FB',alpha = .1, alpha_shade=.1)\n",
    "\n",
    "\n",
    "plot_with_shade(lag_axis, cd_act_r2, 'green', 'CD')\n",
    "plot_with_shade(lag_axis, fb_act_r2, 'magenta', 'FB')\n",
    "\n",
    "plot_with_shade(lag_axis, cd_fb_act_r2, 'brown', 'CD_FB')\n",
    "plot_with_shade(lag_axis, neural_spikes_act_r2, 'grey', 'neurons', alpha = .5, alpha_shade=.5,linestyle='--')\n",
    "plot_with_shade(lag_axis, neural_pc_act_r2, 'lightgrey', 'PCs', alpha = .5, alpha_shade=.5,linestyle='--')\n",
    "plot_with_shade(lag_axis, act_shuffle_r2_arr, 'k', 'shuffle', alpha_shade=.1,linestyle='--')\n",
    "\n",
    "\n",
    "# Axes labels and aesthetics\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "plt.ylabel('R²')\n",
    "# plt.legend(fontsize=8)\n",
    "plt.xlim([-210, 1010])\n",
    "plt.ylim([-0.1, 1.0])\n",
    "plt.axvline(0, color='k', linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show\n",
    "plt.savefig(figDir + monkey + '_cdfb_dir_r2_active.pdf', dpi='figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_axis = np.arange(-300, 1000, 50) + 25\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# Plotting each dataset\n",
    "# plot_with_shade(lag_axis, cd_pas_r2, 'green', 'CD',alpha = .1, alpha_shade=.1)\n",
    "# plot_with_shade(lag_axis, fb_pas_r2, 'magenta', 'FB',alpha = .1, alpha_shade=.1)\n",
    "# plot_with_shade(lag_axis, SC_cd_fb_pas_r2, 'brown', 'CD_FB',alpha = .1, alpha_shade=.1)\n",
    "\n",
    "\n",
    "plot_with_shade(lag_axis, cd_pas_r2, 'green', 'CD')\n",
    "plot_with_shade(lag_axis, fb_pas_r2, 'magenta', 'FB')\n",
    "\n",
    "plot_with_shade(lag_axis, cd_fb_pas_r2, 'brown', 'CD_FB')\n",
    "plot_with_shade(lag_axis, neural_spikes_pas_r2, 'grey', 'neurons', alpha = .5, alpha_shade=.5,linestyle='--')\n",
    "plot_with_shade(lag_axis, neural_pc_pas_r2, 'lightgrey', 'PCs', alpha = .5, alpha_shade=.5,linestyle='--')\n",
    "\n",
    "plot_with_shade(lag_axis, pas_shuffle_r2_arr, 'k', 'shuffle', alpha_shade=.1,linestyle='--')\n",
    "\n",
    "\n",
    "# Axes labels and aesthetics\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "plt.ylabel('R²')\n",
    "# plt.legend(fontsize=8)\n",
    "plt.xlim([-210, 610])\n",
    "plt.ylim([-0.1, 1.0])\n",
    "plt.axvline(0, color='k', linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show\n",
    "plt.savefig(figDir + monkey + '_cdfb_dir_r2_passive.pdf', dpi='figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_pc_act_r2 = act_r2_arr\n",
    "# neural_pc_pas_r2 = pas_r2_arr\n",
    "# neural_pc_act_xy_r2 = act_r2_xy_arr\n",
    "# neural_pc_pas_xy_r2 = pas_r2_xy_arr\n",
    "# vel_act_r2 = act_r2_arr\n",
    "# vel_pas_r2 = pas_r2_arr\n",
    "# vel_act_xy_r2 = act_r2_xy_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_axis = np.arange(-300,1000,50)+25\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(lag_axis,neural_spikes_act_r2, \"o\",color = 'k', label = 'Active',markersize=10)\n",
    "plt.plot(lag_axis,neural_spikes_pas_r2, \"o\",color = 'red', label = 'Passive',markersize=10)\n",
    "\n",
    "# plt.plot(lag_axis,neural_pc_act_r2, \"-o\",color = 'k', label = 'Active, 20PC')\n",
    "# plt.plot(lag_axis,neural_pc_act_xy_r2[:,0], \"-o\",label = 'Active, 20PC,x-dir')\n",
    "# plt.plot(lag_axis,neural_pc_act_xy_r2[:,1], \"-o\", label = 'Active, 20PC,y-dir')\n",
    "# plt.plot(lag_axis,vel_act_r2,\"-o\", color = 'gray',label = 'Active, velocity')\n",
    "# plt.plot(lag_axis,vel_act_xy_r2[:,0], \"-o\",color = 'gray',label = 'Active, velocity,x-dir')\n",
    "# plt.plot(lag_axis,vel_act_xy_r2[:,1], \"-o\", color = 'gray',label = 'Active, velocity,y-dir')\n",
    "\n",
    "# plt.plot(lag_axis,neural_pc_pas_r2,\"-o\", color = 'red', label = 'Passive, 20PC')\n",
    "# plt.plot(lag_axis,pas_shuffle_r2_arr, color = 'red', ls='--',label = 'Passive shuffle')\n",
    "# plt.plot(lag_axis,vel_pas_r2,\"-o\", color = 'gray',label = 'Passive, velocity')\n",
    "\n",
    "plt.plot(lag_axis,act_shuffle_r2_arr,  \"-\",color = 'grey', label = 'shuffle')\n",
    "\n",
    "plt.xlabel('Time after movement onset (ms)'); plt.ylabel('Cross-validated R$^2$'); \n",
    "# plt.title('Direction r2')\n",
    "# plt.legend(fontsize=8)\n",
    "plt.xlim([-300, 610])\n",
    "plt.ylim([-0.1, 1])\n",
    "\n",
    "plt.axvline(0, color = 'k',linestyle = '--')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_direction_r2.pdf',dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-lag decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_10ms\n",
    "dataset.data.keys().unique(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'spikes'\n",
    "data = np.load(monkey+'_v6_zscore_smoothed_cdfb_data_'+x_field+'.npz')\n",
    "data.files\n",
    "dataset.add_continuous_data(data['CD_FB_proj'],'CD_FB_proj')\n",
    "dataset.add_continuous_data(data['CD_proj'],'CD_proj')\n",
    "dataset.add_continuous_data(data['FB_proj'],'FB_proj')\n",
    "dataset.data.keys().unique(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_field = 'spikes'\n",
    "# data = np.load(monkey+'_v6_qsignal_smoothed100_cdfb_data_'+x_field+'.npz')\n",
    "# data.files\n",
    "# dataset.add_continuous_data(data['CD_FB_proj'],'CD_FB_proj')\n",
    "# dataset.add_continuous_data(data['CD_proj'],'CD_proj')\n",
    "# dataset.add_continuous_data(data['FB_proj'],'FB_proj')\n",
    "# dataset.add_continuous_data(data['FBq_proj'],'FBq_proj')\n",
    "# dataset.data.keys().unique(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = (False, 180.0)\n",
    "cond_mask = (dataset_10ms.trial_info['ctr_hold_bump'] == cond[0]) & \\\n",
    "            (dataset_10ms.trial_info['cond_dir']%360==cond[1]) & \\\n",
    "            (dataset_10ms.trial_info.split != 'none')\n",
    "align_range = (-200, 1000)\n",
    "cond_data = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=align_range, ignored_trials=~cond_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_x, cd_y, fb_x, fb_y, vx, vy, neural = [],[],[],[],[],[],[]\n",
    "for idx, trial in cond_data.groupby('trial_id'):\n",
    "    neural.append(trial['spikes_smth_40'].to_numpy()[:,:])\n",
    "    cd_x.append(trial['CD_proj'].to_numpy()[:,:2])\n",
    "    cd_y.append(trial['CD_proj'].to_numpy()[:,2])\n",
    "    fb_x.append(trial['FB_proj'].to_numpy()[:,:4])\n",
    "    fb_y.append(trial['FB_proj'].to_numpy()[:,4:])\n",
    "    vx.append(trial['hand_vel'].to_numpy()[:,0])\n",
    "    vy.append(trial['hand_vel'].to_numpy()[:,1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.arctan2(np.array(vy), np.array(vx))           \n",
    "cos_theta = np.cos(theta)           \n",
    "sin_theta = np.sin(theta)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_axis = np.arange(align_range[0],align_range[1],dataset_10ms.bin_width)\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "neural_var = np.sum(np.var(np.array(neural),axis=0),axis=1)\n",
    "\n",
    "# plt.plot(t_axis,np.sum(np.var(np.array(cd_x),axis=0),axis=1),color='green', label='CD_x')\n",
    "# plt.plot(t_axis,np.var(np.array(cd_y),axis=0),color='k',label='CD_y')\n",
    "# plt.plot(t_axis,np.sum(np.var(np.array(fb_x),axis=0),axis=1),color='magenta', label='FB_x')\n",
    "# plt.plot(t_axis,np.sum(np.var(np.array(fb_y),axis=0),axis=1),color='grey',label='FB_y')\n",
    "\n",
    "plt.plot(t_axis,np.sum(np.var(np.array(cd_x),axis=0),axis=1)/neural_var,color='green', label='CD_x')\n",
    "plt.plot(t_axis,np.var(np.array(cd_y),axis=0)/neural_var,color='k',label='CD_y')\n",
    "plt.plot(t_axis,np.sum(np.var(np.array(fb_x),axis=0),axis=1)/neural_var,color='magenta', label='FB_x')\n",
    "plt.plot(t_axis,np.sum(np.var(np.array(fb_y),axis=0),axis=1)/neural_var,color='grey',label='FB_y')\n",
    "# plt.plot(t_axis,neural_var,color='red',label='neural')\n",
    "\n",
    "# plt.plot(t_axis,np.var(np.array(vel_x),axis=0),color='blue', label='vel_x')\n",
    "# plt.plot(t_axis,np.var(np.array(vel_y),axis=0),color='yellow',label='vel_y')\n",
    "plt.legend()\n",
    "plt.title('180 deg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.plot(t_axis,np.var(np.array(vx),axis=0),color='blue', label='vel_x')\n",
    "plt.plot(t_axis,np.var(np.array(vy),axis=0),color='yellow',label='vel_y')\n",
    "plt.legend()\n",
    "plt.title('270 deg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State space plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 16 conditions, in the format (ctr_hold_bump, cond_dir)\n",
    "active_unique_conditions = [(False, 0.0), (False, 45.0), (False, 90.0), (False, 135.0),\n",
    "                     (False, 180.0), (False, 225.0), (False, 270.0), (False, 315.0)]\n",
    "passive_unique_conditions = [(True, 0.0), (True, 45.0), (True, 90.0), (True, 135.0),\n",
    "                     (True, 180.0), (True, 225.0), (True, 270.0), (True, 315.0)]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax_0 = fig.add_subplot(221) \n",
    "ax_1 = fig.add_subplot(222, projection='3d')\n",
    "\n",
    "var1_name = ['CD_proj']\n",
    "indices = [0,0,1]\n",
    "align_range = (-100, 200)\n",
    "t0_index = np.argwhere(np.arange(align_range[0], align_range[1],dataset.bin_width)==0)[0,0]\n",
    "\n",
    "var_name = ['hand_vel']\n",
    "for cond in active_unique_conditions:\n",
    "    # Filter out invalid trials (labeled 'none') and trials in other conditions\n",
    "    cond_mask = (dataset.trial_info['ctr_hold_bump'] == cond[0]) & \\\n",
    "                (dataset.trial_info['cond_dir']%360==cond[1]) & \\\n",
    "                (dataset.trial_info.split != 'none')\n",
    "\n",
    "    # Extract relevant portion of selected trials\n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=align_range, ignored_trials=~cond_mask)\n",
    "    all_x, all_y = [], []\n",
    "    for idx, trial in cond_data.groupby('trial_id'):\n",
    "        all_x.append(trial[var_name].to_numpy()[:,0])\n",
    "        all_y.append(trial[var_name].to_numpy()[:,1])\n",
    "    avg_x = np.mean(np.vstack(all_x), axis=0)\n",
    "    avg_y = np.mean(np.vstack(all_y), axis=0)\n",
    "\n",
    "    ax_0.plot(avg_x, avg_y,color=plt.cm.hsv(cond[1] / 360), linewidth=1.5)\n",
    "    ax_0.scatter(avg_x[t0_index], avg_y[t0_index],  color=plt.cm.hsv(cond[1] / 360), edgecolor='black', s=80, zorder=3)\n",
    "    arrow_idx = -5\n",
    "    ax_0.quiver(avg_x[arrow_idx], avg_y[arrow_idx], \n",
    "        avg_x[arrow_idx+1] - avg_x[arrow_idx], \n",
    "        avg_y[arrow_idx+1] - avg_y[arrow_idx], \n",
    "        angles='xy', scale_units='xy', scale=1, color=plt.cm.hsv(cond[1] / 360), linewidth=3)\n",
    "        \n",
    "for cond in active_unique_conditions:\n",
    "    # Filter out invalid trials (labeled 'none') and trials in other conditions\n",
    "    cond_mask = (dataset.trial_info['ctr_hold_bump'] == cond[0]) & \\\n",
    "                (dataset.trial_info['cond_dir']%360==cond[1]) & \\\n",
    "                (dataset.trial_info.split != 'none')\n",
    "    \n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=align_range, ignored_trials=~cond_mask)\n",
    "    \n",
    "    all_x, all_y, all_z = [], [], []\n",
    "    for idx, trial in cond_data.groupby('trial_id'):\n",
    "        all_x.append(trial[var1_name].to_numpy()[:,indices[0]])\n",
    "        all_y.append(trial[var1_name].to_numpy()[:,indices[1]])\n",
    "        all_z.append(trial[var1_name].to_numpy()[:,indices[2]])\n",
    "    \n",
    "    avg_x = np.mean(np.vstack(all_x), axis=0)\n",
    "    avg_y = np.mean(np.vstack(all_y), axis=0)\n",
    "    avg_z = np.mean(np.vstack(all_z), axis=0)\n",
    "        \n",
    "    ax_1.plot(avg_x, avg_y, avg_z, color=plt.cm.hsv(cond[1] / 360), linewidth=1.5)\n",
    "    ax_1.scatter(avg_x[t0_index], avg_y[t0_index], avg_z[t0_index], color=plt.cm.hsv(cond[1] / 360), edgecolor='black', s=80, zorder=3)\n",
    "    arrow_idx = -5\n",
    "    ax_1.quiver(avg_x[arrow_idx], avg_y[arrow_idx], avg_z[arrow_idx], \n",
    "        avg_x[arrow_idx+1] - avg_x[arrow_idx], \n",
    "        avg_y[arrow_idx+1] - avg_y[arrow_idx], \n",
    "        avg_z[arrow_idx+1] - avg_z[arrow_idx], \n",
    "        color=plt.cm.hsv(cond[1] / 360), linewidth=3, arrow_length_ratio=1)\n",
    "\n",
    "# ax_2 = fig.add_subplot(223) \n",
    "ax_2 = fig.add_subplot(223, sharex=ax_0, sharey=ax_0) \n",
    "ax_3 = fig.add_subplot(224, projection='3d')\n",
    "\n",
    "align_range = (-100, 120)\n",
    "t0_index = np.argwhere(np.arange(align_range[0], align_range[1],dataset.bin_width)==0)[0,0]\n",
    "\n",
    "for cond in passive_unique_conditions:\n",
    "    # Filter out invalid trials (labeled 'none') and trials in other conditions\n",
    "    cond_mask = (dataset.trial_info['ctr_hold_bump'] == cond[0]) & \\\n",
    "                (dataset.trial_info['cond_dir']%360==cond[1]) & \\\n",
    "                (dataset.trial_info.split != 'none')\n",
    "\n",
    "    # Extract relevant portion of selected trials\n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=align_range, ignored_trials=~cond_mask)\n",
    "    all_x, all_y = [], []\n",
    "    for idx, trial in cond_data.groupby('trial_id'):\n",
    "        all_x.append(trial[var_name].to_numpy()[:,0])\n",
    "        all_y.append(trial[var_name].to_numpy()[:,1])\n",
    "    avg_x = np.mean(np.vstack(all_x), axis=0)\n",
    "    avg_y = np.mean(np.vstack(all_y), axis=0)\n",
    "\n",
    "    ax_2.plot(avg_x, avg_y,color=plt.cm.hsv(cond[1] / 360), linewidth=1.5)\n",
    "    ax_2.scatter(avg_x[t0_index], avg_y[t0_index],  color=plt.cm.hsv(cond[1] / 360), edgecolor='black', s=80, zorder=3)\n",
    "    arrow_idx = -5\n",
    "    ax_2.quiver(avg_x[arrow_idx], avg_y[arrow_idx], \n",
    "        avg_x[arrow_idx+1] - avg_x[arrow_idx], \n",
    "        avg_y[arrow_idx+1] - avg_y[arrow_idx], \n",
    "        angles='xy', scale_units='xy', scale=1, color=plt.cm.hsv(cond[1] / 360), linewidth=3)\n",
    "\n",
    "for cond in passive_unique_conditions:\n",
    "    # Filter out invalid trials (labeled 'none') and trials in other conditions\n",
    "    cond_mask = (dataset.trial_info['ctr_hold_bump'] == cond[0]) & \\\n",
    "                (dataset.trial_info['cond_dir']%360==cond[1]) & \\\n",
    "                (dataset.trial_info.split != 'none')\n",
    "    \n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=align_range, ignored_trials=~cond_mask)\n",
    "    \n",
    "    all_x, all_y, all_z = [], [], []\n",
    "    for idx, trial in cond_data.groupby('trial_id'):\n",
    "        all_x.append(trial[var1_name].to_numpy()[:,indices[0]])\n",
    "        all_y.append(trial[var1_name].to_numpy()[:,indices[1]])\n",
    "        all_z.append(trial[var1_name].to_numpy()[:,indices[2]])\n",
    "    \n",
    "    avg_x = np.mean(np.vstack(all_x), axis=0)\n",
    "    avg_y = np.mean(np.vstack(all_y), axis=0)\n",
    "    avg_z = np.mean(np.vstack(all_z), axis=0)\n",
    "        \n",
    "    ax_3.plot(avg_x, avg_y, avg_z, color=plt.cm.hsv(cond[1] / 360), linewidth=1.5)\n",
    "    ax_3.scatter(avg_x[t0_index], avg_y[t0_index], avg_z[t0_index], color=plt.cm.hsv(cond[1] / 360), edgecolor='black', s=80, zorder=3)\n",
    "    arrow_idx = -5\n",
    "    ax_3.quiver(avg_x[arrow_idx], avg_y[arrow_idx], avg_z[arrow_idx], \n",
    "        avg_x[arrow_idx+1] - avg_x[arrow_idx], \n",
    "        avg_y[arrow_idx+1] - avg_y[arrow_idx], \n",
    "        avg_z[arrow_idx+1] - avg_z[arrow_idx], \n",
    "        color=plt.cm.hsv(cond[1] / 360), linewidth=3, arrow_length_ratio=1)\n",
    "x_min = min(ax_1.get_xlim()[0], ax_3.get_xlim()[0])\n",
    "x_max = max(ax_1.get_xlim()[1], ax_3.get_xlim()[1])\n",
    "y_min = min(ax_1.get_ylim()[0], ax_3.get_ylim()[0])\n",
    "y_max = max(ax_1.get_ylim()[1], ax_3.get_ylim()[1])\n",
    "z_min = min(ax_1.get_zlim()[0], ax_3.get_zlim()[0])\n",
    "z_max = max(ax_1.get_zlim()[1], ax_3.get_zlim()[1])\n",
    "\n",
    "# Apply the same limits to both 3D plots\n",
    "ax_1.set_xlim(x_min, x_max)\n",
    "ax_1.set_ylim(y_min, y_max)\n",
    "ax_1.set_zlim(z_min, z_max)\n",
    "\n",
    "ax_3.set_xlim(x_min, x_max)\n",
    "ax_3.set_ylim(y_min, y_max)\n",
    "ax_3.set_zlim(z_min, z_max)    \n",
    "    \n",
    "ax_1.set_xticks([]); ax_1.set_yticks([]); ax_1.set_zticks([]); \n",
    "ax_3.set_xticks([]); ax_3.set_yticks([]); ax_3.set_zticks([]); \n",
    "ax_0.set_xticks([]); ax_0.set_yticks([]); \n",
    "ax_2.set_xticks([]); ax_2.set_yticks([]);\n",
    "ax_0.grid(False); ax_1.grid(False);  ax_2.grid(False) ; ax_3.grid(False)\n",
    "# ax_1.set_xlabel('X')\n",
    "# ax_1.set_ylabel('Y')\n",
    "# ax_1.set_zlabel('Z')\n",
    "\n",
    "ax_0.spines['top'].set_visible(False); ax_0.spines['right'].set_visible(False); ax_0.spines['bottom'].set_visible(False); ax_0.spines['left'].set_visible(False)\n",
    "ax_2.spines['top'].set_visible(False); ax_2.spines['right'].set_visible(False); ax_2.spines['bottom'].set_visible(False); ax_2.spines['left'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_cd_traj_200120.pdf',dpi = 'figure')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'spikes'\n",
    "data = np.load(monkey+'_v6_alt_zscore_unsmoothed_cdfb_weights_'+x_field+'.npz')\n",
    "data.files\n",
    "data['CD_axes'].shape\n",
    "\n",
    "# data = np.load(monkey+'_v6_unsmoothed100_cdfb_weights_'+x_field+'.npz')\n",
    "# data.files\n",
    "# data['CD_axes'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dims = 3 \n",
    "all_data = np.array(dataset_10ms.data.spikes_smth_40)\n",
    "print(all_data.shape)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(all_data)\n",
    "pca = PCA(n_components=n_dims,random_state = 42)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project all neural activity data into PC space\n",
    "PCA_data = pca.transform(X)\n",
    "print(PCA_data.shape)\n",
    "dataset_10ms.add_continuous_data(PCA_data,'PCA')\n",
    "print('PCA total var explained:',sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dims = 3 \n",
    "all_data = np.array(dataset_10ms.data.spikes_smth_40)\n",
    "print(all_data.shape)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(all_data)\n",
    "pca = PCA(n_components=n_dims,random_state = 42)\n",
    "pca.fit(X)\n",
    "\n",
    "v1 = pca.transform(PCA(n_components=2,random_state = 42).fit_transform(data['CD_axes'].T).T)\n",
    "print(v1)\n",
    "v1 /= np.linalg.norm(v1, axis=1, keepdims=True)\n",
    "v1\n",
    "\n",
    "v2 = pca.transform(PCA(n_components=2,random_state = 42).fit_transform(data['FB_axes'].T).T)\n",
    "print(v2)\n",
    "v2 /= np.linalg.norm(v2, axis=1, keepdims=True)\n",
    "v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2 = pca.transform(PCA(n_components=2,random_state = 42).fit_transform(data['FB_axes'].T).T)\n",
    "print(v2)\n",
    "v2 /= np.linalg.norm(v2, axis=1, keepdims=True)\n",
    "v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "%matplotlib widget\n",
    "\n",
    "active_unique_conditions = [(False, 0.0), (False, 45.0), (False, 90.0), (False, 135.0),\n",
    "                     (False, 180.0), (False, 225.0), (False, 270.0), (False, 315.0)]\n",
    "cmap = plt.get_cmap('coolwarm',len(active_unique_conditions))\n",
    "custom_palette = [mpl.colors.rgb2hex(cmap(i)) for i in range(len(active_unique_conditions))]\n",
    "\n",
    "align_range = (-100, 500)\n",
    "t0_index = np.argwhere(np.arange(align_range[0], align_range[1],dataset.bin_width)==0)[0,0]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "var_name = ['PCA']\n",
    "j=0\n",
    "for cond in active_unique_conditions:\n",
    "    # Filter out invalid trials (labeled 'none') and trials in other conditions\n",
    "    cond_mask = (dataset.trial_info['ctr_hold_bump'] == cond[0]) & \\\n",
    "                (dataset.trial_info['cond_dir']%360==cond[1]) & \\\n",
    "                (dataset.trial_info.split != 'none')\n",
    "    \n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=align_range, ignored_trials=~cond_mask)\n",
    "    \n",
    "    all_x, all_y, all_z = [], [], []\n",
    "    for idx, trial in cond_data.groupby('trial_id'):\n",
    "        all_x.append(trial[var_name].to_numpy()[:,0])\n",
    "        all_y.append(trial[var_name].to_numpy()[:,1])\n",
    "        all_z.append(trial[var_name].to_numpy()[:,2])\n",
    "    \n",
    "    avg_x = np.mean(np.vstack(all_x), axis=0)\n",
    "    avg_y = np.mean(np.vstack(all_y), axis=0)\n",
    "    avg_z = np.mean(np.vstack(all_z), axis=0)\n",
    "        \n",
    "    # Plot trajectories before and after time zero in different colors\n",
    "    ax.plot(avg_x[:t0_index], avg_y[:t0_index], avg_z[:t0_index], color=custom_palette[j], linewidth=1.5, label=f'Before Time Zero - {cond[1]}°')\n",
    "    ax.plot(avg_x[t0_index:], avg_y[t0_index:], avg_z[t0_index:], color=custom_palette[j], linewidth=1.5, linestyle='--', label=f'After Time Zero - {cond[1]}°')\n",
    "\n",
    "    # Scatter plot to mark the position at time zero\n",
    "    ax.scatter(avg_x[t0_index], avg_y[t0_index], avg_z[t0_index], color=custom_palette[j], s=40, zorder=3)\n",
    "    j+=1\n",
    "    \n",
    "u = np.linspace(-1, 1, 10)\n",
    "v = np.linspace(-1, 1, 10)\n",
    "U, V = np.meshgrid(u, v)\n",
    "\n",
    "# Calculate the range of the neural data\n",
    "min_x, max_x = np.min(avg_x), np.max(avg_x)\n",
    "min_y, max_y = np.min(avg_y), np.max(avg_y)\n",
    "min_z, max_z = np.min(avg_z), np.max(avg_z)\n",
    "\n",
    "# Scale factors (optional, adjust based on how you want the planes to appear)\n",
    "# scale_factor_cd = [7,5,5]\n",
    "# trans_cd = [0,0,0]\n",
    "# scale_factor_fb = [5,5,7]\n",
    "# trans_fb = [7,0,0]\n",
    "\n",
    "scale_factor_cd = [6,2,1]\n",
    "trans_cd = [0,1,-2]\n",
    "scale_factor_fb = [10,10,10]\n",
    "trans_fb = [5,5,0]\n",
    "\n",
    "# Plot CD plane (scaled and translated)\n",
    "X_cd = U * v1[0,0] + V * v1[1,0]\n",
    "Y_cd = U * v1[0,1] + V * v1[1,1]\n",
    "Z_cd = U * v1[0,2] + V * v1[1,2]\n",
    "\n",
    "ax.plot_surface(X_cd * scale_factor_cd[0] + trans_cd[0], \n",
    "                Y_cd * scale_factor_cd[1] + trans_cd[1], \n",
    "                Z_cd * scale_factor_cd[2] + trans_cd[2], \n",
    "                color='green', alpha=0.2)   \n",
    "\n",
    "# Plot FB plane (scaled and translated)\n",
    "X_fb = U * v2[0,0] + V * v2[1,0]\n",
    "Y_fb = U * v2[0,1] + V * v2[1,1]\n",
    "Z_fb = U * v2[0,2] + V * v2[1,2]\n",
    "\n",
    "ax.plot_surface(X_fb * scale_factor_fb[0] + trans_fb[0], \n",
    "                Y_fb * scale_factor_fb[1] + trans_fb[1], \n",
    "                Z_fb * scale_factor_fb[2] + trans_fb[2], \n",
    "                color='magenta', alpha=0.2)\n",
    "# params = [3.116883116883127, -66.62337662337663]\n",
    "# params = [1.558,-59.99]\n",
    "\n",
    "params = [24.935064935065018, -102.46753246753245]\n",
    "# params = [29.610389610389664, -99.35064935064926]\n",
    "# params = [-19.09090909090899, -97.40259740259721]\n",
    "ax.view_init(elev=params[0], azim=params[1])\n",
    "# Labels and title\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "ax.set_xlim(-5, 10)\n",
    "ax.set_ylim(-7, 7)\n",
    "ax.set_zlim(-7, 7)\n",
    "# Remove the grid\n",
    "ax.grid(False)\n",
    "\n",
    "# Remove the axis labels\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_zticks([])\n",
    "# ax.set_title('Neural Trajectories and CD Subspace')\n",
    "plt.show()\n",
    "\n",
    "# print(ax.elev, ax.azim) #in separate cell\n",
    "fig.savefig(figDir+monkey+\"_trajectory_3d_plot.pdf\", format=\"pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ax.elev, ax.azim) #in separate cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Straight decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'CD_FB_proj'\n",
    "y_field ='hand_vel'\n",
    "lag_axis = np.arange(-300,320,20)\n",
    "pred_range = (-100, 120)\n",
    "# trial_mask = active_mask\n",
    "# cond_dict = active_cond_dict\n",
    "trial_mask = passive_mask\n",
    "cond_dict = passive_cond_dict\n",
    "dataset=dataset_10ms\n",
    "dim = dataset.data[x_field].shape[1]\n",
    "print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# monkey = \"Han_20171207\"\n",
    "monkey = 'Duncan_20190710'\n",
    "data = np.load(monkey+'_MC_norm_zscore_smooth40_spikes_hand_vel_pas_r2s.npz')\n",
    "print(data.files)\n",
    "r2_cd_fb = data['r2_cd_fb']\n",
    "mean_cd_fb = np.nanmean(r2_cd_fb, axis=1)\n",
    "std_cd_fb = np.nanstd(r2_cd_fb, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_axis = np.arange(-300,320,20)\n",
    "\n",
    "lag_axis[np.argmax(np.nanmean(data['r2_cd_only'], axis=1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5.5, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(lag_axis, mean_cd_fb, linewidth=3, color='brown', label='cd+fb')\n",
    "plt.fill_between(lag_axis, mean_cd_fb - std_cd_fb, mean_cd_fb + std_cd_fb, color='brown',alpha=0.3)\n",
    "plt.ylim([-0.1, 0.85])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lag = lag_axis[np.argmax(mean_cd_fb)]\n",
    "print(best_lag)\n",
    "best_r2 = np.max(mean_cd_fb)\n",
    "print(best_r2)\n",
    "good_r2 = np.max(mean_cd_fb) * 0.95\n",
    "print(good_r2)\n",
    "good_lag = lag_axis[np.argwhere(mean_cd_fb>=good_r2)[0,0]]\n",
    "print(good_lag)\n",
    "good_r2 = np.max(mean_cd_fb) * 0.9\n",
    "print(good_r2)\n",
    "good_lag = lag_axis[np.argwhere(mean_cd_fb>=good_r2)[0,0]]\n",
    "print(good_lag)\n",
    "good_r2 = np.max(mean_cd_fb) * 0.8\n",
    "print(good_r2)\n",
    "good_lag = lag_axis[np.argwhere(mean_cd_fb>=good_r2)[0,0]]\n",
    "print(good_lag)\n",
    "good_r2 = np.max(mean_cd_fb) * 0.7\n",
    "print(good_r2)\n",
    "good_lag = lag_axis[np.argwhere(mean_cd_fb>=good_r2)[0,0]]\n",
    "print(good_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = 0\n",
    "norm_x = True\n",
    "_, _,_,vel_df,_ = fit_and_predict_MC(dataset, trial_mask, 'move_onset_time',pred_range, lag, x_field, y_field, norm_x=norm_x, pos_bool = False,split_pred = True, n_cd_dims = 3,cond_dict=cond_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for plotting\n",
    "plot_dir = [0.0, 90.0, 180.0, 270.0] # limit plot directions to reduce cluttering\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "var_name = 'cd_pred_vel'\n",
    "plot_dim = 'y' # plot x velocity \n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "x_axis = np.arange(-100,120,dataset.bin_width)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir %360 == trial_dir].trial_id\n",
    "    all_traces = []\n",
    "    for _, trial in vel_df[np.isin(vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        all_traces.append(trial[var_name][plot_dim].to_numpy())\n",
    "    avg_trace = np.mean(np.vstack(all_traces), axis=0)\n",
    "    plt.plot(x_axis, avg_trace, color=color, linewidth=3)\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "plt.ylabel('Velocity (cm/s)')\n",
    "plt.ylim([-15, 15])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_cd_y_95_early_active.pdf',dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_name = 'fb_pred_vel'\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir %360 == trial_dir].trial_id\n",
    "    all_traces = []\n",
    "    for _, trial in vel_df[np.isin(vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        all_traces.append(trial[var_name][plot_dim].to_numpy())\n",
    "    avg_trace = np.mean(np.vstack(all_traces), axis=0)\n",
    "    plt.plot(x_axis, avg_trace, color=color, linewidth=3)\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "plt.ylabel('Velocity (cm/s)')\n",
    "plt.ylim([-15, 15])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_fb_y_95_early_active.pdf',dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_name = 'pred_vel'\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir %360 == trial_dir].trial_id\n",
    "    all_traces = []\n",
    "    for _, trial in vel_df[np.isin(vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        all_traces.append(trial[var_name][plot_dim].to_numpy())\n",
    "    avg_trace = np.mean(np.vstack(all_traces), axis=0)\n",
    "    plt.plot(x_axis, avg_trace, color=color, linewidth=3)\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "plt.ylabel('Velocity (cm/s)')\n",
    "plt.ylim([-15,15])\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_y_95_pred_passive.pdf',dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_name = 'hand_vel'\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir %360 == trial_dir].trial_id\n",
    "    all_traces = []\n",
    "    for _, trial in vel_df[np.isin(vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        all_traces.append(trial[var_name][plot_dim].to_numpy())\n",
    "    avg_trace = np.mean(np.vstack(all_traces), axis=0)\n",
    "    plt.plot(x_axis, avg_trace, color=color, linewidth=3)\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "plt.ylabel('Velocity (cm/s)')\n",
    "plt.ylim([-15, 15])\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_y_95_true_passive.pdf',dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.data.keys().unique(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'FBq_proj'\n",
    "y_field ='hand_vel'\n",
    "lag_axis = np.arange(-300,320,20)\n",
    "\n",
    "pred_range = (-100, 1000)\n",
    "trial_mask = active_mask\n",
    "cond_dict = active_cond_dict\n",
    "\n",
    "# pred_range = (-100, 120)\n",
    "# trial_mask = passive_mask\n",
    "# cond_dict = passive_cond_dict\n",
    "dataset=dataset_10ms\n",
    "dim = dataset.data[x_field].shape[1]\n",
    "print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_x = True \n",
    "n_splits = 20\n",
    "pos_bool = False\n",
    "r2_array_MC = nans([len(lag_axis),n_splits]); \n",
    "r2_feature_array = nans([len(lag_axis),n_splits,dataset.data[y_field].shape[1]])\n",
    "coef_array = nans([len(lag_axis),dataset.data[y_field].shape[1],dim])\n",
    "for i in range(len(lag_axis)):\n",
    "    lag = lag_axis[i]\n",
    "    print(int(i/len(lag_axis)*100),'%')\n",
    "    r2, coef,_,vel_df,r2_arr = fit_and_predict_MC(dataset, trial_mask, 'move_onset_time',pred_range, lag, x_field, y_field, norm_x=norm_x, pos_bool = pos_bool, cond_dict=cond_dict)\n",
    "    r2_array_MC[i,:] = r2; r2_feature_array[i,:,:] = r2_arr\n",
    "    coef_array[i,:,:] = coef\n",
    "time_max = lag_axis[np.argmax(np.mean(r2_array_MC,axis=1))]\n",
    "print(np.max(np.mean(r2_array_MC,axis=1)))\n",
    "print(time_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_r2_cd_only = r2_feature_array[:,:,0]\n",
    "# y_r2_cd_only = r2_feature_array[:,:,1]\n",
    "# r2_cd_only = r2_array_MC\n",
    "# cd_coefs = coef_array\n",
    "\n",
    "# x_r2_fbq_only = r2_feature_array[:,:,0]\n",
    "# y_r2_fbq_only = r2_feature_array[:,:,1]\n",
    "# r2_fbq_only = r2_array_MC\n",
    "# fbq_coefs = coef_array\n",
    "\n",
    "# x_r2_fb_only = r2_feature_array[:,:,0]\n",
    "# y_r2_fb_only = r2_feature_array[:,:,1]\n",
    "# r2_fb_only = r2_array_MC\n",
    "# fb_coefs = coef_array\n",
    "\n",
    "x_r2_cd_fb = r2_feature_array[:,:,0]\n",
    "y_r2_cd_fb = r2_feature_array[:,:,1]\n",
    "r2_cd_fb = r2_array_MC\n",
    "cd_fb_coefs = coef_array\n",
    "\n",
    "# SC_x_r2_cd_only = r2_feature_array[:,:,0]\n",
    "# SC_y_r2_cd_only = r2_feature_array[:,:,1]\n",
    "# SC_r2_cd_only = r2_array_MC\n",
    "# SC_cd_coefs = coef_array\n",
    "\n",
    "# SC_x_r2_fb_only = r2_feature_array[:,:,0]\n",
    "# SC_y_r2_fb_only = r2_feature_array[:,:,1]\n",
    "# SC_r2_fb_only = r2_array_MC\n",
    "# SC_fb_coefs = coef_array \n",
    "\n",
    "# SC_x_r2_cd_fb = r2_feature_array[:,:,0]\n",
    "# SC_y_r2_cd_fb = r2_feature_array[:,:,1]\n",
    "# SC_r2_cd_fb = r2_array_MC\n",
    "# SC_cd_fb_coefs = coef_array\n",
    "\n",
    "# x_r2_nrn = r2_feature_array[:,:,0]\n",
    "# y_r2_nrn = r2_feature_array[:,:,1]\n",
    "# r2_nrn = r2_array_MC\n",
    "\n",
    "# x_r2_pc = r2_feature_array[:,:,0]\n",
    "# y_r2_pc = r2_feature_array[:,:,1]\n",
    "# r2_pc = r2_array_MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(monkey+'_MC_alt_qsignal_smooth40_spikes_'+y_field +'_act_r2s', \\\n",
    "         x_r2_fbq_only = x_r2_fbq_only, y_r2_fbq_only = y_r2_fbq_only, r2_fbq_only = r2_fbq_only, fbq_coefs=fbq_coefs,\\\n",
    "         x_r2_fb_only = x_r2_fb_only, y_r2_fb_only = y_r2_fb_only, r2_fb_only = r2_fb_only, fb_coefs=fb_coefs,\\\n",
    "         x_r2_cd_fb = x_r2_cd_fb, y_r2_cd_fb = y_r2_cd_fb, r2_cd_fb = r2_cd_fb, cd_fb_coefs=cd_fb_coefs)\n",
    "\n",
    "# np.savez(monkey+'_MC_norm_zscore_smooth40_spikes_'+y_field +'_act_r2s', \\\n",
    "#          x_r2_cd_only = x_r2_cd_only, y_r2_cd_only = y_r2_cd_only, r2_cd_only = r2_cd_only, cd_coefs=cd_coefs,\\\n",
    "#          x_r2_fb_only = x_r2_fb_only, y_r2_fb_only = y_r2_fb_only, r2_fb_only = r2_fb_only, fb_coefs=fb_coefs,\\\n",
    "#          x_r2_cd_fb = x_r2_cd_fb, y_r2_cd_fb = y_r2_cd_fb, r2_cd_fb = r2_cd_fb, cd_fb_coefs=cd_fb_coefs,\\\n",
    "#          SC_x_r2_cd_only = SC_x_r2_cd_only, SC_y_r2_cd_only = SC_y_r2_cd_only, SC_r2_cd_only = SC_r2_cd_only, SC_cd_coefs=SC_cd_coefs,\\\n",
    "#          SC_x_r2_fb_only = SC_x_r2_fb_only, SC_y_r2_fb_only = SC_y_r2_fb_only, SC_r2_fb_only = SC_r2_fb_only, SC_fb_coefs=SC_fb_coefs,\\\n",
    "#          SC_x_r2_cd_fb = SC_x_r2_cd_fb, SC_y_r2_cd_fb = SC_y_r2_cd_fb, SC_r2_cd_fb = SC_r2_cd_fb, SC_cd_fb_coefs=SC_cd_fb_coefs,\\\n",
    "#          x_r2_nrn = x_r2_nrn, y_r2_nrn = y_r2_nrn, r2_nrn = r2_nrn,\\\n",
    "#          x_r2_pc = x_r2_pc, y_r2_pc = y_r2_pc, r2_pc = r2_pc)\n",
    "\n",
    "\n",
    "# monkey = \"Han_20171207\"\n",
    "# monkey = 'Duncan_20190710'\n",
    "\n",
    "# data = np.load(monkey+'_MC_norm_zscore_smooth40_spikes_hand_vel_act_r2s.npz')\n",
    "# condition = 'active'\n",
    "\n",
    "# ## avoid re-runs; norm and sign constraint don't factor\n",
    "# data = np.load(monkey+'_MC_norm_zscore_smooth40_spikes_hand_vel_act_r2s.npz')\n",
    "# data.files\n",
    "# x_r2_nrn = data['x_r2_nrn']; y_r2_nrn = data['y_r2_nrn']; r2_nrn = data['r2_nrn']; \n",
    "# x_r2_pc = data['x_r2_pc']; y_r2_pc = data['y_r2_pc']; r2_pc = data['r2_pc']; \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "lw = 3\n",
    "lag_axis = np.arange(-300, 320, 20)\n",
    "fig, ax = plt.subplots(figsize=(5.5, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "condition = 'active'\n",
    "\n",
    "# data = np.load(monkey+'_MC_norm_zscore_smooth40_spikes_hand_vel_act_r2s.npz')\n",
    "# # CD only\n",
    "# r2_cd = data['r2_cd_only']  # shape [n_lags, n_splits]\n",
    "# mean_cd = np.nanmean(r2_cd, axis=1)\n",
    "# std_cd = np.nanstd(r2_cd, axis=1)\n",
    "# plt.plot(lag_axis, mean_cd, linewidth=lw, color='green', label='cd')\n",
    "# plt.fill_between(lag_axis, mean_cd - std_cd, mean_cd + std_cd, color='green', alpha=0.3)\n",
    "\n",
    "# print(np.nanmax(mean_cd))\n",
    "# print(lag_axis[np.nanargmax(mean_cd)])\n",
    "\n",
    "data = np.load(monkey+'_MC_alt_qsignal_smooth40_spikes_hand_vel_act_r2s.npz')\n",
    "# FB only\n",
    "r2_fb = data['r2_fb_only']\n",
    "mean_fb = np.nanmean(r2_fb, axis=1)\n",
    "std_fb = np.nanstd(r2_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_fb, linewidth=lw, color='magenta', label='alt_fb')\n",
    "plt.fill_between(lag_axis, mean_fb - std_fb, mean_fb + std_fb, color='magenta', alpha=0.5)\n",
    "plt.axvline(lag_axis[np.argmax(mean_fb)], color = 'magenta', linestyle='--', alpha=0.5)\n",
    "print(np.nanmax(mean_fb))\n",
    "print(lag_axis[np.nanargmax(mean_fb)])\n",
    "\n",
    "r2_fb = data['r2_fbq_only']\n",
    "mean_fb = np.nanmean(r2_fb, axis=1)\n",
    "std_fb = np.nanstd(r2_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_fb, linewidth=lw, color='magenta', linestyle = '--',label='fb?',alpha=0.2)\n",
    "plt.fill_between(lag_axis, mean_fb - std_fb, mean_fb + std_fb, color='magenta', alpha=0.2)\n",
    "plt.axvline(lag_axis[np.argmax(mean_fb)], color = 'magenta', linestyle='--', alpha=0.2)\n",
    "print(np.nanmax(mean_fb))\n",
    "print(lag_axis[np.nanargmax(mean_fb)])\n",
    "\n",
    "# CD + FB\n",
    "# r2_cd_fb = data['r2_cd_fb']\n",
    "# mean_cd_fb = np.nanmean(r2_cd_fb, axis=1)\n",
    "# std_cd_fb = np.nanstd(r2_cd_fb, axis=1)\n",
    "# plt.plot(lag_axis, mean_cd_fb, linewidth=lw, color='brown', label='cd+fb')\n",
    "# plt.fill_between(lag_axis, mean_cd_fb - std_cd_fb, mean_cd_fb + std_cd_fb, color='brown', alpha=0.3)\n",
    "# print(np.nanmax(mean_cd_fb))\n",
    "# print(lag_axis[np.nanargmax(mean_cd_fb)])\n",
    "\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R²')\n",
    "plt.ylim([-0.05, 0.65])\n",
    "plt.legend(fontsize=8)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(figDir + monkey + '_alt_qsignal_'+ condition + '_r2.pdf',dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "lw = 3\n",
    "lag_axis = np.arange(-300, 320, 20)\n",
    "fig, ax = plt.subplots(figsize=(5.5, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    " \n",
    "# CD only\n",
    "# r2_cd = data['r2_cd_only']  # shape [n_lags, n_splits]\n",
    "# mean_cd = np.nanmean(r2_cd, axis=1)\n",
    "# std_cd = np.nanstd(r2_cd, axis=1)\n",
    "# plt.plot(lag_axis, mean_cd, linewidth=lw, color='green', label='cd',alpha=0.1)\n",
    "# plt.fill_between(lag_axis, mean_cd - std_cd, mean_cd + std_cd, color='green', alpha=0.1)\n",
    "\n",
    "# print(np.nanmax(mean_cd))\n",
    "# print(lag_axis[np.nanargmax(mean_cd)])\n",
    "\n",
    "r2_cd = data['SC_r2_cd_only']  # shape [n_lags, n_splits]\n",
    "mean_cd = np.nanmean(r2_cd, axis=1)\n",
    "std_cd = np.nanstd(r2_cd, axis=1)\n",
    "plt.plot(lag_axis, mean_cd, linewidth=lw, color='green', label='cd')\n",
    "plt.fill_between(lag_axis, mean_cd - std_cd, mean_cd + std_cd, color='green', alpha=0.3)\n",
    "\n",
    "print(np.nanmax(mean_cd))\n",
    "print(lag_axis[np.nanargmax(mean_cd)])\n",
    "\n",
    "# FB only\n",
    "# r2_fb = data['r2_fb_only']\n",
    "# mean_fb = np.nanmean(r2_fb, axis=1)\n",
    "# std_fb = np.nanstd(r2_fb, axis=1)\n",
    "# plt.plot(lag_axis, mean_fb, linewidth=lw, color='magenta', label='fb',alpha=0.1)\n",
    "# plt.fill_between(lag_axis, mean_fb - std_fb, mean_fb + std_fb, color='magenta', alpha=0.1)\n",
    "\n",
    "# print(np.nanmax(mean_fb))\n",
    "# print(lag_axis[np.nanargmax(mean_fb)])\n",
    "\n",
    "r2_fb = data['SC_r2_fb_only']\n",
    "mean_fb = np.nanmean(r2_fb, axis=1)\n",
    "std_fb = np.nanstd(r2_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_fb, linewidth=lw, color='magenta', label='fb')\n",
    "plt.fill_between(lag_axis, mean_fb - std_fb, mean_fb + std_fb, color='magenta', alpha=0.3)\n",
    "\n",
    "print(np.nanmax(mean_fb))\n",
    "print(lag_axis[np.nanargmax(mean_fb)])\n",
    "\n",
    "\n",
    "# CD + FB\n",
    "r2_cd_fb = data['SC_r2_cd_fb']\n",
    "mean_cd_fb = np.nanmean(r2_cd_fb, axis=1)\n",
    "std_cd_fb = np.nanstd(r2_cd_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_cd_fb, linewidth=lw, color='brown', label='cd+fb',alpha=0.1)\n",
    "plt.fill_between(lag_axis, mean_cd_fb - std_cd_fb, mean_cd_fb + std_cd_fb, color='brown', alpha=0.1)\n",
    "print(np.nanmax(mean_cd_fb))\n",
    "print(lag_axis[np.nanargmax(mean_cd_fb)])\n",
    "\n",
    "r2_cd_fb = data['r2_cd_fb']\n",
    "mean_cd_fb = np.nanmean(r2_cd_fb, axis=1)\n",
    "std_cd_fb = np.nanstd(r2_cd_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_cd_fb, linewidth=lw, color='brown', label='cd+fb')\n",
    "plt.fill_between(lag_axis, mean_cd_fb - std_cd_fb, mean_cd_fb + std_cd_fb, color='brown', alpha=0.3)\n",
    "print(np.nanmax(mean_cd_fb))\n",
    "print(lag_axis[np.nanargmax(mean_cd_fb)])\n",
    "\n",
    "# All nrns\n",
    "r2_nrn = data['r2_nrn']\n",
    "mean_nrn = np.nanmean(r2_nrn, axis=1)\n",
    "std_nrn = np.nanstd(r2_nrn, axis=1)\n",
    "plt.plot(lag_axis, mean_nrn, linewidth=lw, linestyle = '--',color='grey', label='neurons', alpha=0.5)\n",
    "plt.fill_between(lag_axis, mean_nrn - std_nrn, mean_nrn + std_nrn, color='grey', alpha=0.5)\n",
    "print(np.nanmax(mean_nrn))\n",
    "print(lag_axis[np.nanargmax(mean_nrn)])\n",
    "\n",
    "# 20 PC\n",
    "r2_pc = data['r2_pc']\n",
    "mean_pc = np.nanmean(r2_pc, axis=1)\n",
    "std_pc = np.nanstd(r2_pc, axis=1)\n",
    "plt.plot(lag_axis, mean_pc, linewidth=lw, linestyle = '--',color='lightgrey', label='PCs', alpha=0.5)\n",
    "plt.fill_between(lag_axis, mean_pc - std_pc, mean_pc + std_pc, color='lightgrey', alpha=0.5)\n",
    "print(np.nanmax(mean_pc))\n",
    "print(lag_axis[np.nanargmax(mean_pc)])\n",
    "plt.axvline(0, color = 'k', linestyle='--')\n",
    "\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R²')\n",
    "plt.ylim([-0.1, 0.899999999])\n",
    "# plt.legend(fontsize=8)\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(figDir + monkey + '_norm_zscore_'+ condition + '_r2.pdf',dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw = 3\n",
    "lag_axis = np.arange(-300, 320, 20)\n",
    "fig, ax = plt.subplots(figsize=(5.5, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# CD only\n",
    "# r2_cd = data['x_r2_cd_only']  # shape [n_lags, n_splits]\n",
    "# mean_cd = np.nanmean(r2_cd, axis=1)\n",
    "# std_cd = np.nanstd(r2_cd, axis=1)\n",
    "# plt.plot(lag_axis, mean_cd, linewidth=lw, color='green', label='cd', alpha=0.1)\n",
    "# plt.fill_between(lag_axis, mean_cd - std_cd, mean_cd + std_cd, color='green', alpha=0.1)\n",
    "\n",
    "# print(np.nanmax(mean_cd))\n",
    "# print(lag_axis[np.nanargmax(mean_cd)])\n",
    "\n",
    "r2_cd = data['SC_x_r2_cd_only']  # shape [n_lags, n_splits]\n",
    "mean_cd = np.nanmean(r2_cd, axis=1)\n",
    "std_cd = np.nanstd(r2_cd, axis=1)\n",
    "plt.plot(lag_axis, mean_cd, linewidth=lw, color='green', label='cd')\n",
    "plt.fill_between(lag_axis, mean_cd - std_cd, mean_cd + std_cd, color='green', alpha=0.3)\n",
    "\n",
    "print(np.nanmax(mean_cd))\n",
    "print(lag_axis[np.nanargmax(mean_cd)])\n",
    "\n",
    "# FB only\n",
    "# r2_fb = data['x_r2_fb_only']\n",
    "# mean_fb = np.nanmean(r2_fb, axis=1)\n",
    "# std_fb = np.nanstd(r2_fb, axis=1)\n",
    "# plt.plot(lag_axis, mean_fb, linewidth=lw, color='magenta', label='fb', alpha=0.1)\n",
    "# plt.fill_between(lag_axis, mean_fb - std_fb, mean_fb + std_fb, color='magenta', alpha=0.1)\n",
    "\n",
    "# print(np.nanmax(mean_fb))\n",
    "# print(lag_axis[np.nanargmax(mean_fb)])\n",
    "\n",
    "r2_fb = data['SC_x_r2_fb_only']\n",
    "mean_fb = np.nanmean(r2_fb, axis=1)\n",
    "std_fb = np.nanstd(r2_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_fb, linewidth=lw, color='magenta', label='fb')\n",
    "plt.fill_between(lag_axis, mean_fb - std_fb, mean_fb + std_fb, color='magenta', alpha=0.3)\n",
    "\n",
    "print(np.nanmax(mean_fb))\n",
    "print(lag_axis[np.nanargmax(mean_fb)])\n",
    "\n",
    "# # CD + FB\n",
    "# r2_cd_fb = data['SC_x_r2_cd_fb']\n",
    "# mean_cd_fb = np.nanmean(r2_cd_fb, axis=1)\n",
    "# std_cd_fb = np.nanstd(r2_cd_fb, axis=1)\n",
    "# plt.plot(lag_axis, mean_cd_fb, linewidth=lw, color='brown', label='cd+fb',alpha=0.1)\n",
    "# plt.fill_between(lag_axis, mean_cd_fb - std_cd_fb, mean_cd_fb + std_cd_fb, color='brown', alpha=0.1)\n",
    "\n",
    "# print(np.nanmax(mean_cd_fb))\n",
    "# print(lag_axis[np.nanargmax(mean_cd_fb)])\n",
    "\n",
    "r2_cd_fb = data['x_r2_cd_fb']\n",
    "mean_cd_fb = np.nanmean(r2_cd_fb, axis=1)\n",
    "std_cd_fb = np.nanstd(r2_cd_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_cd_fb, linewidth=lw, color='brown', label='cd+fb')\n",
    "plt.fill_between(lag_axis, mean_cd_fb - std_cd_fb, mean_cd_fb + std_cd_fb, color='brown', alpha=0.3)\n",
    "\n",
    "print(np.nanmax(mean_cd_fb))\n",
    "print(lag_axis[np.nanargmax(mean_cd_fb)])\n",
    "\n",
    "# All nrns\n",
    "r2_nrn = data['x_r2_nrn']\n",
    "mean_nrn = np.nanmean(r2_nrn, axis=1)\n",
    "std_nrn = np.nanstd(r2_nrn, axis=1)\n",
    "plt.plot(lag_axis, mean_nrn, linewidth=lw, linestyle = '--',color='grey', label='neurons', alpha=0.5)\n",
    "plt.fill_between(lag_axis, mean_nrn - std_nrn, mean_nrn + std_nrn, color='grey', alpha=0.5)\n",
    "print(np.nanmax(mean_nrn))\n",
    "print(lag_axis[np.nanargmax(mean_nrn)])\n",
    "\n",
    "# 20 PC\n",
    "r2_pc = data['x_r2_pc']\n",
    "mean_pc = np.nanmean(r2_pc, axis=1)\n",
    "std_pc = np.nanstd(r2_pc, axis=1)\n",
    "plt.plot(lag_axis, mean_pc, linewidth=lw, linestyle = '--',color='lightgrey', label='PCs', alpha=0.5)\n",
    "plt.fill_between(lag_axis, mean_pc - std_pc, mean_pc + std_pc, color='lightgrey', alpha=0.5)\n",
    "print(np.nanmax(mean_pc))\n",
    "print(lag_axis[np.nanargmax(mean_pc)])\n",
    "plt.axvline(0, color = 'k', linestyle='--')\n",
    "plt.ylim([-0.1, 0.899999999])\n",
    "\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('X R²')\n",
    "# plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(figDir + monkey + '_norm_zscore_'+ condition + '_x_r2.pdf',dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw = 3\n",
    "lag_axis = np.arange(-300, 320, 20)\n",
    "fig, ax = plt.subplots(figsize=(5.5, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# CD only\n",
    "# r2_cd = data['y_r2_cd_only']  # shape [n_lags, n_splits]\n",
    "# mean_cd = np.nanmean(r2_cd, axis=1)\n",
    "# std_cd = np.nanstd(r2_cd, axis=1)\n",
    "# plt.plot(lag_axis, mean_cd, linewidth=lw, color='green', label='cd', alpha=0.1)\n",
    "# plt.fill_between(lag_axis, mean_cd - std_cd, mean_cd + std_cd, color='green', alpha=0.1)\n",
    "\n",
    "# print(np.nanmax(mean_cd))\n",
    "# print(lag_axis[np.nanargmax(mean_cd)])\n",
    "\n",
    "r2_cd = data['SC_y_r2_cd_only']  # shape [n_lags, n_splits]\n",
    "mean_cd = np.nanmean(r2_cd, axis=1)\n",
    "std_cd = np.nanstd(r2_cd, axis=1)\n",
    "plt.plot(lag_axis, mean_cd, linewidth=lw, color='green', label='cd')\n",
    "plt.fill_between(lag_axis, mean_cd - std_cd, mean_cd + std_cd, color='green', alpha=0.3)\n",
    "\n",
    "print(np.nanmax(mean_cd))\n",
    "print(lag_axis[np.nanargmax(mean_cd)])\n",
    "\n",
    "# FB only\n",
    "# r2_fb = data['y_r2_fb_only']\n",
    "# mean_fb = np.nanmean(r2_fb, axis=1)\n",
    "# std_fb = np.nanstd(r2_fb, axis=1)\n",
    "# plt.plot(lag_axis, mean_fb, linewidth=lw, color='magenta', label='fb', alpha=0.1)\n",
    "# plt.fill_between(lag_axis, mean_fb - std_fb, mean_fb + std_fb, color='magenta', alpha=0.1)\n",
    "\n",
    "# print(np.nanmax(mean_fb))\n",
    "# print(lag_axis[np.nanargmax(mean_fb)])\n",
    "\n",
    "r2_fb = data['SC_y_r2_fb_only']\n",
    "mean_fb = np.nanmean(r2_fb, axis=1)\n",
    "std_fb = np.nanstd(r2_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_fb, linewidth=lw, color='magenta', label='fb')\n",
    "plt.fill_between(lag_axis, mean_fb - std_fb, mean_fb + std_fb, color='magenta', alpha=0.3)\n",
    "\n",
    "print(np.nanmax(mean_fb))\n",
    "print(lag_axis[np.nanargmax(mean_fb)])\n",
    "\n",
    "# CD + FB\n",
    "# r2_cd_fb = data['SC_y_r2_cd_fb']\n",
    "# mean_cd_fb = np.nanmean(r2_cd_fb, axis=1)\n",
    "# std_cd_fb = np.nanstd(r2_cd_fb, axis=1)\n",
    "# plt.plot(lag_axis, mean_cd_fb, linewidth=lw, color='brown', label='cd+fb',alpha=0.1)\n",
    "# plt.fill_between(lag_axis, mean_cd_fb - std_cd_fb, mean_cd_fb + std_cd_fb, color='brown', alpha=0.1)\n",
    "\n",
    "# print(np.nanmax(mean_cd_fb))\n",
    "# print(lag_axis[np.nanargmax(mean_cd_fb)])\n",
    "\n",
    "r2_cd_fb = data['y_r2_cd_fb']\n",
    "mean_cd_fb = np.nanmean(r2_cd_fb, axis=1)\n",
    "std_cd_fb = np.nanstd(r2_cd_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_cd_fb, linewidth=lw, color='brown', label='cd+fb')\n",
    "plt.fill_between(lag_axis, mean_cd_fb - std_cd_fb, mean_cd_fb + std_cd_fb, color='brown', alpha=0.3)\n",
    "\n",
    "print(np.nanmax(mean_cd_fb))\n",
    "print(lag_axis[np.nanargmax(mean_cd_fb)])\n",
    "\n",
    "\n",
    "# All nrns\n",
    "r2_nrn = data['y_r2_nrn']\n",
    "mean_nrn = np.nanmean(r2_nrn, axis=1)\n",
    "std_nrn = np.nanstd(r2_nrn, axis=1)\n",
    "plt.plot(lag_axis, mean_nrn, linewidth=lw, linestyle = '--',color='grey', label='neurons', alpha=0.3)\n",
    "plt.fill_between(lag_axis, mean_nrn - std_nrn, mean_nrn + std_nrn, color='grey', alpha=0.3)\n",
    "print(np.nanmax(mean_nrn))\n",
    "print(lag_axis[np.nanargmax(mean_nrn)])\n",
    "\n",
    "# 20 PC\n",
    "r2_pc = data['y_r2_pc']\n",
    "mean_pc = np.nanmean(r2_pc, axis=1)\n",
    "std_pc = np.nanstd(r2_pc, axis=1)\n",
    "plt.plot(lag_axis, mean_pc, linewidth=lw, linestyle = '--',color='lightgrey', label='PCs', alpha=0.5)\n",
    "plt.fill_between(lag_axis, mean_pc - std_pc, mean_pc + std_pc, color='lightgrey', alpha=0.5)\n",
    "print(np.nanmax(mean_pc))\n",
    "print(lag_axis[np.nanargmax(mean_pc)])\n",
    "plt.axvline(0, color = 'k', linestyle='--')\n",
    "plt.ylim([-0.1, 0.899999999])\n",
    "\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('Y R²')\n",
    "# plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_norm_zscore_'+ condition + '_y_r2.pdf',dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = '8020_CD_FB_proj_spikes_smth_40'\n",
    "y_field ='hand_vel'\n",
    "lag_axis = np.arange(-300,300,20)\n",
    "pred_range = (-100, 1000)\n",
    "trial_mask = active_mask\n",
    "cond_dict = active_cond_dict\n",
    "# pred_range = (-100, 500)\n",
    "# trial_mask = passive_mask\n",
    "# cond_dict = passive_cond_dict\n",
    "\n",
    "dim = dataset.data[x_field].shape[1]\n",
    "print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_range_arr = [(-100, 0),(0, 200),(200, 400),(400, 600),(600, 800),(800, 1000)]\n",
    "# pred_range_arr = [(-100, 0),(0, 100),(100, 200),(200, 300),(300, 400),(400, 500)]\n",
    "\n",
    "r2_array_dynam = nans([len(pred_range_arr),len(lag_axis)]); r2_feature_array_dynam = nans([len(pred_range_arr),len(lag_axis),dataset.data[y_field].shape[1]])\n",
    "coef_array_dynam = nans([len(pred_range_arr),len(lag_axis),dataset.data[y_field].shape[1],dim])\n",
    "time_max_array_dynam = nans([len(pred_range_arr)])\n",
    "vel_df_array_dynam = []\n",
    "for tw in range(len(pred_range_arr)):\n",
    "    tw_range = pred_range_arr[tw]\n",
    "    for i in range(len(lag_axis)):\n",
    "        lag = lag_axis[i]\n",
    "        r2, coef,_,vel_df,r2_arr = fit_and_predict(dataset, trial_mask, 'move_onset_time',tw_range, lag, x_field, y_field,cond_dict)\n",
    "        r2_array_dynam[tw,i] = r2; r2_feature_array_dynam[tw,i,:] = r2_arr\n",
    "        coef_array_dynam[tw,i,:,:] = coef\n",
    "    t_max = lag_axis[np.argmax(r2_array_dynam[tw,:])]\n",
    "    print(np.max(r2_array_dynam[tw,:]))\n",
    "    time_max_array_dynam[tw] = t_max\n",
    "    _, _,_, vel_df, _ = fit_and_predict(dataset, trial_mask, 'move_onset_time',tw_range, t_max, x_field, y_field, cond_dict = cond_dict)\n",
    "    vel_df_array_dynam.append(np.array(vel_df['pred_vel']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(range(len(pred_range_arr)), r2_array_tw,'k',label='static')\n",
    "plt.plot(range(len(pred_range_arr)), r2_feature_array_tw[:,0],'blue')\n",
    "plt.plot(range(len(pred_range_arr)), r2_feature_array_tw[:,1],'orange')\n",
    "plt.plot(range(len(pred_range_arr)), np.max(r2_array_dynam,axis=1),'k--',label='dynamic')\n",
    "plt.plot(range(len(pred_range_arr)), np.max(r2_feature_array_dynam[:,:,0],axis=1),'blue',ls='--')\n",
    "plt.plot(range(len(pred_range_arr)), np.max(r2_feature_array_dynam[:,:,1],axis=1),'orange',ls='--')\n",
    "plt.xticks(range(len(pred_range_arr)),['-100','0','200','400','600','800'])\n",
    "plt.ylim([-0.1,1])\n",
    "plt.xlabel('Time window start (ms)')\n",
    "plt.ylabel('R2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(time_max_array_dynam,range(len(pred_range_arr)),'k--')\n",
    "plt.axvline(x=time_max,color='k')\n",
    "plt.yticks(range(len(pred_range_arr)),['-100','0','200','400','600','800'])\n",
    "plt.xlabel('Best time lag (ms)')\n",
    "plt.ylabel('Time window start (ms)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_array = []\n",
    "for i in range(len(vel_df_array_dynam)):\n",
    "    stack_array.append(vel_df_array_dynam[i].reshape(active_n_trials,-1,2))\n",
    "concat_pred = np.hstack(stack_array)\n",
    "print(concat_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dir = np.array([0.0, 45.0,90.0, 135.0, 180.0, 225.0, 270.0, 315.0])\n",
    "plot_dir = [0.0, 90.0, 180.0, 270.0] # limit plot directions to reduce cluttering\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "for i in range(len(plot_dir)):\n",
    "    idx = np.argwhere(all_dir==plot_dir[i])[0,0]\n",
    "    plt.plot(np.arange(-100, 1000, 10),concat_pred[active_cond_dict==idx,:,0].T,color=colors[i],alpha=0.5,linewidth=1)\n",
    "plt.xlabel('Time after move onset (ms)')\n",
    "plt.ylabel('Hand velocity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dir = np.array([0.0, 45.0,90.0, 135.0, 180.0, 225.0, 270.0, 315.0])\n",
    "plot_dir = [0.0, 90.0, 180.0, 270.0] # limit plot directions to reduce cluttering\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "for i in range(len(plot_dir)):\n",
    "    idx = np.argwhere(all_dir==plot_dir[i])[0,0]\n",
    "    plt.plot(np.arange(-100, 1000, 10),concat_pred[active_cond_dict==idx,:,1].T,color=colors[i],alpha=0.5,linewidth=1)\n",
    "plt.xlabel('Time after move onset (ms)')\n",
    "plt.ylabel('Hand velocity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for i in range(len(pred_range_arr)):\n",
    "    plt.plot(lag_axis, r2_array_dynam[i,:].T,label=pred_range_arr[i])\n",
    "plt.legend(fontsize=8)\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for i in range(len(pred_range_arr)):\n",
    "    plt.plot(lag_axis, r2_feature_array_dynam[i,:,0].T,label=pred_range_arr[i])\n",
    "plt.legend(fontsize=8)\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R2')\n",
    "plt.title('x-dir')\n",
    "plt.show()\n",
    "\n",
    "x_time_max_array = lag_axis[np.argmax(r2_feature_array_dynam[:,:,0],axis=1)]\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(x_time_max_array,range(len(pred_range_arr)),'k--')\n",
    "plt.axvline(x=x_time_max,color='k')\n",
    "plt.yticks(range(len(pred_range_arr)),['-100','0','200','400','600','800'])\n",
    "plt.xlabel('Best time lag (ms)')\n",
    "plt.ylabel('Time window start (ms)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for i in range(len(pred_range_arr)):\n",
    "    plt.plot(lag_axis, r2_feature_array_dynam[i,:,1].T,label=pred_range_arr[i])\n",
    "plt.legend(fontsize=8)\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R2')\n",
    "plt.title('y-dir')\n",
    "\n",
    "y_time_max_array = lag_axis[np.argmax(r2_feature_array_dynam[:,:,1],axis=1)]\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(y_time_max_array,range(len(pred_range_arr)),'k--')\n",
    "plt.axvline(x=y_time_max,color='k')\n",
    "plt.yticks(range(len(pred_range_arr)),['-100','0','200','400','600','800'])\n",
    "plt.xlabel('Best time lag (ms)')\n",
    "plt.ylabel('Time window start (ms)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cd_dims = 4\n",
    "x_max_idx_arr = np.argmax(r2_feature_array_dynam[:,:,0],axis=1)\n",
    "x_cd_dynam_weight=[]\n",
    "x_fb_dynam_weight = []\n",
    "for i in range(len(pred_range_arr)):\n",
    "    x_cd_dynam_weight.append(coef_array_dynam[i,x_max_idx_arr[i],0,:n_cd_dims])\n",
    "    x_fb_dynam_weight.append(coef_array_dynam[i,x_max_idx_arr[i],0,n_cd_dims:])\n",
    "x_cd_dynam_weight = np.sum(abs(np.array((x_cd_dynam_weight))),axis=1)\n",
    "x_fb_dynam_weight = np.sum(abs(np.array((x_fb_dynam_weight))),axis=1)\n",
    "\n",
    "y_max_idx_arr = np.argmax(r2_feature_array_dynam[:,:,1],axis=1)\n",
    "y_cd_dynam_weight=[]\n",
    "y_fb_dynam_weight = []\n",
    "for i in range(len(pred_range_arr)):\n",
    "    y_cd_dynam_weight.append(coef_array_dynam[i,y_max_idx_arr[i],1,:n_cd_dims])\n",
    "    y_fb_dynam_weight.append(coef_array_dynam[i,y_max_idx_arr[i],1,n_cd_dims:])\n",
    "y_cd_dynam_weight = np.sum(abs(np.array((y_cd_dynam_weight))),axis=1)\n",
    "y_fb_dynam_weight = np.sum(abs(np.array((y_fb_dynam_weight))),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if x_time_max == y_time_max:\n",
    "    x_cd_weight = np.sum(abs(best_coef[0,:n_cd_dims]))\n",
    "    x_fb_weight = np.sum(abs(best_coef[0,n_cd_dims:]))\n",
    "    y_cd_weight = np.sum(abs(best_coef[1,:n_cd_dims]))\n",
    "    y_fb_weight = np.sum(abs(best_coef[1,n_cd_dims:]))\n",
    "else:\n",
    "    x_cd_weight = np.sum(abs(x_best_coef[0,:n_cd_dims]))\n",
    "    x_fb_weight = np.sum(abs(x_best_coef[0,n_cd_dims:]))\n",
    "    y_cd_weight = np.sum(abs(y_best_coef[1,:n_cd_dims]))\n",
    "    y_fb_weight = np.sum(abs(y_best_coef[1,n_cd_dims:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.array(range(len(pred_range_arr)))\n",
    "plt.bar(x_axis,x_cd_dynam_weight,width=0.2,color='green',label='CD')\n",
    "plt.bar(x_axis+0.2,x_fb_dynam_weight,width=0.2,color='magenta',label='FB')\n",
    "plt.axhline(x_cd_weight,color='green')\n",
    "plt.axhline(x_fb_weight,color='magenta')\n",
    "\n",
    "plt.xticks(range(len(pred_range_arr)),['-100','0','200','400','600','800'])\n",
    "plt.ylabel('Sum signal dim weight in decoder')\n",
    "plt.xlabel('Time window start (ms)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.array(range(len(pred_range_arr)))\n",
    "plt.bar(x_axis+0,y_cd_dynam_weight,width=0.2,color='green',label='CD')\n",
    "plt.bar(x_axis+0.2,y_fb_dynam_weight,width=0.2,color='magenta',label='FB')\n",
    "plt.axhline(y_cd_weight,color='green')\n",
    "plt.axhline(y_fb_weight,color='magenta')\n",
    "plt.xticks(range(len(pred_range_arr)),['-100','0','200','400','600','800'])\n",
    "plt.ylabel('Sum signal weight in decoder')\n",
    "plt.xlabel('Time window start (ms)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey = \"Han_20171207\"\n",
    "# monkey = 'Duncan_20190710'\n",
    "data = np.load(monkey+'_smooth40_spikes_hand_vel_act_r2s.npz')\n",
    "print(data.files)\n",
    "data_all_nrn = np.load(monkey+'_all_nrn_hand_vel_act_r2s.npz')\n",
    "data_all_nrn.files\n",
    "\n",
    "# data = np.load(monkey+'_MC_smooth40_spikes_hand_vel_act_r2s.npz')\n",
    "# data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_axis = np.arange(-300,320,20)\n",
    "print(lag_axis[np.argmax(data['x_r2_cd_only'])])\n",
    "print(lag_axis[np.argmax(data['x_r2_fb_only'])])\n",
    "print(lag_axis[np.argmax(data['x_r2_cd_fb'])])\n",
    "print(lag_axis[np.argmax(data_all_nrn['r2_feature_all_nrn_array'][:,0])])\n",
    "print()\n",
    "print(lag_axis[np.argmax(data['y_r2_cd_only'])])\n",
    "print(lag_axis[np.argmax(data['y_r2_fb_only'])])\n",
    "print(lag_axis[np.argmax(data['y_r2_cd_fb'])])\n",
    "print(lag_axis[np.argmax(data_all_nrn['r2_feature_all_nrn_array'][:,1])])\n",
    "print()\n",
    "print(lag_axis[np.argmax(data['r2_cd_only'])])\n",
    "print(lag_axis[np.argmax(data['r2_fb_only'])])\n",
    "print(lag_axis[np.argmax(data['r2_cd_fb'])])\n",
    "print(lag_axis[np.argmax(data_all_nrn['r2_all_nrn_array'])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lag_axis = np.arange(-300,320,20)\n",
    "# print(lag_axis[np.argmax(np.mean(data['x_r2_cd_only'],axis=1))])\n",
    "# print(lag_axis[np.argmax(np.mean(data['x_r2_fb_only'],axis=1))])\n",
    "# print(lag_axis[np.argmax(np.mean(data['x_r2_cd_fb'],axis=1))])\n",
    "# print(lag_axis[np.argmax(np.mean(data['x_r2_nrn'],axis=1))])\n",
    "# print()\n",
    "# print(lag_axis[np.argmax(np.mean(data['y_r2_cd_only'],axis=1))])\n",
    "# print(lag_axis[np.argmax(np.mean(data['y_r2_fb_only'],axis=1))])\n",
    "# print(lag_axis[np.argmax(np.mean(data['y_r2_cd_fb'],axis=1))])\n",
    "# print(lag_axis[np.argmax(np.mean(data['y_r2_nrn'],axis=1))])\n",
    "# print()\n",
    "# print(lag_axis[np.argmax(np.mean(data['r2_cd_only'],axis=1))])\n",
    "# print(lag_axis[np.argmax(np.mean(data['r2_fb_only'],axis=1))])\n",
    "# print(lag_axis[np.argmax(np.mean(data['r2_cd_fb'],axis=1))])\n",
    "# print(lag_axis[np.argmax(np.mean(data['r2_nrn'],axis=1))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['r2_cd_only'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'CD_proj'\n",
    "y_field = 'hand_vel'\n",
    "train_mask = active_mask\n",
    "cond_dict = active_cond_dict\n",
    "train_range = (-100, 1000)\n",
    "x_lag = lag_axis[np.argmax(data['x_r2_cd_only'])]\n",
    "y_lag = lag_axis[np.argmax(data['y_r2_cd_only'])]\n",
    "\n",
    "_, x_coef, x_intercept, _,_ = fit_and_predict(dataset, train_mask, 'move_onset_time',train_range, x_lag, x_field, y_field, cond_dict = cond_dict)\n",
    "_, y_coef, y_intercept, _,_ = fit_and_predict(dataset, train_mask, 'move_onset_time',train_range, y_lag, x_field, y_field, cond_dict = cond_dict)\n",
    "pas_r2_array = nans([len(lag_axis)]); pas_x_r2_array = nans([len(lag_axis)]); pas_y_r2_array = nans([len(lag_axis)])\n",
    "for i in range(len(lag_axis)):\n",
    "    lag = lag_axis[i]\n",
    "    _, _, x_r2,_,x_vel_df = pred_with_new_weights(dataset, passive_mask, 'move_onset_time',(-100,120), lag, x_field,\n",
    "                                                    y_field, x_coef, x_intercept, 'move_onset_time',train_range, (train_range[0]+x_lag, train_range[1]+x_lag), active_mask)\n",
    "    \n",
    "    _, _, _,y_r2,y_vel_df = pred_with_new_weights(dataset, passive_mask, 'move_onset_time',(-100,120), lag, x_field,\n",
    "                                                    y_field, y_coef, y_intercept, 'move_onset_time',train_range, (train_range[0]+y_lag, train_range[1]+y_lag), active_mask)\n",
    "    pas_x_r2_array[i] = x_r2; pas_y_r2_array[i] = y_r2\n",
    "    true_vel = np.array(x_vel_df['hand_vel'])\n",
    "    pred_vel = np.vstack([np.array(x_vel_df['pred_vel']['x']),np.array(y_vel_df['pred_vel']['y'])]).T\n",
    "    sses =get_sses_pred(true_vel,pred_vel)\n",
    "    sses_mean=get_sses_mean(true_vel)\n",
    "    r2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "    pas_r2_array[i] = r2\n",
    "\n",
    "pas_cd_r2_array = pas_r2_array\n",
    "pas_cd_x_r2_array = pas_x_r2_array\n",
    "pas_cd_y_r2_array = pas_y_r2_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'FB_proj'\n",
    "y_field = 'hand_vel'\n",
    "train_mask = active_mask\n",
    "cond_dict = active_cond_dict\n",
    "train_range = (-100, 1000)\n",
    "x_lag = lag_axis[np.argmax(data['x_r2_fb_only'])]\n",
    "y_lag = lag_axis[np.argmax(data['y_r2_fb_only'])]\n",
    "\n",
    "_, x_coef, x_intercept, _,_ = fit_and_predict(dataset, train_mask, 'move_onset_time',train_range, x_lag, x_field, y_field, cond_dict = cond_dict)\n",
    "_, y_coef, y_intercept, _,_ = fit_and_predict(dataset, train_mask, 'move_onset_time',train_range, y_lag, x_field, y_field, cond_dict = cond_dict)\n",
    "pas_r2_array = nans([len(lag_axis)]); pas_x_r2_array = nans([len(lag_axis)]); pas_y_r2_array = nans([len(lag_axis)])\n",
    "for i in range(len(lag_axis)):\n",
    "    lag = lag_axis[i]\n",
    "    _, _, x_r2,_,x_vel_df = pred_with_new_weights(dataset, passive_mask, 'move_onset_time',(-100,120), lag, x_field,\n",
    "                                                    y_field, x_coef, x_intercept, 'move_onset_time',train_range, (train_range[0]+x_lag, train_range[1]+x_lag), active_mask)\n",
    "    \n",
    "    _, _, _,y_r2,y_vel_df = pred_with_new_weights(dataset, passive_mask, 'move_onset_time',(-100,120), lag, x_field,\n",
    "                                                    y_field, y_coef, y_intercept, 'move_onset_time',train_range, (train_range[0]+y_lag, train_range[1]+y_lag), active_mask)\n",
    "    pas_x_r2_array[i] = x_r2; pas_y_r2_array[i] = y_r2\n",
    "    true_vel = np.array(x_vel_df['hand_vel'])\n",
    "    pred_vel = np.vstack([np.array(x_vel_df['pred_vel']['x']),np.array(y_vel_df['pred_vel']['y'])]).T\n",
    "    sses =get_sses_pred(true_vel,pred_vel)\n",
    "    sses_mean=get_sses_mean(true_vel)\n",
    "    r2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "    pas_r2_array[i] = r2\n",
    "\n",
    "pas_fb_r2_array = pas_r2_array\n",
    "pas_fb_x_r2_array = pas_x_r2_array\n",
    "pas_fb_y_r2_array = pas_y_r2_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'CD_FB_proj'\n",
    "y_field = 'hand_vel'\n",
    "train_mask = active_mask\n",
    "cond_dict = active_cond_dict\n",
    "train_range = (-100, 1000)\n",
    "x_lag = lag_axis[np.argmax(data_all_nrn['r2_feature_all_nrn_array'][:,0])]\n",
    "y_lag = lag_axis[np.argmax(data_all_nrn['r2_feature_all_nrn_array'][:,1])]\n",
    "\n",
    "_, x_coef, x_intercept, _,_ = fit_and_predict(dataset, train_mask, 'move_onset_time',train_range, x_lag, x_field, y_field, cond_dict = cond_dict)\n",
    "_, y_coef, y_intercept, _,_ = fit_and_predict(dataset, train_mask, 'move_onset_time',train_range, y_lag, x_field, y_field, cond_dict = cond_dict)\n",
    "pas_r2_array = nans([len(lag_axis)]); pas_x_r2_array = nans([len(lag_axis)]); pas_y_r2_array = nans([len(lag_axis)])\n",
    "for i in range(len(lag_axis)):\n",
    "    lag = lag_axis[i]\n",
    "    _, _, x_r2,_,x_vel_df = pred_with_new_weights(dataset, passive_mask, 'move_onset_time',(-100,120), lag, x_field,\n",
    "                                                    y_field, x_coef, x_intercept, 'move_onset_time',train_range, (train_range[0]+x_lag, train_range[1]+x_lag), active_mask)\n",
    "    \n",
    "    _, _, _,y_r2,y_vel_df = pred_with_new_weights(dataset, passive_mask, 'move_onset_time',(-100,120), lag, x_field,\n",
    "                                                    y_field, y_coef, y_intercept, 'move_onset_time',train_range, (train_range[0]+y_lag, train_range[1]+y_lag), active_mask)\n",
    "    pas_x_r2_array[i] = x_r2; pas_y_r2_array[i] = y_r2\n",
    "    true_vel = np.array(x_vel_df['hand_vel'])\n",
    "    pred_vel = np.vstack([np.array(x_vel_df['pred_vel']['x']),np.array(y_vel_df['pred_vel']['y'])]).T\n",
    "    sses =get_sses_pred(true_vel,pred_vel)\n",
    "    sses_mean=get_sses_mean(true_vel)\n",
    "    r2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "    pas_r2_array[i] = r2\n",
    "\n",
    "pas_cdfb_r2_array = pas_r2_array\n",
    "pas_cdfb_x_r2_array = pas_x_r2_array\n",
    "pas_cdfb_y_r2_array = pas_y_r2_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez(monkey+'_hand_vel_pas120_r2s_cross', \\\n",
    "#          pas_cd_r2_array = pas_cd_r2_array, pas_cd_x_r2_array = pas_cd_x_r2_array, pas_cd_y_r2_array = pas_cd_y_r2_array, \\\n",
    "#          pas_fb_r2_array = pas_fb_r2_array, pas_fb_x_r2_array = pas_fb_x_r2_array, pas_fb_y_r2_array = pas_fb_y_r2_array, \\\n",
    "#          pas_cdfb_r2_array = pas_cdfb_r2_array, pas_cdfb_x_r2_array = pas_cdfb_x_r2_array, pas_cdfb_y_r2_array = pas_cdfb_y_r2_array)\n",
    "\n",
    "monkey = \"Han_20171207\"\n",
    "# monkey = 'Duncan_20190710'\n",
    "data = np.load(monkey+'_hand_vel_pas120_r2s_cross.npz')\n",
    "data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw = 3\n",
    "lag_axis = np.arange(-300,320,20)\n",
    "ylim = [-0.7, 0.7]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5.5,4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(lag_axis, data['pas_cd_r2_array'],linewidth=lw,color = 'green',label='cd')\n",
    "plt.plot(lag_axis, data['pas_fb_r2_array'],linewidth=lw,color = 'magenta',label='fb')\n",
    "plt.plot(lag_axis, data['pas_cdfb_r2_array'],linewidth=lw,color = 'brown',label='cd+fb')\n",
    "plt.axvline(0, color = 'k', linestyle='--')\n",
    "# print(time_max)\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R²')\n",
    "# plt.title(x_field)\n",
    "plt.ylim(ylim)\n",
    "# plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_passive_r2_dot.pdf',dpi = 'figure')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5.5,4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(lag_axis, data['pas_cd_x_r2_array'],linewidth=lw,color = 'green',label='cd')\n",
    "plt.plot(lag_axis, data['pas_fb_x_r2_array'],linewidth=lw,color = 'magenta',label='fb')\n",
    "plt.plot(lag_axis, data['pas_cdfb_x_r2_array'],linewidth=lw,color = 'brown',label='cd+fb')\n",
    "plt.axvline(0, color = 'k', linestyle='--')\n",
    "# print(time_max)\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('X R²')\n",
    "# plt.title(x_field)\n",
    "plt.ylim(ylim)\n",
    "# plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_passive_x_r2_dot.pdf',dpi = 'figure')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5.5,4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(lag_axis, data['pas_cd_y_r2_array'],linewidth=lw,color = 'green',label='cd')\n",
    "plt.plot(lag_axis, data['pas_fb_y_r2_array'],linewidth=lw,color = 'magenta',label='fb')\n",
    "plt.plot(lag_axis, data['pas_cdfb_y_r2_array'],linewidth=lw,color = 'brown',label='cd+fb')\n",
    "plt.axvline(0, color = 'k', linestyle='--')\n",
    "# print(time_max)\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('Y R²')\n",
    "# plt.title(x_field)\n",
    "plt.ylim(ylim)\n",
    "# plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_passive_y_r2_dot.pdf',dpi = 'figure')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data.keys().unique(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'spikes_smth_40'\n",
    "data = np.load(monkey+'_X_cdfb_data_proj_out.npz')\n",
    "data.files\n",
    "dataset.add_continuous_data(data['CD_FB_proj'],'CD_FB_proj')\n",
    "dataset.add_continuous_data(data['CD_proj'],'CD_proj')\n",
    "dataset.add_continuous_data(data['FB_proj'],'FB_proj')\n",
    "dataset.data.keys().unique(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "negative_lag = -180\n",
    "positive_lag = 60\n",
    "both_lag = 0\n",
    "cond_dict = active_cond_dict\n",
    "n_trials = active_n_trials\n",
    "\n",
    "y_field ='hand_vel'\n",
    "train_range = (-100,1000)\n",
    "train_pos_lag_range = (train_range[0]+positive_lag, train_range[1]+positive_lag)\n",
    "train_neg_lag_range = (train_range[0]+negative_lag, train_range[1]+negative_lag)\n",
    "train_both_lag_range = (train_range[0]+both_lag, train_range[1]+both_lag)\n",
    "n_timepoints = int((train_range[1] - train_range[0])/dataset.bin_width)\n",
    "\n",
    "train_mask = active_mask\n",
    "# _, eff_weights, eff_offset, _,_ = fit_and_predict(dataset, train_mask, 'move_onset_time',train_range, negative_lag, 'CD_proj', y_field, cond_dict = cond_dict)\n",
    "# _, _, _,_,act_eff_vel_df = pred_with_new_weights(dataset, train_mask, 'move_onset_time',train_range, 0,'CD_proj',\n",
    "#                                                  y_field, eff_weights, eff_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "\n",
    "# _, aff_weights, aff_offset, _,_ = fit_and_predict(dataset, train_mask, 'move_onset_time',train_range, positive_lag, 'FB_proj', y_field, cond_dict = cond_dict)\n",
    "# _, _, _,_,act_aff_vel_df = pred_with_new_weights(dataset, train_mask, 'move_onset_time',train_range, 0,'FB_proj',\n",
    "#                                                  y_field, aff_weights, aff_offset, 'move_onset_time',train_range, train_pos_lag_range, train_mask)\n",
    "\n",
    "_, all_weights, all_offset, _,_ = fit_and_predict(dataset, train_mask, 'move_onset_time',train_range, both_lag, 'spikes_smth_40', y_field, cond_dict = cond_dict)\n",
    "_, _, _,_,act_all_vel_df = pred_with_new_weights(dataset, train_mask, 'move_onset_time',train_range, 0,'spikes_smth_40',\n",
    "                                                 y_field, all_weights, all_offset, 'move_onset_time',train_range, train_both_lag_range, train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=train_range, ignored_trials=~active_mask, allow_overlap=True)\n",
    "latents = np.array(df.CD_FB_proj).reshape(n_trials, n_timepoints, -1)\n",
    "latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(act_all_vel_df.pred_vel).reshape(active_n_trials, n_timepoints, -1)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Neural_Decoding.decoders import DenseNNDecoder\n",
    "Y = predictions\n",
    "data_field = 'CD_FB_proj'\n",
    "lag_axis = np.arange(-100,200,50)\n",
    "regressed_r2_array = nans([len(lag_axis)])\n",
    "for i in range(len(lag_axis)):\n",
    "    lag = lag_axis[i]\n",
    "    print(lag)\n",
    "    lag_train_range = (train_range[0]+lag, train_range[1]+lag)\n",
    "    df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=lag_train_range, ignored_trials=~active_mask, allow_overlap=True)\n",
    "    latents = np.array(df[data_field]).reshape(n_trials, n_timepoints, -1)\n",
    "    X = X = latents \n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True,random_state = 42)   \n",
    "    true_concat = nans([n_trials*n_timepoints,predictions.shape[-1]])\n",
    "    pred_concat = nans([n_trials*n_timepoints,predictions.shape[-1]])\n",
    "    trial_save_idx = 0\n",
    "    for training_set, test_set in skf.split(range(0,n_trials),cond_dict):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test, y_train, y_test = process_train_test(X,Y,training_set,test_set)\n",
    "        # lr = LinearRegression().fit(X_train, y_train)\n",
    "        # y_test_predicted = lr.predict(X_test)\n",
    "        dnn = DenseNNDecoder(units=400,dropout=0.25,num_epochs=10)\n",
    "        dnn.fit(X_train, y_train)\n",
    "        y_test_predicted = dnn.predict(X_test)\n",
    "\n",
    "        n = y_test_predicted.shape[0]\n",
    "        true_concat[trial_save_idx:trial_save_idx+n,:] = y_test\n",
    "        pred_concat[trial_save_idx:trial_save_idx+n,:] = y_test_predicted\n",
    "        trial_save_idx += n\n",
    "        \n",
    "    sses =get_sses_pred(true_concat,pred_concat)\n",
    "    sses_mean=get_sses_mean(true_concat)\n",
    "    R2 =1-np.sum(sses)/np.sum(sses_mean)  \n",
    "    regressed_r2_array[i] = R2\n",
    "    print(R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lag_axis,regressed_r2_array)\n",
    "plt.ylabel('R2')\n",
    "plt.xlabel('Latent lag relative to prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "best_lag = lag_axis[np.argmax(regressed_r2_array)]\n",
    "lag_train_range = (train_range[0]+best_lag, train_range[1]+best_lag)\n",
    "df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=lag_train_range, ignored_trials=~active_mask, allow_overlap=True)\n",
    "X = np.array(df[data_field])\n",
    "Y = np.array(act_all_vel_df.pred_vel)\n",
    "lr = LinearRegression().fit(X, Y)\n",
    "pred = lr.predict(X)\n",
    "act_all_vel_df = pd.concat([act_all_vel_df, pd.DataFrame(pred, columns=dataset._make_midx('regr_vel_dnn_CDFB', ['x', 'y'], 2))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = act_all_vel_df\n",
    "# Prepare for plotting\n",
    "plot_dir = [0.0, 90.0, 180.0, 270.0] # limit plot directions to reduce cluttering\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "plot_dim = 'x' # plot x velocity \n",
    "\n",
    "x_axis = np.arange(-100,1000,dataset.bin_width)\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir %360 == trial_dir].trial_id\n",
    "    for _, trial in df[np.isin(df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        plt.plot(x_axis, trial[y_field][plot_dim], color=color, linewidth=0.5)\n",
    "        # plt.plot(x_axis, trial[y_field].to_numpy()[:,0], color=color, linewidth=0.5)\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "# plt.ylabel('Hand velocity (cm/s)')\n",
    "\n",
    "# plt.xlim([-100,500])\n",
    "# plt.ylim([-0.65,0.65])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + label + 'true.pdf', dpi = 'figure')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir %360 == trial_dir].trial_id\n",
    "    for _, trial in df[np.isin(df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "         plt.plot(x_axis, trial['pred_vel'][plot_dim], color=color, linewidth=0.5)\n",
    "        # plt.plot(x_axis, trial.pred_vel.to_numpy()[:,0], color=color, linewidth=0.5)\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "# plt.ylabel('Hand velocity (cm/s)')\n",
    "# plt.xlim([-100,500])\n",
    "# plt.ylim([-0.65,0.65])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + label + str(0) +'_pred.pdf', dpi = 'figure')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir %360 == trial_dir].trial_id\n",
    "    for _, trial in df[np.isin(df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "         plt.plot(x_axis, trial['regr_vel_dnn_CDFB'][plot_dim], color=color, linewidth=0.5)\n",
    "        # plt.plot(x_axis, trial.pred_vel.to_numpy()[:,0], color=color, linewidth=0.5)\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "# plt.ylabel('Hand velocity (cm/s)')\n",
    "# plt.xlim([-100,500])\n",
    "# plt.ylim([-0.65,0.65])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + label + str(0) +'_pred.pdf', dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_field ='hand_vel'\n",
    "lag_axis = np.arange(-300,300,20)\n",
    "pred_range = (0, 120)\n",
    "trial_mask = active_mask\n",
    "cond_dict = active_cond_dict\n",
    "\n",
    "# Note it differs for x- and y-dir\n",
    "cd_lag = -60\n",
    "fb_lag = 40\n",
    "cdfb_lag = 60\n",
    "\n",
    "_, active_cd_coef,active_cd_intercept, cd_vel_df, _ = fit_and_predict(dataset, trial_mask, 'move_onset_time',pred_range, cd_lag, 'CD_proj', y_field, cond_dict = cond_dict)\n",
    "_, active_fb_coef,active_fb_intercept, fb_vel_df, _ = fit_and_predict(dataset, trial_mask, 'move_onset_time',pred_range, fb_lag, 'FB_proj', y_field, cond_dict = cond_dict)\n",
    "_, active_cdfb_coef,active_cdfb_intercept, cdfb_vel_df, _ = fit_and_predict(dataset, trial_mask, 'move_onset_time',pred_range, cdfb_lag, 'CD_FB_proj', y_field, cond_dict = cond_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-decoding\n",
    "\n",
    "dataset = dataset_10ms\n",
    "# all_mask = dataset.trial_info.split != 'none'\n",
    "\n",
    "\n",
    "negative_lag = -100\n",
    "positive_lag = 60\n",
    "both_lag = 0\n",
    "\n",
    "y_field ='hand_vel'\n",
    "train_range = (-100,1000)\n",
    "train_pos_lag_range = (train_range[0]+positive_lag, train_range[1]+positive_lag)\n",
    "train_neg_lag_range = (train_range[0]+negative_lag, train_range[1]+negative_lag)\n",
    "train_mask = active_mask\n",
    "# train_mask = passive_mask\n",
    "_, aff_weights, aff_offset, act_aff_vel_df,_ = fit_and_predict(dataset, train_mask, 'move_onset_time', train_range, positive_lag, 'ac150_FB_proj_spikes_smth_150_oneside', y_field)\n",
    "_, eff_weights, eff_offset, act_eff_vel_df,_ = fit_and_predict(dataset, train_mask, 'move_onset_time', train_range, negative_lag,'ac150_CD_proj_spikes_smth_150_oneside',y_field)\n",
    "_, both_weights, both_offset, act_both_vel_df,_ = fit_and_predict(dataset, train_mask, 'move_onset_time', train_range, both_lag,'ac150_CD_FB_proj_spikes_smth_150_oneside',y_field)\n",
    "\n",
    "#pred active\n",
    "pred_range = (-100,1000)\n",
    "active_x_axis = np.arange(pred_range[0], pred_range[1], dataset.bin_width)\n",
    "_, _, _,_,act_aff_vel_df = pred_with_new_weights(dataset, active_mask, 'move_onset_time',pred_range, positive_lag,'ac150_FB_proj_spikes_smth_150_oneside',\n",
    "                                                 y_field, aff_weights, aff_offset, 'move_onset_time',train_range, train_pos_lag_range, train_mask)\n",
    "_, _, _,_,act_eff_vel_df = pred_with_new_weights(dataset, active_mask, 'move_onset_time',pred_range, negative_lag,'ac150_CD_proj_spikes_smth_150_oneside',\n",
    "                                                 y_field, eff_weights, eff_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "_, _, _,_,act_both_vel_df = pred_with_new_weights(dataset, active_mask, 'move_onset_time',pred_range, both_lag,'ac150_CD_FB_proj_spikes_smth_150_oneside',\n",
    "                                                 y_field, both_weights, both_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "#pred passive\n",
    "pred_range = (-100, 500)\n",
    "passive_x_axis = np.arange(pred_range[0], pred_range[1], dataset.bin_width)\n",
    "_, _, _,_,pas_aff_vel_df = pred_with_new_weights(dataset, passive_mask, 'move_onset_time',pred_range, positive_lag,'ac150_FB_proj_spikes_smth_150_oneside',\n",
    "                                                 y_field, aff_weights, aff_offset, 'move_onset_time',train_range, train_pos_lag_range, train_mask)\n",
    "_, _, _,_,pas_eff_vel_df = pred_with_new_weights(dataset, passive_mask, 'move_onset_time',pred_range, negative_lag,'ac150_CD_proj_spikes_smth_150_oneside',\n",
    "                                                 y_field, eff_weights, eff_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "_, _, _,_,pas_both_vel_df = pred_with_new_weights(dataset, passive_mask, 'move_onset_time',pred_range, both_lag,'ac150_CD_FB_proj_spikes_smth_150_oneside',\n",
    "                                                 y_field, both_weights, both_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "\n",
    "# #pred nan\n",
    "# pred_range = (-100, 1000)\n",
    "# passive_x_axis = np.arange(pred_range[0], pred_range[1], dataset.bin_width)\n",
    "# _, _, _,_,pas_aff_vel_df = pred_with_new_weights(dataset, nan_mask, 'move_onset_time',pred_range, positive_lag,'FB_proj',\n",
    "#                                                  y_field, aff_weights, aff_offset, 'move_onset_time',train_range, train_pos_lag_range, train_mask)\n",
    "# _, _, _,_,pas_eff_vel_df = pred_with_new_weights(dataset, nan_mask, 'move_onset_time',pred_range, negative_lag,'CD_proj',\n",
    "#                                                  y_field, eff_weights, eff_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "# _, _, _,_,pas_both_vel_df = pred_with_new_weights(dataset, nan_mask, 'move_onset_time',pred_range, both_lag,'CD_FB_proj',\n",
    "#                                                  y_field, both_weights, both_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "\n",
    "plot_dir = [0.0, 180.0] # limit plot directions to reduce cluttering\n",
    "colors = ['gray','gray']\n",
    "plot_dim = 'x'\n",
    "fig, axs = plt.subplots(6, 4, sharex=False, sharey=True, figsize=(18, 18))\n",
    "# plt.ylim(-50,50)\n",
    "i = 0\n",
    "alpha = 0.5\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir%360 == trial_dir].trial_id\n",
    "    for _, trial in act_eff_vel_df[np.isin(act_eff_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        axs[0][i].plot(active_x_axis, trial[y_field][plot_dim], color=color, alpha = alpha, linewidth=0.5)\n",
    "        axs[0][i].plot(active_x_axis, trial.pred_vel[plot_dim], color='green', alpha = alpha, linewidth=0.5)\n",
    "        axs[0][i].axvline(x=300, ls='--')\n",
    "    for _, trial in act_aff_vel_df[np.isin(act_aff_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        axs[1][i].plot(active_x_axis, trial.pred_vel[plot_dim], color='magenta', alpha = alpha, linewidth=0.5)\n",
    "        axs[1][i].plot(active_x_axis, trial[y_field][plot_dim], color=color, alpha = alpha, linewidth=0.5)\n",
    "        axs[1][i].axvline(x=300, ls='--')\n",
    "    for _, trial in act_both_vel_df[np.isin(act_both_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        axs[2][i].plot(active_x_axis, trial[y_field][plot_dim], color=color, alpha = alpha, linewidth=0.5)\n",
    "        axs[2][i].plot(active_x_axis, trial.pred_vel[plot_dim], color='brown', alpha = alpha, linewidth=0.5)    \n",
    "        axs[2][i].spines[['right', 'top']].set_visible(False) \n",
    "        axs[2][i].axvline(x=300, ls='--')\n",
    "    # cond_ids = dataset.trial_info[dataset.trial_info.bump_dir%360 == trial_dir].trial_id\n",
    "    for _, trial in pas_eff_vel_df[np.isin(pas_eff_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        axs[3][i].plot(passive_x_axis, trial.pred_vel[plot_dim], color='green', alpha = alpha, linewidth=0.5)\n",
    "        axs[3][i].plot(passive_x_axis, trial[y_field][plot_dim], color=color, alpha = alpha, linewidth=0.5)\n",
    "        axs[3][i].axvline(x=120, ls='--')\n",
    "    for _, trial in pas_aff_vel_df[np.isin(pas_aff_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        axs[4][i].plot(passive_x_axis, trial.pred_vel[plot_dim], color='magenta', alpha = alpha, linewidth=0.5)\n",
    "        axs[4][i].plot(passive_x_axis, trial[y_field][plot_dim], color=color, alpha = alpha, linewidth=0.5)\n",
    "        axs[4][i].axvline(x=120, ls='--')\n",
    "    for _, trial in pas_both_vel_df[np.isin(pas_both_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        axs[5][i].plot(passive_x_axis, trial.pred_vel[plot_dim], color='brown', alpha = alpha, linewidth=0.5)\n",
    "        axs[5][i].plot(passive_x_axis, trial[y_field][plot_dim], color=color, alpha = alpha, linewidth=0.5)\n",
    "        axs[5][i].spines[['right', 'top']].set_visible(False)\n",
    "        axs[5][i].axvline(x=120, ls='--')\n",
    "    i+=2\n",
    "\n",
    "\n",
    "negative_lag = -100\n",
    "positive_lag = 120\n",
    "both_lag = 0\n",
    "\n",
    "y_field ='hand_vel'\n",
    "train_range = (-100,1000)\n",
    "train_pos_lag_range = (train_range[0]+positive_lag, train_range[1]+positive_lag)\n",
    "train_neg_lag_range = (train_range[0]+negative_lag, train_range[1]+negative_lag)\n",
    "train_mask = active_mask\n",
    "# train_mask = passive_mask\n",
    "\n",
    "_, aff_weights, aff_offset, act_aff_vel_df,_ = fit_and_predict(dataset, train_mask, 'move_onset_time', train_range, positive_lag, 'ac150_FB_proj_spikes_smth_150_oneside', y_field)\n",
    "_, eff_weights, eff_offset, act_eff_vel_df,_ = fit_and_predict(dataset, train_mask, 'move_onset_time', train_range, negative_lag,'ac150_CD_proj_spikes_smth_150_oneside',y_field)\n",
    "_, both_weights, both_offset, act_both_vel_df,_ = fit_and_predict(dataset, train_mask, 'move_onset_time', train_range, both_lag,'ac150_CD_FB_proj_spikes_smth_150_oneside',y_field)\n",
    "\n",
    "#pred active\n",
    "pred_range = (-100, 1000)\n",
    "active_x_axis = np.arange(pred_range[0], pred_range[1], dataset.bin_width)\n",
    "_, _, _,_,act_aff_vel_df = pred_with_new_weights(dataset, active_mask, 'move_onset_time',pred_range, positive_lag,'ac150_FB_proj_spikes_smth_150_oneside',\n",
    "                                                 y_field, aff_weights, aff_offset, 'move_onset_time',train_range, train_pos_lag_range, train_mask)\n",
    "_, _, _,_,act_eff_vel_df = pred_with_new_weights(dataset, active_mask, 'move_onset_time',pred_range, negative_lag,'ac150_CD_proj_spikes_smth_150_oneside',\n",
    "                                                 y_field, eff_weights, eff_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "_, _, _,_,act_both_vel_df = pred_with_new_weights(dataset, active_mask, 'move_onset_time',pred_range, both_lag,'ac150_CD_FB_proj_spikes_smth_150_oneside',\n",
    "                                                 y_field, both_weights, both_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "#pred passive\n",
    "pred_range = (-100, 500)\n",
    "passive_x_axis = np.arange(pred_range[0], pred_range[1], dataset.bin_width)\n",
    "_, _, _,_,pas_aff_vel_df = pred_with_new_weights(dataset, passive_mask, 'move_onset_time',pred_range, positive_lag,'ac150_FB_proj_spikes_smth_150_oneside',\n",
    "                                                 y_field, aff_weights, aff_offset, 'move_onset_time',train_range, train_pos_lag_range, train_mask)\n",
    "_, _, _,_,pas_eff_vel_df = pred_with_new_weights(dataset, passive_mask, 'move_onset_time',pred_range, negative_lag,'ac150_CD_proj_spikes_smth_150_oneside',\n",
    "                                                 y_field, eff_weights, eff_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "_, _, _,_,pas_both_vel_df = pred_with_new_weights(dataset, passive_mask, 'move_onset_time',pred_range, both_lag,'ac150_CD_FB_proj_spikes_smth_150_oneside',\n",
    "                                                 y_field, both_weights, both_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "\n",
    "#pred nan\n",
    "# pred_range = (-100, 1000)\n",
    "# passive_x_axis = np.arange(pred_range[0], pred_range[1], dataset.bin_width)\n",
    "# _, _, _,_,pas_aff_vel_df = pred_with_new_weights(dataset, nan_mask, 'move_onset_time',pred_range, positive_lag,'FB_proj',\n",
    "#                                                  y_field, aff_weights, aff_offset, 'move_onset_time',train_range, train_pos_lag_range, train_mask)\n",
    "# _, _, _,_,pas_eff_vel_df = pred_with_new_weights(dataset, nan_mask, 'move_onset_time',pred_range, negative_lag,'CD_proj',\n",
    "#                                                  y_field, eff_weights, eff_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "# _, _, _,_,pas_both_vel_df = pred_with_new_weights(dataset, nan_mask, 'move_onset_time',pred_range,both_lag,'CD_FB_proj',\n",
    "#                                                  y_field, both_weights, both_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "\n",
    "\n",
    "plot_dir = [90.0, 270.0] # limit plot directions to reduce cluttering\n",
    "colors = ['gray', 'gray']\n",
    "plot_dim = 'y'\n",
    "i = 1\n",
    "alpha = 0.5\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir%360 == trial_dir].trial_id\n",
    "    for _, trial in act_eff_vel_df[np.isin(act_eff_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        axs[0][i].plot(active_x_axis, trial[y_field][plot_dim], color=color, alpha = alpha, linewidth=0.5)\n",
    "        axs[0][i].plot(active_x_axis, trial.pred_vel[plot_dim], color='green', alpha = alpha, linewidth=0.5)\n",
    "        axs[0][i].axvline(x=300, ls='--')\n",
    "    for _, trial in act_aff_vel_df[np.isin(act_aff_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        axs[1][i].plot(active_x_axis, trial.pred_vel[plot_dim], color='magenta', alpha = alpha, linewidth=0.5)\n",
    "        axs[1][i].plot(active_x_axis, trial[y_field][plot_dim], color=color, alpha = alpha, linewidth=0.5)\n",
    "        axs[1][i].axvline(x=300, ls='--')\n",
    "    for _, trial in act_both_vel_df[np.isin(act_both_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        axs[2][i].plot(active_x_axis, trial[y_field][plot_dim], color=color, alpha = alpha, linewidth=0.5)\n",
    "        axs[2][i].plot(active_x_axis, trial.pred_vel[plot_dim], color='brown', alpha = alpha, linewidth=0.5)    \n",
    "        axs[2][i].spines[['right', 'top']].set_visible(False) \n",
    "        axs[2][i].axvline(x=300, ls='--')\n",
    "    # cond_ids = dataset.trial_info[dataset.trial_info.bump_dir%360 == trial_dir].trial_id\n",
    "    for _, trial in pas_eff_vel_df[np.isin(pas_eff_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        axs[3][i].plot(passive_x_axis, trial.pred_vel[plot_dim], color='green', alpha = alpha, linewidth=0.5)\n",
    "        axs[3][i].plot(passive_x_axis, trial[y_field][plot_dim], color=color, alpha = alpha, linewidth=0.5)\n",
    "        axs[3][i].axvline(x=120, ls='--')\n",
    "    for _, trial in pas_aff_vel_df[np.isin(pas_aff_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        axs[4][i].plot(passive_x_axis, trial.pred_vel[plot_dim], color='magenta', alpha = alpha, linewidth=0.5)\n",
    "        axs[4][i].plot(passive_x_axis, trial[y_field][plot_dim], color=color, alpha = alpha, linewidth=0.5)\n",
    "        axs[4][i].axvline(x=120, ls='--')\n",
    "    for _, trial in pas_both_vel_df[np.isin(pas_both_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        axs[5][i].plot(passive_x_axis, trial.pred_vel[plot_dim], color='brown', alpha = alpha, linewidth=0.5)\n",
    "        axs[5][i].plot(passive_x_axis, trial[y_field][plot_dim], color=color, alpha = alpha, linewidth=0.5)\n",
    "        axs[5][i].spines[['right', 'top']].set_visible(False)\n",
    "        axs[5][i].axvline(x=120, ls='--')\n",
    "    i+=2\n",
    "\n",
    "fig.supxlabel('Time after movement onset (ms)')\n",
    "# axs[0][0].set_ylabel('Prediction',fontsize=14)\n",
    "# axs[1][0].set_ylabel('Prediction',fontsize=14)\n",
    "# axs[0][0].set_ylabel('Hand acceleration \\n (cm/s^2)',fontsize=14)\n",
    "# axs[1][0].set_ylabel('Hand acceleration \\n (cm/s^2)',fontsize=14)\n",
    "axs[0][0].set_ylabel('Hand velocity \\n (cm/s)',fontsize=14)\n",
    "axs[1][0].set_ylabel('Hand velocity \\n (cm/s)',fontsize=14)\n",
    "\n",
    "\n",
    "axs[0][0].set_title('0 deg')\n",
    "axs[0][1].set_title('90 deg')\n",
    "axs[0][2].set_title('180 deg')\n",
    "axs[0][3].set_title('270 deg')\n",
    "\n",
    "\n",
    "# legend_elements = [Patch(facecolor='magenta', label='Afferent prediction'),\n",
    "#                     Patch(facecolor='k', label='Efferent prediction')]\n",
    "# legend_elements = [Patch(facecolor='magenta', label='Afferent prediction')]\n",
    "# plt.legend(handles=legend_elements)\n",
    "plt.tight_layout()\n",
    "# figDir = '/Users/sherryan/area2_population_analysis/'\n",
    "# plt.savefig(figDir + monkey + '_cross_vel_both_early_aligned.pdf', dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross condition signal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monkey = 'Duncan_20190710'\n",
    "monkey = 'Han_20171207'\n",
    "data = np.load(monkey+'_MC_smooth40_spikes_hand_vel_act_r2s'+'.npz')\n",
    "data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lag_idx = np.argmax(np.mean(data['r2_cd_fb'],axis=1))\n",
    "plt.plot(data['cd_fb_coefs'][best_lag_idx,:,:].T)\n",
    "data['cd_fb_coefs'][best_lag_idx,:,:]>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lag_idx = np.argmax(np.mean(data['SC_r2_cd_fb'],axis=1))\n",
    "plt.plot(data['SC_cd_fb_coefs'][best_lag_idx,:,:].T)\n",
    "data['SC_cd_fb_coefs'][best_lag_idx,:,:]>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lag_idx = np.argmax(np.mean(data['r2_cd_fb'],axis=1))\n",
    "plt.plot(data['cd_fb_coefs'][best_lag_idx,:,:].T)\n",
    "data['cd_fb_coefs'][best_lag_idx,:,:]>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lag_idx = np.argmax(np.mean(data['SC_r2_cd_fb'],axis=1))\n",
    "plt.plot(data['SC_cd_fb_coefs'][best_lag_idx,:,:].T)\n",
    "data['SC_cd_fb_coefs'][best_lag_idx,:,:]>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['cdfb_coefs'].T)\n",
    "data['cdfb_coefs']>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['cdfb_coefs_sc'].T)\n",
    "data['cdfb_coefs_sc']>0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### PC proj_back -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(monkey+'_CDFB_weights_'+'PCA_40'+'.npz')\n",
    "X = data['CD_axes']\n",
    "eff_weights = pca.inverse_transform(X)\n",
    "print(eff_weights.shape)\n",
    "X = data['FB_axes']\n",
    "aff_weights = pca.inverse_transform(X)\n",
    "print(aff_weights.shape)\n",
    "np.savez(monkey+'_CDFB_weights_pc_proj_back', CD_axes = eff_weights, FB_axes = aff_weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_weights_mean = np.mean(abs(aff_weights),axis=0)\n",
    "print(aff_weights_mean.shape)\n",
    "plt.hist(aff_weights_mean)\n",
    "plt.show()\n",
    "eff_weights_mean = np.mean(abs(eff_weights),axis=0)\n",
    "print(eff_weights_mean.shape)\n",
    "plt.hist(eff_weights_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neuron weights plot\n",
    "\n",
    "def adjacent_values(vals, q1, q3):\n",
    "    upper_adjacent_value = q3 + (q3 - q1) * 1.5\n",
    "    upper_adjacent_value = np.clip(upper_adjacent_value, q3, vals[-1])\n",
    "    lower_adjacent_value = q1 - (q3 - q1) * 1.5\n",
    "    lower_adjacent_value = np.clip(lower_adjacent_value, vals[0], q1)\n",
    "    return lower_adjacent_value, upper_adjacent_value\n",
    "    \n",
    "plt.hist(aff_weights_mean)\n",
    "plt.show()\n",
    "plt.hist(eff_weights_mean)\n",
    "plt.show()\n",
    "\n",
    "Ki_x = []\n",
    "for i in range(n_neurons):\n",
    "    Wa = aff_weights_mean[i]\n",
    "    We = eff_weights_mean[i]\n",
    "    if abs(Wa) > 0.02 or abs(We) > 0.02:\n",
    "        Ki_x.append((abs(Wa) - abs(We)) / (abs(Wa)+abs(We)))\n",
    "    else:\n",
    "        Ki_x.append(np.nan)\n",
    "print(len(Ki_x))\n",
    "\n",
    "Ki_x = np.array(Ki_x)\n",
    "Ki_x_plot = Ki_x[~np.isnan(Ki_x)]\n",
    "print(len(Ki_x_plot))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "# fig.suptitle('Relative contribution to acc decoder between Afference and Efference')\n",
    "import seaborn as sns\n",
    "parts = ax.violinplot(Ki_x_plot,showmeans = False, showextrema=False)\n",
    "for pc in parts['bodies']:\n",
    "    pc.set_facecolor('grey')\n",
    "    pc.set_edgecolor('black')\n",
    "    pc.set_alpha(.5)\n",
    "\n",
    "\n",
    "quartile1, medians, quartile3 = np.percentile(Ki_x_plot, [25, 50, 75])\n",
    "whiskers = adjacent_values(sorted(Ki_x_plot), quartile1, quartile3)\n",
    "whiskers_min, whiskers_max = whiskers[0], whiskers[1]\n",
    "ax.scatter(1, medians, marker='o', color='white', s=20, zorder=3)\n",
    "ax.vlines(1, quartile1, quartile3, color='k', linestyle='-', lw=5)\n",
    "ax.vlines(1, whiskers_min, whiskers_max, color='k', linestyle='-', lw=1)\n",
    "\n",
    "ax.set_ylabel('Relative contribution')\n",
    "ax.set_xticks([])\n",
    "# ax.set_xlabel('All neurons')\n",
    "ax.set_ylim([-1,1])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_contrib_violin_vel.pdf', dpi = 'figure')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.hist(sorted(Ki_x_plot),10,rwidth=0.8,color = 'grey')\n",
    "plt.xlabel('Relative contribution')\n",
    "plt.ylabel('Neuron count')\n",
    "plt.xlim([-1.01,1.01])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_contrib_bar_vel.pdf', dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argwhere(np.isnan(np.array(Ki_x))).squeeze()\n",
    "# np.argsort(np.array(Ki_x))\n",
    "valid_sort = np.array([x for x in np.argsort(np.array(Ki_x)) if x not in np.argwhere(np.isnan(np.array(Ki_x))).squeeze()])\n",
    "valid_sort\n",
    "\n",
    "#To pick exammple single neuron \n",
    "# np.array(Ki_x)[47]\n",
    "\n",
    "np.savez(monkey+'_cdfb_contrib'+'_pc_proj', relative_contrib = np.array(Ki_x), valid_sort=valid_sort)\n",
    "data = np.load(monkey+'_cdfb_contrib_pc_proj.npz')\n",
    "data.files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Null distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monkey = 'Duncan_20190710'\n",
    "monkey = 'Han_20171207'\n",
    "data = np.load(monkey+'_v6_alt_zscore_unsmoothed_cdfb_weights_spikes.npz')\n",
    "eff_weights = data['CD_axes']\n",
    "print(eff_weights.shape)\n",
    "aff_weights = data['FB_axes']\n",
    "print(aff_weights.shape)\n",
    "true_p_angles = principal_angles(eff_weights.T, aff_weights.T)\n",
    "print(\"Principal angles\", np.degrees(true_p_angles)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.degrees(true_p_angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(eff_weights.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(aff_weights.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_matrix = eff_weights\n",
    "# Compute the range (max - min) and standard deviation of the entire matrix\n",
    "E_matrix_range = np.max(E_matrix) - np.min(E_matrix)\n",
    "E_matrix_mean = np.mean(E_matrix)\n",
    "E_matrix_std = np.std(E_matrix)\n",
    "E_matrix_mean = np.mean(E_matrix)\n",
    "print(f\"Range of the matrix: {E_matrix_range}\")\n",
    "print(f\"Mean of the matrix: {E_matrix_mean}\")\n",
    "print(f\"Standard deviation of the matrix: {E_matrix_std}\")\n",
    "\n",
    "A_matrix = aff_weights\n",
    "# Compute the range (max - min) and standard deviation of the entire matrix\n",
    "A_matrix_range = np.max(A_matrix) - np.min(A_matrix)\n",
    "A_matrix_mean = np.mean(A_matrix)\n",
    "A_matrix_std = np.std(A_matrix)\n",
    "print(f\"Range of the matrix: {A_matrix_range}\")\n",
    "print(f\"Mean of the matrix: {A_matrix_mean}\")\n",
    "print(f\"Standard deviation of the matrix: {A_matrix_std}\")\n",
    "\n",
    "N = 1000\n",
    "p_angles = nans([N])\n",
    "for i in range(N):\n",
    "    # Draw samples from a Gaussian distribution with the computed standard deviation\n",
    "    rand_E_matrix = np.random.normal(loc=E_matrix_mean, scale=E_matrix_std, size=E_matrix.shape)\n",
    "    rand_A_matrix = np.random.normal(loc=A_matrix_mean, scale=A_matrix_std, size=A_matrix.shape)\n",
    "    X = rand_E_matrix.T\n",
    "    Y = rand_A_matrix.T\n",
    "    p_angles[i] = np.degrees(principal_angles(X, Y))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.hist(p_angles,color='blue',alpha=0.5,edgecolor='black',bins=20)\n",
    "plt.axvline(x=np.degrees(true_p_angles)[0],color='k',linestyle='--',linewidth=2)\n",
    "plt.xlabel('First principal angle (deg)')\n",
    "plt.ylabel('Count')\n",
    "plt.text(np.degrees(true_p_angles)[0] + 1, plt.gca().get_ylim()[1] * 0.9,  # Adjust position as needed\n",
    "         \"True principal angle \" + str(np.degrees(true_p_angles)[0]), color='black', fontsize=10)\n",
    "plt.text(70, 60,np.sum(p_angles < np.degrees(true_p_angles)[0])/1000)\n",
    "plt.ylim([0,140])\n",
    "# plt.savefig(figDir + monkey + '_zscore_null_dist.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relative contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monkey = 'Duncan_20190710'\n",
    "monkey = 'Han_20171207'\n",
    "data = np.load(monkey+'_v6_zscore_unsmoothed_cdfb_weights_spikes.npz')\n",
    "\n",
    "eff_weights = data['CD_axes']\n",
    "print(eff_weights.shape)\n",
    "aff_weights = data['FB_axes']\n",
    "print(aff_weights.shape)\n",
    "\n",
    "n_neurons = aff_weights.shape[1]\n",
    "print(n_neurons,'neurons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_weights_mean = np.mean(abs(aff_weights),axis=0)\n",
    "print(aff_weights_mean.shape)\n",
    "plt.hist(aff_weights_mean)\n",
    "plt.show()\n",
    "eff_weights_mean = np.mean(abs(eff_weights),axis=0)\n",
    "print(eff_weights_mean.shape)\n",
    "plt.hist(eff_weights_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_weights_sum = np.sum(abs(aff_weights),axis=0)\n",
    "print(aff_weights_sum.shape)\n",
    "plt.hist(aff_weights_sum)\n",
    "plt.show()\n",
    "eff_weights_sum = np.sum(abs(eff_weights),axis=0)\n",
    "print(eff_weights_sum.shape)\n",
    "plt.hist(eff_weights_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neuron weights plot\n",
    "\n",
    "def adjacent_values(vals, q1, q3):\n",
    "    upper_adjacent_value = q3 + (q3 - q1) * 1.5\n",
    "    upper_adjacent_value = np.clip(upper_adjacent_value, q3, vals[-1])\n",
    "    lower_adjacent_value = q1 - (q3 - q1) * 1.5\n",
    "    lower_adjacent_value = np.clip(lower_adjacent_value, vals[0], q1)\n",
    "    return lower_adjacent_value, upper_adjacent_value\n",
    "    \n",
    "plt.hist(aff_weights_mean)\n",
    "plt.show()\n",
    "plt.hist(eff_weights_mean)\n",
    "plt.show()\n",
    "\n",
    "aff_threshold = .0\n",
    "eff_threshold = .0\n",
    "\n",
    "print('aff_threshold',aff_threshold)\n",
    "print('eff_threshold',eff_threshold)\n",
    "Ki_x = []\n",
    "for i in range(n_neurons):\n",
    "    Wa = aff_weights_mean[i]\n",
    "    We = eff_weights_mean[i]\n",
    "    if abs(Wa) > aff_threshold or abs(We) > eff_threshold:\n",
    "        Ki_x.append((abs(Wa) - abs(We)) / (abs(Wa)+abs(We)))\n",
    "    else:\n",
    "        Ki_x.append(np.nan)\n",
    "print(len(Ki_x))\n",
    "\n",
    "Ki_x = np.array(Ki_x)\n",
    "Ki_x_plot = Ki_x[~np.isnan(Ki_x)]\n",
    "print(len(Ki_x_plot))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "# fig.suptitle('Relative contribution to acc decoder between Afference and Efference')\n",
    "import seaborn as sns\n",
    "parts = ax.violinplot(Ki_x_plot,showmeans = False, showextrema=False)\n",
    "for pc in parts['bodies']:\n",
    "    pc.set_facecolor('grey')\n",
    "    pc.set_edgecolor('black')\n",
    "    pc.set_alpha(.5)\n",
    "\n",
    "\n",
    "quartile1, medians, quartile3 = np.percentile(Ki_x_plot, [25, 50, 75])\n",
    "whiskers = adjacent_values(sorted(Ki_x_plot), quartile1, quartile3)\n",
    "whiskers_min, whiskers_max = whiskers[0], whiskers[1]\n",
    "ax.scatter(1, medians, marker='o', color='white', s=20, zorder=3)\n",
    "ax.vlines(1, quartile1, quartile3, color='k', linestyle='-', lw=5)\n",
    "ax.vlines(1, whiskers_min, whiskers_max, color='k', linestyle='-', lw=1)\n",
    "\n",
    "ax.set_ylabel('Relative contribution')\n",
    "ax.set_xticks([])\n",
    "# ax.set_xlabel('All neurons')\n",
    "ax.set_ylim([-1,1])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_contrib_violin_vel.pdf', dpi = 'figure')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.hist(sorted(Ki_x_plot),10,rwidth=0.8,color = 'grey')\n",
    "plt.xlabel('Relative contribution')\n",
    "plt.ylabel('Neuron count')\n",
    "plt.xlim([-1.01,1.01])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_contrib_bar_vel.pdf', dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argwhere(np.isnan(np.array(Ki_x))).squeeze()\n",
    "# np.argsort(np.array(Ki_x))\n",
    "valid_sort = np.array([x for x in np.argsort(np.array(Ki_x)) if x not in np.argwhere(np.isnan(np.array(Ki_x))).squeeze()])\n",
    "print(valid_sort)\n",
    "print(len(valid_sort),'neurons')\n",
    "\n",
    "#To pick exammple single neuron \n",
    "print(np.array(Ki_x)[47])\n",
    "print(np.array(Ki_x)[151])\n",
    "# print(np.array(Ki_x)[30])\n",
    "print(np.array(Ki_x)[75])\n",
    "\n",
    "# np.savez(monkey+'_cdfb_contrib'+'_neuron', relative_contrib = np.array(Ki_x), valid_sort=valid_sort)\n",
    "# data = np.load(monkey+'_cdfb_contrib_neuron.npz')\n",
    "# data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.linalg import subspace_angles\n",
    "\n",
    "# # Parameters\n",
    "# ambient_dim = 50  # Ambient space dimension\n",
    "# dim_A = 4          # Dimension of first subspace\n",
    "# dim_B = 2          # Dimension of second subspace\n",
    "# num_trials = 1000  # Number of random subspace pairs\n",
    "\n",
    "# # Function to generate a random orthonormal basis of a subspace\n",
    "# def random_subspace(dim, ambient_dim):\n",
    "#     Q, _ = np.linalg.qr(np.random.randn(ambient_dim, dim))\n",
    "#     return Q\n",
    "\n",
    "# # Store smallest principal angles (in degrees)\n",
    "# min_angles = []\n",
    "\n",
    "# # Simulation loop\n",
    "# for _ in range(num_trials):\n",
    "#     A = random_subspace(dim_A, ambient_dim)\n",
    "#     B = random_subspace(dim_B, ambient_dim)\n",
    "#     angles_rad = subspace_angles(A, B)        # Principal angles in radians\n",
    "#     smallest_angle_deg = np.degrees(min(angles_rad))  # Convert to degrees\n",
    "#     min_angles.append(smallest_angle_deg)\n",
    "\n",
    "# # Plotting the histogram\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.hist(min_angles, bins=50, density=True, alpha=0.75, edgecolor='black')\n",
    "# plt.axvline(71.3, color='red', linestyle='--', label='Angle (71.3°)')\n",
    "# plt.title('Distribution of Smallest Principal Angle\\nBetween Random 4D and 2D Subspaces in ℝ¹⁰⁰')\n",
    "# plt.xlabel('Smallest Principal Angle (degrees)')\n",
    "# plt.ylabel('Density')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.linalg import subspace_angles\n",
    "\n",
    "# # Parameters\n",
    "# ambient_dim = 100  # Ambient space dimension\n",
    "# dim_A = 4          # Dimension of first subspace\n",
    "# dim_B = 2          # Dimension of second subspace\n",
    "# num_trials = 1000  # Number of random subspace pairs\n",
    "\n",
    "# # Function to generate a random orthonormal basis of a subspace\n",
    "# def random_subspace(dim, ambient_dim):\n",
    "#     Q, _ = np.linalg.qr(np.random.randn(ambient_dim, dim))\n",
    "#     return Q\n",
    "\n",
    "# # Store smallest principal angles (in degrees)\n",
    "# min_angles = []\n",
    "\n",
    "# # Simulation loop\n",
    "# for _ in range(num_trials):\n",
    "#     A = random_subspace(dim_A, ambient_dim)\n",
    "#     B = random_subspace(dim_B, ambient_dim)\n",
    "#     angles_rad = subspace_angles(A, B)        # Principal angles in radians\n",
    "#     smallest_angle_deg = np.degrees(min(angles_rad))  # Convert to degrees\n",
    "#     min_angles.append(smallest_angle_deg)\n",
    "\n",
    "# # Plotting the histogram\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.hist(min_angles, bins=50, density=True, alpha=0.75, edgecolor='black')\n",
    "# plt.axvline(71, color='red', linestyle='--', label='Angle (71.0°)')\n",
    "# plt.title('Distribution of Smallest Principal Angle\\nBetween Random 4D and 2D Subspaces in ℝ100')\n",
    "# plt.xlabel('Smallest Principal Angle (degrees)')\n",
    "# plt.ylabel('Density')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdt_env",
   "language": "python",
   "name": "sdt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
