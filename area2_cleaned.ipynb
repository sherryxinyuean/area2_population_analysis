{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlb_tools.nwb_interface import NWBDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "from sklearn.linear_model import Ridge, LinearRegression, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, StratifiedShuffleSplit\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "from scipy.linalg import orth\n",
    "\n",
    "from Neural_Decoding.preprocessing_funcs import get_spikes_with_history\n",
    "from Area2_analysis.lr_funcs import process_train_test, gaussian_filter1d_oneside, comp_cc, xcorr, r2_score\n",
    "from Area2_analysis.lr_funcs import get_sses_pred, get_sses_mean, nans\n",
    "from Area2_analysis.lr_funcs import fit_and_predict, sub_and_predict, pred_with_new_weights\n",
    "from Area2_analysis.lr_funcs import fit_and_predict_lasso, sub_and_predict_lasso, fit_and_predict_MC\n",
    "from Area2_analysis.lr_funcs import calc_proj, principal_angles, angle_between\n",
    "\n",
    "matplotlib.rc('font', size=18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import Area2_analysis.lr_funcs\n",
    "# importlib.reload(Area2_analysis.lr_funcs)\n",
    "# from Area2_analysis.lr_funcs import fit_and_predict_DNN, fit_and_predict_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figDir = '/Users/sherryan/area2_population_analysis/paper/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldername = \"~/area2_population_analysis/s1-kinematics/actpas_NWB/\"\n",
    "# monkey = \"Han_20171207\"\n",
    "# filename = foldername + monkey + \"_COactpas_TD_offset6.nwb\"\n",
    "\n",
    "# monkey = \"Chips_20170913\"\n",
    "# filename = foldername + monkey + \"_COactpas_TD.nwb\"\n",
    "\n",
    "monkey = 'Duncan_20190710'\n",
    "filename = foldername + monkey + \"_COactpas_offset6.nwb\"\n",
    "\n",
    "dataset_10ms = NWBDataset(filename, split_heldout=False)\n",
    "\n",
    "dataset_10ms.resample(10) #in 10-ms bin, has to resample first for Duncan\n",
    "bin_width = dataset_10ms.bin_width\n",
    "print(bin_width)\n",
    "\n",
    "# xyz_force = np.array([dataset_5ms.data['force']['x'].to_numpy(), dataset_5ms.data['force']['y'].to_numpy(), dataset_5ms.data['force']['z'].to_numpy()]).T\n",
    "# dataset_10ms.add_continuous_data(xyz_force,'manip_force',chan_names = ['x','y','z'])\n",
    "\n",
    "dataset_10ms.smooth_spk(40, name='smth_40')\n",
    "\n",
    "# dataset_10ms.smooth_spk(20, name='smth_20')\n",
    "\n",
    "# gaussian_kernel_width = 150 #in ms\n",
    "# sigma = int(gaussian_kernel_width/bin_width)\n",
    "# data_smoothed = gaussian_filter1d_oneside(dataset_10ms.data.spikes.to_numpy().astype(np.float64),sigma,axis=0)\n",
    "# dataset_10ms.add_continuous_data(data_smoothed,'spikes_smth_150_oneside')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xy_pos = dataset_10ms.data['hand_pos'].to_numpy()\n",
    "# xy_vel = np.diff(xy_pos, axis = 0, prepend=[xy_pos[0]])\n",
    "# dataset_10ms.add_continuous_data(xy_vel,'hand_vel_2',chan_names = ['x','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xy_vel = dataset_10ms.data['hand_vel'].to_numpy()\n",
    "# xy_acc = np.diff(xy_vel, axis = 0, prepend=[xy_vel[0]])\n",
    "# dataset_10ms.add_continuous_data(xy_acc,'hand_acc',chan_names = ['x','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = '/Users/sherryan/area2_population_analysis/s1-kinematics/'+monkey+'_COactpas_with_emg_TD.mat'\n",
    "# import scipy.io\n",
    "# mat = scipy.io.loadmat(filename)\n",
    "# EMG = mat['trial_data']['emg'][0,0]\n",
    "# dataset_10ms.add_continuous_data(EMG,'EMG')\n",
    "# mat['trial_data']['emg_names'][0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = dataset_10ms.data['EMG'].to_numpy()\n",
    "# print(all_data.shape)\n",
    "# data_for_pca = all_data[~np.isnan(all_data).any(axis=1)]\n",
    "# print(data_for_pca.shape)\n",
    "# explained_var = []\n",
    "# for n in range(20):\n",
    "#     scaler = StandardScaler()\n",
    "#     X = scaler.fit_transform(data_for_pca)\n",
    "#     pca = PCA(n_components=n)\n",
    "#     X = pca.fit(X)\n",
    "#     explained_var.append(np.sum(pca.explained_variance_ratio_))\n",
    "# plt.plot(range(20),explained_var)\n",
    "# print(explained_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = dataset_10ms.data['EMG'].to_numpy()\n",
    "# print(all_data.shape)\n",
    "# data_for_pca = all_data[~np.isnan(all_data).any(axis=1)]\n",
    "# print(data_for_pca.shape)\n",
    "# n_dims = 10\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(data_for_pca)\n",
    "# pca = PCA(n_components=n_dims,random_state = 42)\n",
    "# X = pca.fit(X)\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(data_for_pca)\n",
    "# pca = PCA(n_components=n_dims,random_state = 42)\n",
    "# X = pca.fit(X)\n",
    "    \n",
    "# PCA_data = nans([all_data.shape[0],n_dims])\n",
    "# idx = 0\n",
    "# for dp in all_data:\n",
    "#     dp = dp.reshape((1, -1))\n",
    "#     if np.isnan(dp).any():\n",
    "#         dp_pca = nans([1,n_dims])\n",
    "#     else:\n",
    "#         dp_pca = pca.transform(scaler.transform(dp))\n",
    "#     PCA_data[idx,:] = dp_pca\n",
    "#     idx+=1\n",
    "# print(PCA_data.shape)\n",
    "# dataset_10ms.add_continuous_data(PCA_data,'EMG_PCA')\n",
    "# print('PCA total var explained:',sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muscle_len = dataset_10ms.data['muscle_len'].to_numpy()\n",
    "# muscle_vel = dataset_10ms.data['muscle_vel'].to_numpy()\n",
    "# joint_ang = dataset_10ms.data['joint_ang'].to_numpy()\n",
    "# joint_vel = dataset_10ms.data['joint_vel'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = np.hstack([muscle_len,muscle_vel])\n",
    "# print(all_data.shape)\n",
    "# data_for_pca = all_data[~np.isnan(all_data).any(axis=1)]\n",
    "# print(data_for_pca.shape)\n",
    "# explained_var = []\n",
    "# for n in range(20):\n",
    "#     scaler = StandardScaler()\n",
    "#     X = scaler.fit_transform(data_for_pca)\n",
    "#     pca = PCA(n_components=n)\n",
    "#     X = pca.fit(X)\n",
    "#     explained_var.append(np.sum(pca.explained_variance_ratio_))\n",
    "# plt.plot(range(20),explained_var)\n",
    "# plt.title('muscle len+vel')\n",
    "# print(explained_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = np.hstack([joint_ang,joint_vel])\n",
    "# print(all_data.shape)\n",
    "# data_for_pca = all_data[~np.isnan(all_data).any(axis=1)]\n",
    "# print(data_for_pca.shape)\n",
    "# explained_var = []\n",
    "# for n in range(14):\n",
    "#     scaler = StandardScaler()\n",
    "#     X = scaler.fit_transform(data_for_pca)\n",
    "#     pca = PCA(n_components=n)\n",
    "#     X = pca.fit(X)\n",
    "#     explained_var.append(np.sum(pca.explained_variance_ratio_))\n",
    "# plt.plot(range(14),explained_var)\n",
    "# plt.title('joint ang+vel')\n",
    "# print(explained_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = np.hstack([muscle_len,muscle_vel])\n",
    "# print(all_data.shape)\n",
    "# data_for_pca = all_data[~np.isnan(all_data).any(axis=1)]\n",
    "# print(data_for_pca.shape)\n",
    "# n_dims = 10\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(data_for_pca)\n",
    "# pca = PCA(n_components=n_dims,random_state = 42)\n",
    "# X = pca.fit(X)\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(data_for_pca)\n",
    "# pca = PCA(n_components=n_dims,random_state = 42)\n",
    "# X = pca.fit(X)\n",
    "    \n",
    "# PCA_data = nans([all_data.shape[0],n_dims])\n",
    "# idx = 0\n",
    "# for dp in all_data:\n",
    "#     dp = dp.reshape((1, -1))\n",
    "#     if np.isnan(dp).any():\n",
    "#         dp_pca = nans([1,n_dims])\n",
    "#     else:\n",
    "#         dp_pca = pca.transform(scaler.transform(dp))\n",
    "#     PCA_data[idx,:] = dp_pca\n",
    "#     idx+=1\n",
    "# print(PCA_data.shape)\n",
    "# dataset_10ms.add_continuous_data(PCA_data,'muscle_PCA')\n",
    "# print('PCA total var explained:',sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = np.hstack([joint_ang,joint_vel])\n",
    "# print(all_data.shape)\n",
    "# data_for_pca = all_data[~np.isnan(all_data).any(axis=1)]\n",
    "# print(data_for_pca.shape)\n",
    "# n_dims = 10\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(data_for_pca)\n",
    "# pca = PCA(n_components=n_dims,random_state = 42)\n",
    "# X = pca.fit(X)\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(data_for_pca)\n",
    "# pca = PCA(n_components=n_dims,random_state = 42)\n",
    "# X = pca.fit(X)\n",
    "    \n",
    "# PCA_data = nans([all_data.shape[0],n_dims])\n",
    "# idx = 0\n",
    "# for dp in all_data:\n",
    "#     dp = dp.reshape((1, -1))\n",
    "#     if np.isnan(dp).any():\n",
    "#         dp_pca = nans([1,n_dims])\n",
    "#     else:\n",
    "#         dp_pca = pca.transform(scaler.transform(dp))\n",
    "#     PCA_data[idx,:] = dp_pca\n",
    "#     idx+=1\n",
    "# print(PCA_data.shape)\n",
    "# dataset_10ms.add_continuous_data(PCA_data,'joint_PCA')\n",
    "# print('PCA total var explained:',sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dims = 20 \n",
    "all_data = np.array(dataset_10ms.data.spikes_smth_40)\n",
    "print(all_data.shape)\n",
    "if not np.isnan(all_data).any():\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(all_data)\n",
    "    pca = PCA(n_components=n_dims,random_state = 42)\n",
    "    PCA_data = pca.fit_transform(X)\n",
    "print(PCA_data.shape)\n",
    "# dataset_10ms.add_continuous_data(X,'spikes_smth_40_zscored')\n",
    "dataset_10ms.add_continuous_data(PCA_data,'PCA_40')\n",
    "print('PCA total var explained:',sum(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial_mask = active_mask\n",
    "n_trials = dataset_10ms.trial_info.shape[0]\n",
    "print(n_trials,'total trials')\n",
    "n_neurons = dataset_10ms.data.spikes.shape[1]\n",
    "print(n_neurons,'neurons')\n",
    "\n",
    "#make dictionary for trial condition (reaching directions) for Stratified CV\n",
    "dataset = dataset_10ms\n",
    "active_mask = (dataset.trial_info.ctr_hold_bump==0) & (dataset.trial_info['split'] != 'none')\n",
    "passive_mask = (dataset.trial_info.ctr_hold_bump==1) & (dataset.trial_info['split'] != 'none')\n",
    "nan_mask = (np.isnan(dataset.trial_info.ctr_hold_bump)) & (dataset.trial_info['split'] != 'none')\n",
    "all_mask = (dataset.trial_info['split'] != 'none')\n",
    "\n",
    "trial_mask = all_mask\n",
    "valid_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(valid_n_trials,'valid trials')\n",
    "\n",
    "trial_mask = active_mask\n",
    "active_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "active_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(active_n_trials,'active trials')\n",
    "\n",
    "trial_mask = passive_mask\n",
    "passive_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "passive_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(passive_n_trials,'passive trials')\n",
    "\n",
    "trial_mask = nan_mask\n",
    "nan_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "nan_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(nan_n_trials,'reach bump trials')\n",
    "\n",
    "active_cond_dir_idx = []\n",
    "passive_cond_dir_idx = []\n",
    "nan_cond_dir_idx = []\n",
    "nan_bump_cond_dir_idx = []\n",
    "for direction in [0,45,90,135,180,225,270,315]:\n",
    "# for direction in [0,90,180,270]:\n",
    "    active_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & (dataset.trial_info['ctr_hold_bump'] == 0) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "    passive_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & (dataset.trial_info['ctr_hold_bump'] == 1) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "    nan_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & (np.isnan(dataset.trial_info.ctr_hold_bump)) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "    nan_bump_cond_dir_idx.append(np.where((dataset.trial_info['bump_dir']%360 == direction) & (np.isnan(dataset.trial_info.ctr_hold_bump)) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "\n",
    "active_cond_dict = nans([active_n_trials])\n",
    "i = 0\n",
    "for idx in active_trials_idx:\n",
    "    for cond in range(0,len(active_cond_dir_idx)):\n",
    "        if idx in active_cond_dir_idx[cond]:\n",
    "            active_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(active_cond_dict)\n",
    "print(len(active_cond_dict))\n",
    "\n",
    "passive_cond_dict = nans([passive_n_trials])\n",
    "i = 0\n",
    "for idx in passive_trials_idx:\n",
    "    for cond in range(0,len(passive_cond_dir_idx)):\n",
    "        if idx in passive_cond_dir_idx[cond]:\n",
    "            passive_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(passive_cond_dict)\n",
    "print(len(passive_cond_dict))\n",
    "\n",
    "nan_cond_dict = nans([nan_n_trials])\n",
    "i = 0\n",
    "for idx in nan_trials_idx:\n",
    "    for cond in range(0,len(nan_cond_dir_idx)):\n",
    "        if idx in nan_cond_dir_idx[cond]:\n",
    "            nan_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(nan_cond_dict)\n",
    "print(len(nan_cond_dict))\n",
    "\n",
    "nan_bump_cond_dict = nans([nan_n_trials])\n",
    "i = 0\n",
    "for idx in nan_trials_idx:\n",
    "    for cond in range(0,len(nan_bump_cond_dir_idx)):\n",
    "        if idx in nan_bump_cond_dir_idx[cond]:\n",
    "            nan_bump_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(nan_bump_cond_dict)\n",
    "print(len(nan_bump_cond_dict))\n",
    "\n",
    "if monkey == 'Duncan_20190710':\n",
    "    active_df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range = (-100,100), ignored_trials = ~active_mask)\n",
    "    del_indices = list(set(active_trials_idx) - set(active_df['trial_id'].unique()))\n",
    "    print('was',active_n_trials,'active trials')\n",
    "    active_n_trials = active_n_trials - len(list(set(active_trials_idx) - set(active_df['trial_id'].unique())))\n",
    "    active_cond_dict_onset = np.delete(active_cond_dict,np.where(np.isin(active_trials_idx, del_indices)))\n",
    "    print('now',active_n_trials,'active trials')\n",
    "    print(len(active_cond_dict_onset))\n",
    "\n",
    "    passive_df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range = (-100,100), ignored_trials = ~passive_mask)\n",
    "    del_indices = list(set(passive_trials_idx) - set(passive_df['trial_id'].unique()))\n",
    "    print('was',passive_n_trials,'passive trials')\n",
    "    passive_n_trials = passive_n_trials - len(list(set(passive_trials_idx) - set(passive_df['trial_id'].unique())))\n",
    "    passive_cond_dict = np.delete(passive_cond_dict,np.where(np.isin(passive_trials_idx, del_indices)))\n",
    "    print('now',passive_n_trials,'passive trials')\n",
    "    print(len(passive_cond_dict))\n",
    "\n",
    "active_df = dataset_10ms.make_trial_data(align_field='move_offset_time', align_range = (-100,0), ignored_trials = ~active_mask)\n",
    "del_indices = list(set(active_trials_idx) - set(active_df['trial_id'].unique()))\n",
    "print('was',active_n_trials,'active trials')\n",
    "active_cond_dict_offset = np.delete(active_cond_dict, np.where(np.isin(active_trials_idx, del_indices))[0])\n",
    "print('now')\n",
    "print(len(active_cond_dict_offset))\n",
    "if monkey == 'Duncan_20190710':\n",
    "    active_cond_dict = active_cond_dict_onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sca.models import SCA\n",
    "align_range = (-100, 1000)\n",
    "active_trial_data = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=align_range, ignored_trials=~active_mask)\n",
    "active_trial_spsm = np.array(active_trial_data.spikes_smth_40)\n",
    "target_n_trials = active_trial_data['trial_id'].nunique()\n",
    "n_timepoints = int((align_range[1]-align_range[0])/bin_width)\n",
    "active_sample_weights= np.ones((target_n_trials, n_timepoints))\n",
    "# active_sample_weights[:,:int(100/dataset_10ms.bin_width)] = 10\n",
    "active_sample_weights = active_sample_weights.flatten()\n",
    "print(active_sample_weights.shape)\n",
    "\n",
    "align_range = (-100, 500)\n",
    "passive_trial_data = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=align_range, ignored_trials=~passive_mask)\n",
    "passive_trial_spsm = np.array(passive_trial_data.spikes_smth_40)\n",
    "target_n_trials = passive_trial_data['trial_id'].nunique()\n",
    "n_timepoints = int((align_range[1]-align_range[0])/bin_width)\n",
    "passive_sample_weights= np.ones((target_n_trials, n_timepoints))\n",
    "passive_sample_weights = passive_sample_weights.flatten()\n",
    "print(passive_sample_weights.shape)\n",
    "\n",
    "sample_weights = np.hstack((active_sample_weights, passive_sample_weights))\n",
    "print(sample_weights.shape)\n",
    "all_trial_spsm = np.concatenate((active_trial_spsm, passive_trial_spsm),axis=0)\n",
    "print(all_trial_spsm.shape)\n",
    "\n",
    "all_data = np.array(dataset_10ms.data.spikes_smth_40)\n",
    "print(all_data.shape)\n",
    "if not np.isnan(all_trial_spsm).any():\n",
    "    scaler = StandardScaler()\n",
    "    X_trial = scaler.fit_transform(all_trial_spsm,sample_weight=sample_weights)\n",
    "    sca = SCA(n_components=n_dims)\n",
    "    sca.fit(X_trial) # scaler and sca fit to trial data\n",
    "    X_all = scaler.transform(all_data) #scaler and sca transform all data\n",
    "    SCA_data = sca.transform(X_all)\n",
    "print(SCA_data.shape)\n",
    "dataset_10ms.add_continuous_data(SCA_data,'SCA_40')\n",
    "print('SCA_40 var explained:',sca.r2_score)\n",
    "\n",
    "ssa_order_smth40 = np.argsort(-np.array(sca.explained_squared_activity))\n",
    "print('SCA_40 activity explained:',sca.explained_squared_activity[ssa_order_smth40])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_range = [-100, 500]\n",
    "lag_range = [-300, 300]\n",
    "kin_range = [trial_range[0] + lag_range[0] + (-200), trial_range[1] + lag_range[1] + (+200)]                                \n",
    "lag_axis = np.arange(lag_range[0], lag_range[1]+1, 10)\n",
    "nrn_axis = np.arange(trial_range[0]+lag_range[0], trial_range[1]+lag_range[1]+1, 10)\n",
    "# To predict trial_range, we need wider neural_range, which requires wider kin_range\n",
    "\n",
    "df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=kin_range, ignored_trials=~active_mask, allow_overlap=True)\n",
    "n_trials = df['trial_id'].nunique()\n",
    "# acc_array = df['hand_acc'].to_numpy().reshape(active_n_trials, -1, 2)\n",
    "vel_array = df['hand_vel'].to_numpy().reshape(active_n_trials, -1, 2)\n",
    "\n",
    "kin_axis = np.arange(kin_range[0], kin_range[1]+1, dataset_10ms.bin_width)\n",
    "print('neural axis',nrn_axis[0], nrn_axis[-1])\n",
    "print('kinematics axis',kin_axis[0], kin_axis[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 0,1\n",
    "n_nrn = 20\n",
    "nrn_weight_0 = np.random.normal(mu, sigma, size = (n_nrn,1)) \n",
    "nrn_weight_1 = np.random.normal(mu, sigma, size = (n_nrn,1))\n",
    "nrn_weight = orth(np.concatenate([nrn_weight_0, nrn_weight_1], axis = 1))\n",
    "nrn_weight_0_save = nrn_weight[:,0][:,np.newaxis]\n",
    "nrn_weight_1_save = nrn_weight[:,1][:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 20\n",
    "r2_runs = nans([n_runs, len(lag_axis)])\n",
    "cd_r2_arr = nans([n_runs, len(lag_axis)])\n",
    "fb_r2_arr = nans([n_runs, len(lag_axis)])\n",
    "cd_lag = -100\n",
    "fb_lag = 50\n",
    "noise_level = 10\n",
    "noise_lv = \"extra\"\n",
    "type = \"align\"\n",
    "\n",
    "for b in range(n_runs):\n",
    "    if b == (n_runs-1):\n",
    "        nrn_weight_0 = nrn_weight_0_save\n",
    "        nrn_weight_1 = nrn_weight_1_save\n",
    "    else:\n",
    "        nrn_weight_0 = np.random.normal(mu, sigma, size = (n_nrn,1)) \n",
    "        nrn_weight_1 = np.random.normal(mu, sigma, size = (n_nrn,1))\n",
    "        nrn_weight = orth(np.concatenate([nrn_weight_0, nrn_weight_1], axis = 1))\n",
    "        nrn_weight_0 = nrn_weight[:,0][:,np.newaxis]\n",
    "        nrn_weight_1 = nrn_weight[:,1][:,np.newaxis]\n",
    "\n",
    "    # Random We and Wa\n",
    "    if type == \"orthog\":\n",
    "        nrn_weight = np.concatenate([nrn_weight_0, nrn_weight_1], axis = 1)\n",
    "    # Random We = Wa\n",
    "    elif type == \"align\":\n",
    "        nrn_weight = np.tile(nrn_weight_0,(1,2)) \n",
    "    elif type == \"oppo\":\n",
    "    # Random We = -Wa\n",
    "        nrn_weight = np.concatenate([nrn_weight_0, -nrn_weight_0], axis = 1) \n",
    "    else:\n",
    "        break\n",
    "    \n",
    "    n_bins = len(nrn_axis)\n",
    "    nrn_activity = nans([n_trials, n_bins, n_nrn])\n",
    "    cd_nrn_activity = nans([n_trials, n_bins, n_nrn])\n",
    "    fb_nrn_activity = nans([n_trials, n_bins, n_nrn])\n",
    "    for i in range(n_bins):\n",
    "        cd_signal = np.outer(vel_array[:, i+np.argwhere(kin_axis==nrn_axis[0]+(-cd_lag))[0,0], 0], nrn_weight[:,0])\n",
    "        fb_signal = np.outer(vel_array[:, i+np.argwhere(kin_axis==nrn_axis[0]+(-fb_lag))[0,0], 0], nrn_weight[:,1])\n",
    "        cd_nrn_activity[:,i,:] = cd_signal\n",
    "        fb_nrn_activity[:,i,:] = fb_signal\n",
    "        nrn_activity[:,i,:] = cd_signal+fb_signal\n",
    "\n",
    "    nrn_activity_flat = nrn_activity.reshape(-1,n_nrn)\n",
    "    sig_noise = np.zeros((n_nrn,n_nrn))\n",
    "    np.fill_diagonal(sig_noise,1)\n",
    "    noise = np.random.multivariate_normal(np.zeros(n_nrn), sig_noise, nrn_activity_flat.shape[0]) * noise_level\n",
    "    noisy_nrn_activity = (nrn_activity_flat+noise).reshape(nrn_activity.shape)\n",
    "\n",
    "    noise = np.random.multivariate_normal(np.zeros(n_nrn), sig_noise, nrn_activity_flat.shape[0]) * noise_level\n",
    "    cd_nrn_activity_flat = cd_nrn_activity.reshape(-1,n_nrn)\n",
    "    noisy_cd_nrn_activity = (cd_nrn_activity_flat+noise).reshape(cd_nrn_activity.shape)\n",
    "\n",
    "    noise = np.random.multivariate_normal(np.zeros(n_nrn), sig_noise, nrn_activity_flat.shape[0]) * noise_level\n",
    "    fb_nrn_activity_flat = fb_nrn_activity.reshape(-1,n_nrn)\n",
    "    noisy_fb_nrn_activity = (fb_nrn_activity_flat+noise).reshape(fb_nrn_activity.shape)    \n",
    "\n",
    "    # noisy_nrn_activity = noisy_cd_nrn_activity + noisy_fb_nrn_activity\n",
    "\n",
    "    coefs_arr = nans([len(lag_axis), n_nrn])\n",
    "    r2_arr = nans([len(lag_axis)]); \n",
    "    cd_r2 = nans([len(lag_axis)]); fb_r2 = nans([len(lag_axis)])\n",
    "    y = vel_array[:,np.argwhere(kin_axis==trial_range[0])[0,0]:np.argwhere(kin_axis==trial_range[1])[0,0],0]\n",
    "    y_reshaped = y.reshape(-1,1)\n",
    "    for j in range(len(lag_axis)):\n",
    "        start, end = int(0+j), int(y.shape[1]+j)\n",
    "        \n",
    "        X = noisy_cd_nrn_activity[:,start:end,:]\n",
    "        X = X.reshape((X.shape[0]* X.shape[1]),X.shape[2])\n",
    "        lr =  LinearRegression().fit(X, y_reshaped)\n",
    "        cd_r2[j] = lr.score(X, y_reshaped)\n",
    "\n",
    "        X = noisy_fb_nrn_activity[:,start:end,:]\n",
    "        X = X.reshape((X.shape[0]* X.shape[1]),X.shape[2])\n",
    "        lr =  LinearRegression().fit(X, y_reshaped)\n",
    "        fb_r2[j] = lr.score(X, y_reshaped)\n",
    "\n",
    "        X = noisy_nrn_activity[:,start:end,:]\n",
    "        X = X.reshape((X.shape[0]* X.shape[1]),X.shape[2])\n",
    "        lr =  LinearRegression().fit(X, y_reshaped)\n",
    "        coefs_arr[j] = lr.coef_\n",
    "        r2_arr[j] = lr.score(X, y_reshaped)\n",
    "\n",
    "    r2_runs[b,:] = r2_arr; \n",
    "    cd_r2_arr[b,:] = cd_r2; fb_r2_arr[b,:] = fb_r2\n",
    "print(r2_runs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(str(monkey)+\"_sim_\"+str(type)+\"_\"+str(noise_lv),\\\n",
    "         noisy_cd_nrn_activity = noisy_cd_nrn_activity, noisy_fb_nrn_activity = noisy_fb_nrn_activity, \\\n",
    "         noisy_nrn_activity = noisy_nrn_activity,\\\n",
    "         cd_r2_arr = cd_r2_arr,fb_r2_arr = fb_r2_arr, \\\n",
    "         r2_runs = r2_runs, nrn_weight=nrn_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "tr_idx = 10\n",
    "nrn_idx = 7\n",
    "\n",
    "plt.plot(nrn_axis,cd_nrn_activity[tr_idx,:,nrn_idx],color = 'green',label = 'cd signal')\n",
    "plt.plot(nrn_axis,fb_nrn_activity[tr_idx,:,nrn_idx],color = 'magenta',label = 'fb signal')\n",
    "plt.plot(nrn_axis,nrn_activity[tr_idx,:,nrn_idx],color = 'brown',label = 'sum signal')\n",
    "plt.plot(nrn_axis,noisy_nrn_activity[tr_idx,:,nrn_idx],color = 'gray',alpha=0.8,label = 'noisy signal')\n",
    "plt.legend(fontsize=8)\n",
    "plt.axvline(0,color = 'k', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey+\"_\"+type+\"_example_nrn_\"+noise_lv+\"_noise.pdf\",dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(r2_runs))\n",
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "# Compute means and standard deviations\n",
    "mean_r2 = np.mean(r2_runs, axis=0)\n",
    "std_r2 = np.std(r2_runs, axis=0)\n",
    "\n",
    "mean_cd = np.mean(cd_r2_arr, axis=0)\n",
    "std_cd = np.std(cd_r2_arr, axis=0)\n",
    "\n",
    "mean_fb = np.mean(fb_r2_arr, axis=0)\n",
    "std_fb = np.std(fb_r2_arr, axis=0)\n",
    "\n",
    "# Plot means\n",
    "plt.plot(lag_axis, mean_r2, color='brown', label='r2_runs')\n",
    "plt.plot(lag_axis, mean_cd, color='green', label='cd_r2_arr',linestyle='--',alpha=0.5)\n",
    "plt.plot(lag_axis, mean_fb, color='magenta', label='fb_r2_arr',linestyle='--',alpha=0.5)\n",
    "\n",
    "# Fill between mean ± std\n",
    "plt.fill_between(lag_axis, mean_r2 - std_r2, mean_r2 + std_r2, color='grey', alpha=0.3)\n",
    "plt.fill_between(lag_axis, mean_cd - std_cd, mean_cd + std_cd, color='grey', alpha=0.3)\n",
    "plt.fill_between(lag_axis, mean_fb - std_fb, mean_fb + std_fb, color='grey', alpha=0.3)\n",
    "\n",
    "plt.axvline(cd_lag, color = 'k', linestyle='--')\n",
    "plt.axvline(fb_lag, color = 'k', linestyle='--')\n",
    "plt.xlabel(\"Time lag (ms)\")\n",
    "plt.ylabel('R2')\n",
    "plt.ylim([-0.05,1.05])\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + \"_\"+type+\"_sim_r2_\"+noise_lv+\"_noise.pdf\", dpi = 'figure')\n",
    "plt.show()\n",
    "# plt.hist(ang_runs)\n",
    "# plt.xlabel(\"Angle (deg)\")\n",
    "# plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_range = [-100, 500]\n",
    "lag_range = [-300, 300]\n",
    "kin_range = [trial_range[0] + lag_range[0] + (-200), trial_range[1] + lag_range[1] + (+200)]                                \n",
    "lag_axis = np.arange(lag_range[0], lag_range[1]+1, 10)\n",
    "nrn_axis = np.arange(trial_range[0]+lag_range[0], trial_range[1]+lag_range[1]+1, 10)\n",
    "# To predict trial_range, we need wider neural_range, which requires wider kin_range\n",
    "\n",
    "df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=kin_range, ignored_trials=~active_mask, allow_overlap=True)\n",
    "n_trials = df['trial_id'].nunique()\n",
    "vel_array = df['hand_vel'].to_numpy().reshape(active_n_trials, -1, 2)\n",
    "\n",
    "kin_axis = np.arange(kin_range[0], kin_range[1], dataset_10ms.bin_width)\n",
    "print('neural axis',nrn_axis[0], nrn_axis[-1])\n",
    "print('kinematics axis',kin_axis[0], kin_axis[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kin_axis\n",
    "perturb_time_ms = 300\n",
    "perturb_idx = np.argmin(np.abs(kin_axis - perturb_time_ms))\n",
    "perturb_width = 30  \n",
    "gaussian_bump = np.exp(-0.5 * ((kin_axis - perturb_time_ms) / perturb_width) ** 2)  # shape: [time]\n",
    "perturb_magnitudes = np.random.choice([-10, 10, -20, 20], size=n_trials)  # or manually define per trial\n",
    "bump_template = gaussian_bump[:, None] * np.array([1, 1]) \n",
    "perturbed_vel_array = vel_array.copy()\n",
    "bump_vel_array = nans(vel_array.shape)\n",
    "for trial in range(n_trials):\n",
    "    bump = perturb_magnitudes[trial] * bump_template  # scale bump\n",
    "    bump_vel_array[trial,:,:] = bump\n",
    "    perturbed_vel_array[trial,:] += bump  # add to original velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,2))\n",
    "plt.plot(kin_axis,bump)\n",
    "plt.xlim([150, 450])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dir =  np.array([0.0, 45.0, 90.0, 135.0, 180.0, 225.0, 270.0, 315.0])\n",
    "plot_dir = [0.0, 90.0, 180.0, 270.0] \n",
    "dir_idx = [np.argwhere(all_dir==dir)[0,0] for dir in plot_dir]\n",
    "\n",
    "plot_dim = 0 # plot x velocity\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "fig, ax = plt.subplots(figsize=(10,2))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "for i in range(len(plot_dir)):\n",
    "    indices = np.array([np.argwhere(active_cond_dict==dir_idx[i])]).flatten()\n",
    "    plt.plot(kin_axis,vel_array[indices,:,plot_dim].T, color = colors[i],linewidth=0.5)\n",
    "plt.axvline([0],color='k',linestyle='--')\n",
    "plt.axvline([-100],color='k')\n",
    "plt.axvline([500],color='k')\n",
    "plt.ylim([-50, 50])\n",
    "plt.xlabel('Time after move onset (ms)')\n",
    "plt.ylabel('Hand velocity (cm/s)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dir =  np.array([0.0, 45.0, 90.0, 135.0, 180.0, 225.0, 270.0, 315.0])\n",
    "plot_dir = [0.0, 90.0, 180.0, 270.0] \n",
    "dir_idx = [np.argwhere(all_dir==dir)[0,0] for dir in plot_dir]\n",
    "\n",
    "plot_dim = 0 # plot x velocity\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "fig, ax = plt.subplots(figsize=(10,2))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "for i in range(len(plot_dir)):\n",
    "    indices = np.array([np.argwhere(active_cond_dict==dir_idx[i])]).flatten()\n",
    "    plt.plot(kin_axis,perturbed_vel_array[indices,:,plot_dim].T, color = colors[i],linewidth=0.5)\n",
    "plt.axvline([0],color='k',linestyle='--')\n",
    "plt.ylim([-50, 50])\n",
    "plt.axvline([-100],color='k')\n",
    "plt.axvline([500],color='k')\n",
    "plt.xlabel('Time after move onset (ms)')\n",
    "plt.ylabel('Hand velocity (cm/s)')\n",
    "# plt.title('perturbation added')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 0,1\n",
    "n_nrn = 20\n",
    "nrn_weight_0 = np.random.normal(mu, sigma, size = (n_nrn,1)) \n",
    "nrn_weight_1 = np.random.normal(mu, sigma, size = (n_nrn,1))\n",
    "nrn_weight = orth(np.concatenate([nrn_weight_0, nrn_weight_1], axis = 1))\n",
    "nrn_weight_0_save = nrn_weight[:,0][:,np.newaxis]\n",
    "nrn_weight_1_save = nrn_weight[:,1][:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 20\n",
    "\n",
    "vel_r2_runs = nans([n_runs, len(lag_axis)])\n",
    "vel_cd_r2_arr = nans([n_runs, len(lag_axis)])\n",
    "vel_fb_r2_arr = nans([n_runs, len(lag_axis)])\n",
    "\n",
    "bump_r2_runs = nans([n_runs, len(lag_axis)])\n",
    "bump_cd_r2_arr = nans([n_runs, len(lag_axis)])\n",
    "bump_fb_r2_arr = nans([n_runs, len(lag_axis)])\n",
    "\n",
    "cd_lag = -100\n",
    "fb_lag = 50\n",
    "noise_level = 5\n",
    "noise_lv = \"hi\"\n",
    "type = \"oppo\"\n",
    "\n",
    "for b in range(n_runs):\n",
    "    if b == (n_runs-1):\n",
    "        nrn_weight_0 = nrn_weight_0_save\n",
    "        nrn_weight_1 = nrn_weight_1_save\n",
    "    else:\n",
    "        nrn_weight_0 = np.random.normal(mu, sigma, size = (n_nrn,1)) \n",
    "        nrn_weight_1 = np.random.normal(mu, sigma, size = (n_nrn,1))\n",
    "        nrn_weight = orth(np.concatenate([nrn_weight_0, nrn_weight_1], axis = 1))\n",
    "        nrn_weight_0 = nrn_weight[:,0][:,np.newaxis]\n",
    "        nrn_weight_1 = nrn_weight[:,1][:,np.newaxis]\n",
    "\n",
    "    # Random We and Wa\n",
    "    if type == \"orthog\":\n",
    "        nrn_weight = np.concatenate([nrn_weight_0, nrn_weight_1], axis = 1)\n",
    "    # Random We = Wa\n",
    "    elif type == \"align\":\n",
    "        nrn_weight = np.tile(nrn_weight_0,(1,2)) \n",
    "    elif type == \"oppo\":\n",
    "    # Random We = -Wa\n",
    "        nrn_weight = np.concatenate([nrn_weight_0, -nrn_weight_0], axis = 1) \n",
    "    else:\n",
    "        break\n",
    "    \n",
    "    n_bins = len(nrn_axis)\n",
    "    nrn_activity = nans([n_trials, n_bins, n_nrn])\n",
    "    cd_nrn_activity = nans([n_trials, n_bins, n_nrn])\n",
    "    fb_nrn_activity = nans([n_trials, n_bins, n_nrn])\n",
    "    for i in range(n_bins):\n",
    "        cd_signal = np.outer(vel_array[:, i+np.argwhere(kin_axis==nrn_axis[0]+(-cd_lag))[0,0], 0], nrn_weight[:,0])\n",
    "        fb_signal = np.outer(perturbed_vel_array[:, i+np.argwhere(kin_axis==nrn_axis[0]+(-fb_lag))[0,0], 0], nrn_weight[:,1])\n",
    "        cd_nrn_activity[:,i,:] = cd_signal\n",
    "        fb_nrn_activity[:,i,:] = fb_signal\n",
    "        nrn_activity[:,i,:] = cd_signal+fb_signal\n",
    "\n",
    "    nrn_activity_flat = nrn_activity.reshape(-1,n_nrn)\n",
    "    sig_noise = np.zeros((n_nrn,n_nrn))\n",
    "    np.fill_diagonal(sig_noise,1)\n",
    "    noise = np.random.multivariate_normal(np.zeros(n_nrn), sig_noise, nrn_activity_flat.shape[0]) * noise_level\n",
    "    noisy_nrn_activity = (nrn_activity_flat+noise).reshape(nrn_activity.shape)\n",
    "\n",
    "    noise = np.random.multivariate_normal(np.zeros(n_nrn), sig_noise, nrn_activity_flat.shape[0]) * noise_level\n",
    "    cd_nrn_activity_flat = cd_nrn_activity.reshape(-1,n_nrn)\n",
    "    noisy_cd_nrn_activity = (cd_nrn_activity_flat+noise).reshape(cd_nrn_activity.shape)\n",
    "\n",
    "    noise = np.random.multivariate_normal(np.zeros(n_nrn), sig_noise, nrn_activity_flat.shape[0]) * noise_level\n",
    "    fb_nrn_activity_flat = fb_nrn_activity.reshape(-1,n_nrn)\n",
    "    noisy_fb_nrn_activity = (fb_nrn_activity_flat+noise).reshape(fb_nrn_activity.shape)    \n",
    "\n",
    "    # noisy_nrn_activity = noisy_cd_nrn_activity + noisy_fb_nrn_activity\n",
    "\n",
    "    # coefs_arr = nans([len(lag_axis), n_nrn])\n",
    "    r2_arr = nans([len(lag_axis)]); \n",
    "    cd_r2 = nans([len(lag_axis)]); fb_r2 = nans([len(lag_axis)])\n",
    "    y = perturbed_vel_array[:,np.argwhere(kin_axis==trial_range[0])[0,0]:np.argwhere(kin_axis==trial_range[1])[0,0],0]    \n",
    "    y_reshaped = y.reshape(-1,1)\n",
    "    for j in range(len(lag_axis)):\n",
    "        start, end = int(0+j), int(y.shape[1]+j)       \n",
    "        X = noisy_cd_nrn_activity[:,start:end,:]\n",
    "        X = X.reshape((X.shape[0]* X.shape[1]),X.shape[2])\n",
    "        lr =  LinearRegression().fit(X, y_reshaped)\n",
    "        cd_r2[j] = lr.score(X, y_reshaped)\n",
    "        X = noisy_fb_nrn_activity[:,start:end,:]\n",
    "        X = X.reshape((X.shape[0]* X.shape[1]),X.shape[2])\n",
    "        lr =  LinearRegression().fit(X, y_reshaped)\n",
    "        fb_r2[j] = lr.score(X, y_reshaped)\n",
    "        X = noisy_nrn_activity[:,start:end,:]\n",
    "        X = X.reshape((X.shape[0]* X.shape[1]),X.shape[2])\n",
    "        lr =  LinearRegression().fit(X, y_reshaped)\n",
    "        # coefs_arr[j] = lr.coef_\n",
    "        r2_arr[j] = lr.score(X, y_reshaped)\n",
    "    vel_r2_runs[b,:] = r2_arr; \n",
    "    vel_cd_r2_arr[b,:] = cd_r2; vel_fb_r2_arr[b,:] = fb_r2\n",
    "\n",
    "    bump_range = [250,350]\n",
    "    r2_arr = nans([len(lag_axis)]); \n",
    "    cd_r2 = nans([len(lag_axis)]); fb_r2 = nans([len(lag_axis)])    \n",
    "    nrn_bump_range = [bump_range[0]+lag_range[0]]\n",
    "    nrn_add_idx = np.argwhere(nrn_axis==(bump_range[0]+lag_range[0]))[0,0]\n",
    "    y = bump_vel_array[:,np.argwhere(kin_axis==bump_range[0])[0,0]:np.argwhere(kin_axis==bump_range[1])[0,0],0]\n",
    "    y_reshaped = y.reshape(-1,1)\n",
    "    for j in range(len(lag_axis)):\n",
    "        start, end = int(0+nrn_add_idx+j), int(0+nrn_add_idx+y.shape[1]+j)\n",
    "        X = noisy_cd_nrn_activity[:,start:end,:]\n",
    "        X = X.reshape((X.shape[0]* X.shape[1]),X.shape[2])\n",
    "        lr =  LinearRegression().fit(X, y_reshaped)\n",
    "        cd_r2[j] = lr.score(X, y_reshaped)\n",
    "        X = noisy_fb_nrn_activity[:,start:end,:]\n",
    "        X = X.reshape((X.shape[0]* X.shape[1]),X.shape[2])\n",
    "        lr =  LinearRegression().fit(X, y_reshaped)\n",
    "        fb_r2[j] = lr.score(X, y_reshaped)\n",
    "        X = noisy_nrn_activity[:,start:end,:]\n",
    "        X = X.reshape((X.shape[0]* X.shape[1]),X.shape[2])\n",
    "        lr =  LinearRegression().fit(X, y_reshaped)\n",
    "        coefs_arr[j] = lr.coef_\n",
    "        r2_arr[j] = lr.score(X, y_reshaped)\n",
    "    bump_r2_runs[b,:] = r2_arr; \n",
    "    bump_cd_r2_arr[b,:] = cd_r2; bump_fb_r2_arr[b,:] = fb_r2\n",
    "\n",
    "# print(r2_runs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(str(monkey)+\"_sim_perturb_\"+str(type)+\"_\"+str(noise_lv),\\\n",
    "         noisy_cd_nrn_activity = noisy_cd_nrn_activity, noisy_fb_nrn_activity = noisy_fb_nrn_activity, \\\n",
    "         noisy_nrn_activity = noisy_nrn_activity,\\\n",
    "         vel_cd_r2_arr = vel_cd_r2_arr,vel_fb_r2_arr = vel_fb_r2_arr, \\\n",
    "         vel_r2_runs = vel_r2_runs, \\\n",
    "         bump_cd_r2_arr = bump_cd_r2_arr,bump_fb_r2_arr = bump_fb_r2_arr, \\\n",
    "         bump_r2_runs = bump_r2_runs, \\\n",
    "         nrn_weight=nrn_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type = 'oppo'\n",
    "data = np.load(str(monkey)+\"_sim_perturb_\"+str(type)+\"_hi.npz\")\n",
    "print(data.files)\n",
    "\n",
    "vel_cd_r2_arr = data['vel_cd_r2_arr'] \n",
    "vel_fb_r2_arr = data['vel_fb_r2_arr'] \n",
    "vel_r2_runs = data['vel_r2_runs'] \n",
    "\n",
    "bump_cd_r2_arr = data['bump_cd_r2_arr'] \n",
    "bump_fb_r2_arr = data['bump_fb_r2_arr'] \n",
    "bump_r2_runs = data['bump_r2_runs'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(vel_r2_runs))\n",
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "# Compute means and standard deviations\n",
    "mean_r2 = np.mean(vel_r2_runs, axis=0)\n",
    "std_r2 = np.std(vel_r2_runs, axis=0)\n",
    "\n",
    "mean_cd = np.mean(vel_cd_r2_arr, axis=0)\n",
    "std_cd = np.std(vel_cd_r2_arr, axis=0)\n",
    "\n",
    "mean_fb = np.mean(vel_fb_r2_arr, axis=0)\n",
    "std_fb = np.std(vel_fb_r2_arr, axis=0)\n",
    "\n",
    "# Plot means\n",
    "plt.plot(lag_axis, mean_r2, color='brown', label='r2_runs')\n",
    "plt.plot(lag_axis, mean_cd, color='green', label='cd_r2_arr',linestyle='--',alpha=0.5)\n",
    "plt.plot(lag_axis, mean_fb, color='magenta', label='fb_r2_arr',linestyle='--',alpha=0.5)\n",
    "\n",
    "# Fill between mean ± std\n",
    "plt.fill_between(lag_axis, mean_r2 - std_r2, mean_r2 + std_r2, color='grey', alpha=0.3)\n",
    "plt.fill_between(lag_axis, mean_cd - std_cd, mean_cd + std_cd, color='grey', alpha=0.3)\n",
    "plt.fill_between(lag_axis, mean_fb - std_fb, mean_fb + std_fb, color='grey', alpha=0.3)\n",
    "\n",
    "plt.axvline(cd_lag, color = 'k', linestyle='--')\n",
    "plt.axvline(fb_lag, color = 'k', linestyle='--')\n",
    "plt.xlabel(\"Time lag (ms)\")\n",
    "plt.ylabel('R2')\n",
    "plt.ylim([-0.05,1.05])\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + \"_\"+type+\"_sim_perturb_vel_r2_\"+noise_lv+\"_noise.pdf\", dpi = 'figure')\n",
    "plt.show()\n",
    "# plt.hist(ang_runs)\n",
    "# plt.xlabel(\"Angle (deg)\")\n",
    "# plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(bump_r2_runs))\n",
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "# Compute means and standard deviations\n",
    "mean_r2 = np.mean(bump_r2_runs, axis=0)\n",
    "std_r2 = np.std(bump_r2_runs, axis=0)\n",
    "\n",
    "mean_cd = np.mean(bump_cd_r2_arr, axis=0)\n",
    "std_cd = np.std(bump_cd_r2_arr, axis=0)\n",
    "\n",
    "mean_fb = np.mean(bump_fb_r2_arr, axis=0)\n",
    "std_fb = np.std(bump_fb_r2_arr, axis=0)\n",
    "\n",
    "# Plot means\n",
    "plt.plot(lag_axis, mean_r2, color='brown', label='r2_runs')\n",
    "plt.plot(lag_axis, mean_cd, color='green', label='cd_r2_arr',linestyle='--',alpha=0.5)\n",
    "plt.plot(lag_axis, mean_fb, color='magenta', label='fb_r2_arr',linestyle='--',alpha=0.5)\n",
    "\n",
    "# Fill between mean ± std\n",
    "plt.fill_between(lag_axis, mean_r2 - std_r2, mean_r2 + std_r2, color='grey', alpha=0.3)\n",
    "plt.fill_between(lag_axis, mean_cd - std_cd, mean_cd + std_cd, color='grey', alpha=0.3)\n",
    "plt.fill_between(lag_axis, mean_fb - std_fb, mean_fb + std_fb, color='grey', alpha=0.3)\n",
    "\n",
    "plt.axvline(cd_lag, color = 'k', linestyle='--')\n",
    "plt.axvline(fb_lag, color = 'k', linestyle='--')\n",
    "plt.xlabel(\"Time lag (ms)\")\n",
    "plt.ylabel('R2')\n",
    "plt.ylim([-0.05,1.05])\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + \"_\"+type+\"_sim_perturb_bump_r2_\"+noise_lv+\"_noise.pdf\", dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ac2_CD_proj_spikes_smth_40_oneside'].to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_c_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(-300, 300, 10)[np.argmax(avg_c_x1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cross-correlation\n",
    "df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range = (-100,1000), ignored_trials = ~active_mask)\n",
    "n_trials = df['trial_id'].nunique()\n",
    "\n",
    "cd_array = df['ac2_CD_proj_spikes_smth_40_oneside'].to_numpy().reshape(n_trials, -1, 3)\n",
    "\n",
    "# pos_array = df['hand_pos'].to_numpy().reshape(n_trials, -1, 2)\n",
    "# acc_array = df['hand_acc'].to_numpy().reshape(n_trials, -1, 2)\n",
    "vel_array = df['hand_vel'].to_numpy().reshape(n_trials, -1, 2)\n",
    "maxlags = 30 # times binsize is in ms (300ms for best display)\n",
    "X = cd_array\n",
    "Y = vel_array\n",
    "\n",
    "print(X.shape)\n",
    "cc_arr = nans([n_trials, maxlags*2+1])\n",
    "for i in range(n_trials):\n",
    "    x = X[i,:,0]\n",
    "    y = Y[i,:,0]\n",
    "    lags, c = xcorr(x, y, maxlags)\n",
    "    cc_arr[i,:] = c\n",
    "avg_c_x = np.mean(cc_arr, axis=0)\n",
    "\n",
    "cc_arr = nans([n_trials, maxlags*2+1])\n",
    "for i in range(n_trials):\n",
    "    x = X[i,:,1]\n",
    "    y = Y[i,:,0]\n",
    "    lags, c = xcorr(x, y, maxlags)\n",
    "    cc_arr[i,:] = c\n",
    "avg_c_x1 = np.mean(cc_arr, axis=0)\n",
    "\n",
    "\n",
    "cc_arr = nans([n_trials, maxlags*2+1])\n",
    "for i in range(n_trials):\n",
    "    x = X[i,:,2]\n",
    "    y = Y[i,:,1]\n",
    "    lags, c = xcorr(x, y, maxlags)\n",
    "    cc_arr[i,:] = c\n",
    "avg_c_y = np.mean(cc_arr, axis=0)\n",
    "\n",
    "x_axis = lags*dataset_10ms.bin_width\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_axis, avg_c_x, color = 'green', label = 'x1')\n",
    "ax.plot(x_axis, avg_c_x1, color = 'green', label = 'x2')\n",
    "\n",
    "ax.plot(x_axis, avg_c_y, color = 'blue', label = 'y')\n",
    "tmax = x_axis[int(np.mean((np.argmax(abs(avg_c_x)),np.argmax(abs(avg_c_y)))))]\n",
    "# ax.axvline(tmax, color = 'k',linestyle = '--')\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "print(tmax)\n",
    "plt.legend()\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('Normalized cross-corr')\n",
    "plt.title('CD dims vs. hand vel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Autocorrelation\n",
    "df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range = (-100,1000), ignored_trials = ~active_mask)\n",
    "n_trials = df['trial_id'].nunique()\n",
    "\n",
    "vel_array = df['hand_vel'].to_numpy().reshape(n_trials, -1, 2)\n",
    "maxTimeLag = 500 #in ms\n",
    "X = vel_array\n",
    "\n",
    "print(X.shape)\n",
    "binSize = dataset_10ms.bin_width\n",
    "numBin = X.shape[1]\n",
    "x1 = X[:,:,0]\n",
    "x2 = X[:,:,0]\n",
    "ac_x = comp_cc(x1,x2,maxTimeLag,binSize,numBin)\n",
    "\n",
    "x1 = X[:,:,1]\n",
    "x2 = X[:,:,1]\n",
    "ac_y = comp_cc(x1,x2,maxTimeLag,binSize,numBin)\n",
    "\n",
    "time_axis = np.arange(0, maxTimeLag, binSize)\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.plot(time_axis,ac_x/ac_x[0],color = 'green', label = 'x')\n",
    "ax.plot(time_axis,ac_y/ac_y[0],color = 'blue', label = 'y')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time lag (ms)\")\n",
    "plt.ylabel(\"Normalized autocorrelation\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_autocorrelation_vel.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dir = np.array([0,45,90,135,180,225,270,315]) \n",
    "directions = np.array([0,45,90,135,180,225,270,315])\n",
    "\n",
    "# plot_dir = np.array([0,90,180,270]) \n",
    "# directions = np.array([0,90,180,270])\n",
    "\n",
    "cmap = plt.get_cmap('coolwarm',len(plot_dir))\n",
    "custom_palette = [mpl.colors.rgb2hex(cmap(i)) for i in range(len(plot_dir))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA projections over trial, for different reaching directions\n",
    "plot_field = 'SCA_40'\n",
    "n_dims = dataset.data[plot_field].shape[1]\n",
    "# order = np.arange(n_dims)\n",
    "order = ssa_order_smth40\n",
    "\n",
    "pred_range = (-100, 1000)\n",
    "trial_mask = active_mask\n",
    "cond_dict = active_cond_dict\n",
    "n_timepoints = int((pred_range[1] - pred_range[0])/dataset_10ms.bin_width)\n",
    "data = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=pred_range, ignored_trials=~trial_mask)\n",
    "n_trials = data['trial_id'].nunique()\n",
    "trials_pca = nans([n_trials,n_timepoints,n_dims])\n",
    "i = 0\n",
    "for idx, trial in data.groupby('trial_id'):\n",
    "    trials_pca[i,:,:]=trial[plot_field].to_numpy()\n",
    "    i+=1\n",
    "print(trials_pca.shape)\n",
    "\n",
    "x_axis = np.arange(pred_range[0], pred_range[1], dataset_10ms.bin_width)\n",
    "# define some useful time points\n",
    "move_idx=0\n",
    "ret_idx = 500\n",
    "\n",
    "plot_dims = 10\n",
    "\n",
    "fig,ax=plt.subplots(plot_dims,1,figsize=(10,15))\n",
    "for i in range(plot_dims):\n",
    "    for j in range(len(plot_dir)):\n",
    "        color = custom_palette[j]\n",
    "        dir_idx = np.argwhere(directions == plot_dir[j])[0]\n",
    "        cond_mean_proj = np.mean(trials_pca[np.argwhere(cond_dict==dir_idx).flatten(),:,:], axis = 0)[:,order[i]] \n",
    "        pca_mean = np.mean(data[plot_field].to_numpy(),axis = 0)[order[i]] \n",
    "        ax[i].plot(x_axis,cond_mean_proj - pca_mean,linewidth=2.25,color = color,label = plot_dir[j])\n",
    "        \n",
    "        ax[i].axvline(move_idx, color='k',linewidth = .5)\n",
    "        ax[i].axvline(ret_idx, color='k',linewidth = .5)\n",
    "        ax[i].set_xlim([-100,1000])\n",
    "        # ax[i].set_ylim([-15, 15])\n",
    "        ax[i].axhline(0,color ='k',ls = '--')\n",
    "        if i<plot_dims-1:\n",
    "            ax[i].set_xticks([])\n",
    "        else:\n",
    "            ax[i].set_xlabel('Time after movement onset (ms)')\n",
    "            \n",
    "        # ax[i].set_yticks([])\n",
    "        ax[i].set_ylabel('Dim. '+str(i+1))\n",
    "\n",
    "    ax[0].set_title('Active trials ' + plot_field)\n",
    "    \n",
    "plt.legend(bbox_to_anchor = (1, 1), loc = 'upper left')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_PCA_active.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA projections over trial, for different reaching directions\n",
    "order = np.arange(n_dims)\n",
    "# order = ssa_order_smth40\n",
    "\n",
    "pred_range = (-100, 500)\n",
    "trial_mask = passive_mask\n",
    "cond_dict = passive_cond_dict\n",
    "n_timepoints = int((pred_range[1] - pred_range[0])/dataset_10ms.bin_width)\n",
    "# n_trials = dataset_10ms.trial_info.loc[trial_mask].shape[0]\n",
    "data = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=pred_range, ignored_trials=~trial_mask)\n",
    "n_trials = data['trial_id'].nunique()\n",
    "trials_pca = nans([n_trials,n_timepoints,n_dims])\n",
    "i = 0\n",
    "for idx, trial in data.groupby('trial_id'):\n",
    "    trials_pca[i,:,:]=trial[plot_field].to_numpy()\n",
    "    i+=1\n",
    "print(trials_pca.shape)\n",
    "\n",
    "x_axis = np.arange(pred_range[0], pred_range[1], dataset_10ms.bin_width)\n",
    "# define some useful time points\n",
    "move_idx=0\n",
    "ret_idx = 200\n",
    "\n",
    "plot_dims = 10\n",
    "\n",
    "fig,ax=plt.subplots(plot_dims,1,figsize=(10,15))\n",
    "for i in range(plot_dims):\n",
    "    for j in range(len(plot_dir)):\n",
    "        color = custom_palette[j]\n",
    "        dir_idx = np.argwhere(directions == plot_dir[j])[0]\n",
    "        cond_mean_proj = np.mean(trials_pca[np.argwhere(cond_dict==dir_idx).flatten(),:,:], axis = 0)[:,order[i]] \n",
    "        pca_mean = np.mean(data[plot_field].to_numpy(),axis = 0)[order[i]]\n",
    "        ax[i].plot(x_axis,cond_mean_proj - pca_mean,linewidth=2.25,color = color,label = plot_dir[j])\n",
    "        \n",
    "        ax[i].axvline(move_idx, color='k',linewidth = .5)\n",
    "        ax[i].axvline(ret_idx, color='k',linewidth = .5)\n",
    "        ax[i].set_xlim([-100,500])\n",
    "        # ax[i].set_ylim([-15, 15])\n",
    "        ax[i].axhline(0,color ='k',ls = '--')\n",
    "        if i<plot_dims-1:\n",
    "            ax[i].set_xticks([])\n",
    "        else:\n",
    "            ax[i].set_xlabel('Time after movement onset (ms)')\n",
    "            \n",
    "        # ax[i].set_yticks([])\n",
    "        ax[i].set_ylabel('Dim. '+str(i+1))\n",
    "\n",
    "    ax[0].set_title('Passive trials ' + plot_field)\n",
    "    \n",
    "plt.legend(bbox_to_anchor = (1, 1), loc = 'upper left')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_PCA_passive.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Get a specific neuron's original index from GLM results to plot\n",
    "# with np.load(monkey+'_hf_neuron_filter.npz') as data:\n",
    "#     neuron_filter = data['neuron_filter']\n",
    "# fr_filtered_idx = np.argwhere(neuron_filter==1).flatten()\n",
    "# index_in_glm = [18, 27, 34, 35, 42, 48]\n",
    "\n",
    "# index_original = fr_filtered_idx[index_in_glm]\n",
    "# print(index_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dir = np.array([0,45,90,135,180,225,270,315]) \n",
    "directions = np.array([0,45,90,135,180,225,270,315])\n",
    "\n",
    "# plot_dir = np.array([0,90,180,270]) \n",
    "# directions = np.array([0,90,180,270])\n",
    "\n",
    "cmap = plt.get_cmap('coolwarm',len(plot_dir))\n",
    "custom_palette = [mpl.colors.rgb2hex(cmap(i)) for i in range(len(plot_dir))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot single neuron activity over trial, for different reaching directions\n",
    "dataset = dataset_10ms\n",
    "nrn_idx = 68\n",
    "pred_range = (-300, 1000)\n",
    "trial_mask = active_mask\n",
    "cond_dict = active_cond_dict\n",
    "n_timepoints = int((pred_range[1] - pred_range[0])/dataset.bin_width)\n",
    "n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "data = dataset.make_trial_data(align_field='move_onset_time', align_range=pred_range, ignored_trials=~trial_mask)\n",
    "trials_activity = nans([n_trials,n_timepoints])\n",
    "i = 0\n",
    "for idx, trial in data.groupby('trial_id'):\n",
    "    trials_activity[i,:]=trial.spikes_smth_40.to_numpy()[:,nrn_idx]\n",
    "    i+=1\n",
    "print(trials_activity.shape)\n",
    "\n",
    "plot_dir = np.array([0,45,90,135,180,225,270,315]) \n",
    "directions = np.array([0,45,90,135,180,225,270,315])\n",
    "# plot_dir = np.array([0,90,180,270]) \n",
    "# directions = np.array([0,90,180,270]) \n",
    "x_axis = np.arange(pred_range[0], pred_range[1], dataset.bin_width)\n",
    "\n",
    "# define some useful time points\n",
    "move_idx=0\n",
    "ret_idx = 500\n",
    "\n",
    "fig,ax=plt.subplots(1,1,figsize=(8,2))\n",
    "for j in range(len(plot_dir)):\n",
    "    color = custom_palette[j]\n",
    "    dir_idx = np.argwhere(directions == plot_dir[j])[0]\n",
    "    cond_mean_proj = np.mean(trials_activity[np.argwhere(cond_dict==dir_idx).flatten(),:], axis = 0)\n",
    "    ax.plot(x_axis,cond_mean_proj/dataset.bin_width*1000,linewidth=2.25,color = color,label = plot_dir[j])\n",
    "\n",
    "ax.axvline(move_idx,color='k')\n",
    "ax.axvline(ret_idx,color='k')\n",
    "\n",
    "ax.set_xlabel('Time after movement onset (ms)')\n",
    "ax.set_ylabel('Firing rate')\n",
    "ax.set_title('Active trials')\n",
    "\n",
    "plt.legend(bbox_to_anchor = (1, 1), loc = 'upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_example4_mix_nrn_active.pdf',dpi = 'figure')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot single neuron activity over trial, for different reaching directions\n",
    "dataset = dataset_10ms\n",
    "pred_range = (-100, 500)\n",
    "trial_mask = passive_mask\n",
    "cond_dict = passive_cond_dict\n",
    "n_timepoints = int((pred_range[1] - pred_range[0])/dataset.bin_width)\n",
    "n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "data = dataset.make_trial_data(align_field='move_onset_time', align_range=pred_range, ignored_trials=~trial_mask)\n",
    "trials_activity = nans([n_trials,n_timepoints])\n",
    "i = 0\n",
    "for idx, trial in data.groupby('trial_id'):\n",
    "    trials_activity[i,:]=trial.spikes_smth_40.to_numpy()[:,nrn_idx]\n",
    "    i+=1\n",
    "print(trials_activity.shape)\n",
    "\n",
    "plot_dir = np.array([0,45,90,135,180,225,270,315]) \n",
    "directions = np.array([0,45,90,135,180,225,270,315])\n",
    "# plot_dir = np.array([0,90,180,270]) \n",
    "# directions = np.array([0,90,180,270]) \n",
    "x_axis = np.arange(pred_range[0], pred_range[1], dataset.bin_width)\n",
    "\n",
    "# define some useful time points\n",
    "move_idx=0\n",
    "ret_idx = 200\n",
    "\n",
    "fig,ax=plt.subplots(1,1,figsize=(8,2))\n",
    "for j in range(len(plot_dir)):\n",
    "    color = custom_palette[j]\n",
    "    dir_idx = np.argwhere(directions == plot_dir[j])[0]\n",
    "    cond_mean_proj = np.mean(trials_activity[np.argwhere(cond_dict==dir_idx).flatten(),:], axis = 0)\n",
    "    ax.plot(x_axis,cond_mean_proj/dataset.bin_width*1000,linewidth=2.25,color = color,label = plot_dir[j])\n",
    "\n",
    "ax.axvline(move_idx,color='k')\n",
    "ax.axvline(ret_idx,color='k')\n",
    "\n",
    "ax.set_xlabel('Time after movement onset (ms)')\n",
    "ax.set_ylabel('Firing rate')\n",
    "ax.set_title('Passive trials')\n",
    "\n",
    "plt.legend(bbox_to_anchor = (1, 1), loc = 'upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_example4_mix_nrn_passive.pdf',dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_range = (-1000,1000)\n",
    "x_axis = np.arange(plot_range[0], plot_range[1], dataset_10ms.bin_width)\n",
    "# active_df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=plot_range, ignored_trials=~active_mask, allow_overlap=True)\n",
    "# passive_df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=plot_range, ignored_trials=~passive_mask, allow_overlap=True)\n",
    "nan_df = dataset_10ms.make_trial_data(align_field='bump_time', align_range=plot_range, ignored_trials=~nan_mask, allow_overlap=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed = np.sqrt(np.sum(dataset_10ms.data['hand_vel'][:].T**2,axis=0)).to_numpy().reshape((-1,1))\n",
    "# dataset_10ms.add_continuous_data(speed,'speed')\n",
    "# acceleration = np.diff(speed, axis = 0, prepend=[speed[0]])\n",
    "# dataset_10ms.add_continuous_data(acceleration,'acceleration') #technically change of speed, for timing plots\n",
    "# jerk = np.diff(acceleration, axis = 0, prepend=[acceleration[0]])\n",
    "# dataset_10ms.add_continuous_data(jerk,'jerk') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = active_df\n",
    "# df = passive_df\n",
    "df = nan_df\n",
    "var = 'hand_vel'\n",
    "plot_dir = [0.0, 90.0, 180.0, 270.0] \n",
    "# plot_dir = [45.0, 135.0, 225.0, 315.0] \n",
    "plot_dim = 0 # plot x velocity\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "fig, ax = plt.subplots(figsize=(10,2))\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir%360 == trial_dir].trial_id\n",
    "    for _, trial in df[np.isin(df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        # plt.plot(x_axis, trial[var][plot_dim], color=color, linewidth=0.5)\n",
    "        plt.plot(x_axis, np.array(trial[var])[:,plot_dim], color=color, linewidth=0.5)\n",
    "# plt.xlim([-200, 200])\n",
    "plt.ylim([-50, 50])\n",
    "plt.axvline([0],color='k',linestyle='--')\n",
    "# plt.axhline([10])\n",
    "plt.xlabel('Time after bump onset (ms)')\n",
    "plt.ylabel('Hand velocity (cm/s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dir = [0.0, 90.0, 180.0, 270.0] \n",
    "# plot_dir = [45.0, 135.0, 225.0, 315.0] \n",
    "plot_dim = 1 # plot x velocity\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "fig, ax = plt.subplots(figsize=(10,2))\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir%360 == trial_dir].trial_id\n",
    "    for _, trial in df[np.isin(df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        # plt.plot(x_axis, trial[var][plot_dim], color=color, linewidth=0.5)\n",
    "        plt.plot(x_axis, np.array(trial[var])[:,plot_dim], color=color, linewidth=0.5)\n",
    "# plt.xlim([-200, 200])\n",
    "plt.ylim([-50, 50])\n",
    "plt.axvline([0],color='k',linestyle='--')\n",
    "# plt.axhline([.5])\n",
    "plt.xlabel('Time after bump onset (ms)')\n",
    "plt.ylabel('Hand velocity (cm/s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_range = (-2000,500)\n",
    "x_axis = np.arange(plot_range[0], plot_range[1], dataset_10ms.bin_width)\n",
    "active_df = dataset_10ms.make_trial_data(align_field='move_offset_time', align_range=plot_range, ignored_trials=~active_mask, allow_overlap=True)\n",
    "passive_df = dataset_10ms.make_trial_data(align_field='move_offset_time', align_range=plot_range, ignored_trials=~passive_mask, allow_overlap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = active_df\n",
    "# df = passive_df\n",
    "var = 'FB_proj'\n",
    "plot_dir = [0.0, 90.0, 180.0, 270.0] \n",
    "# plot_dir = [45.0, 135.0, 225.0, 315.0] \n",
    "plot_dim = 0 # plot x velocity\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "fig, ax = plt.subplots(figsize=(10,2))\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir%360 == trial_dir].trial_id\n",
    "    for _, trial in df[np.isin(df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        # plt.plot(x_axis, trial[var][plot_dim], color=color, linewidth=0.5)\n",
    "        plt.plot(x_axis, np.array(trial[var])[:,plot_dim], color=color, linewidth=0.5)\n",
    "# plt.xlim([-500, 2000])\n",
    "# plt.ylim([-50, 50])\n",
    "plt.axvline([0],color='k',linestyle='--')\n",
    "# plt.axhline([.5])\n",
    "plt.xlabel('Time before movement offset (ms)')\n",
    "plt.ylabel('Hand velocity (cm/s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dir = [0.0, 90.0, 180.0, 270.0] \n",
    "# plot_dir = [45.0, 135.0, 225.0, 315.0] \n",
    "plot_dim = 4 # plot x velocity\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "fig, ax = plt.subplots(figsize=(10,2))\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir%360 == trial_dir].trial_id\n",
    "    for _, trial in df[np.isin(df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        # plt.plot(x_axis, trial[var][plot_dim], color=color, linewidth=0.5)\n",
    "        plt.plot(x_axis, np.array(trial[var])[:,plot_dim], color=color, linewidth=0.5)\n",
    "# plt.ylim([-50, 50])\n",
    "plt.axvline([0],color='k',linestyle='--')\n",
    "# plt.axhline([.5])\n",
    "plt.xlabel('Time before movement offset (ms)')\n",
    "plt.ylabel('Hand velocity (cm/s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = active_df\n",
    "# # df = passive_df\n",
    "# var = 'acceleration'\n",
    "# # plot_dir = [0.0, 90.0, 180.0, 270.0] \n",
    "# plot_dir = [45.0, 135.0, 225.0, 315.0] \n",
    "# colors = ['red', 'blue', 'green', 'orange']\n",
    "# # fig, ax = plt.subplots(figsize=(10,2))\n",
    "\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# for trial_dir, color in zip(plot_dir, colors):\n",
    "#     cond_ids = dataset.trial_info[dataset.trial_info.cond_dir%360 == trial_dir].trial_id\n",
    "#     for _, trial in df[np.isin(df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "#         plt.plot(x_axis, trial[var], color=color, linewidth=0.5)\n",
    "# plt.xlim([-200, 200])\n",
    "# # plt.ylim([-50, 50])\n",
    "# plt.axvline([0],color='k',linestyle='--')\n",
    "# # plt.axhline([5])\n",
    "# plt.xlabel('Time after movement onset (ms)')\n",
    "# plt.ylabel('Speed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df = active_df\n",
    "# df = passive_df\n",
    "# var = 'acceleration'\n",
    "# plot_dir = [0.0, 90.0, 180.0, 270.0] \n",
    "# # plot_dir = [45.0, 135.0, 225.0, 315.0] \n",
    "# colors = ['red', 'blue', 'green', 'orange']\n",
    "# # fig, ax = plt.subplots(figsize=(10,2))\n",
    "\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# for trial_dir, color in zip(plot_dir, colors):\n",
    "#     cond_ids = dataset.trial_info[dataset.trial_info.cond_dir%360 == trial_dir].trial_id\n",
    "#     for _, trial in df[np.isin(df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "#         plt.plot(x_axis, trial[var], color=color, linewidth=0.5)\n",
    "# plt.xlim([-200, 200])\n",
    "# # plt.ylim([-50, 50])\n",
    "# plt.axvline([0],color='k',linestyle='--')\n",
    "# plt.axhline([1])\n",
    "# plt.xlabel('Time after movement onset (ms)')\n",
    "# plt.ylabel('Speed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.trial_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial length\n",
    "dt = (dataset.trial_info.go_cue_time - dataset.trial_info.start_time).dt.total_seconds()*1000\n",
    "dt[active_mask]\n",
    "plt.hist(dt[active_mask])\n",
    "# plt.xlim([-100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_range = (-500,2000)\n",
    "x_axis = np.arange(plot_range[0], plot_range[1], dataset_10ms.bin_width)\n",
    "active_df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=plot_range, ignored_trials=~active_mask, allow_overlap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PSTH\n",
    "active_trials_spikes = []\n",
    "for _, trial in active_df.groupby('trial_id'):\n",
    "    active_trials_spikes.append(np.sum(trial.spikes,axis=1))\n",
    "fig, ax = plt.subplots(figsize=(10,3))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.plot(x_axis,np.sum(active_trials_spikes,axis = 0)/dataset_10ms.bin_width*1000/len(active_trials_spikes)/n_neurons,\"-o\",markersize=5, color = 'k',label = 'Active')\n",
    "ax.set_ylabel('Average FR (sp/s)')\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "plt.axvline(0, color = 'k',linestyle = '--')    \n",
    "\n",
    "plt.xlabel('Time after move onset (ms)')\n",
    "plt.axvline(0, color = 'k',linestyle = '--')    \n",
    "plt.xlim([-500, 2000])\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_range = (-2000,500)\n",
    "x_axis = np.arange(plot_range[0], plot_range[1], dataset_10ms.bin_width)\n",
    "active_df = dataset_10ms.make_trial_data(align_field='move_offset_time', align_range=plot_range, ignored_trials=~active_mask, allow_overlap=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PSTH\n",
    "active_trials_spikes = []\n",
    "for _, trial in active_df.groupby('trial_id'):\n",
    "    active_trials_spikes.append(np.sum(trial.spikes,axis=1))\n",
    "# passive_trials_spikes = []\n",
    "# for _, trial in passive_df.groupby('trial_id'):\n",
    "#     passive_trials_spikes.append(np.sum(trial.spikes,axis=1))\n",
    "fig, ax = plt.subplots(figsize=(10,3))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.plot(x_axis,np.sum(active_trials_spikes,axis = 0)/dataset_10ms.bin_width*1000/len(active_trials_spikes)/n_neurons,\"-o\",markersize=5, color = 'k',label = 'Active')\n",
    "\n",
    "ax.set_ylabel('Average FR (sp/s)')\n",
    "plt.xlabel('Time after movement offset (ms)')\n",
    "plt.axvline(0, color = 'k',linestyle = '--')    \n",
    "\n",
    "\n",
    "plt.xlabel('Time after move offset (ms)')\n",
    "plt.axvline(0, color = 'k',linestyle = '--')    \n",
    "plt.xlim([-2000, 500])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_psth.pdf',dpi = 'figure')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_range = (-300,600)\n",
    "x_axis = np.arange(plot_range[0], plot_range[1], dataset_10ms.bin_width)\n",
    "active_df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=plot_range, ignored_trials=~active_mask, allow_overlap=True)\n",
    "passive_df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=plot_range, ignored_trials=~passive_mask, allow_overlap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PSTH\n",
    "active_trials_spikes = []\n",
    "for _, trial in active_df.groupby('trial_id'):\n",
    "    active_trials_spikes.append(np.sum(trial.spikes,axis=1))\n",
    "passive_trials_spikes = []\n",
    "for _, trial in passive_df.groupby('trial_id'):\n",
    "    passive_trials_spikes.append(np.sum(trial.spikes,axis=1))\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.plot(x_axis,np.sum(active_trials_spikes,axis = 0)/dataset_10ms.bin_width*1000/len(active_trials_spikes)/n_neurons,\"-o\",markersize=5, color = 'k',label = 'Active')\n",
    "ax.plot(x_axis,np.sum(passive_trials_spikes,axis = 0)/dataset_10ms.bin_width*1000/len(passive_trials_spikes)/n_neurons,\"-o\",markersize=5, color = 'red',label = 'Passive')\n",
    "# plt.title('Peristimulus aligned to move_onset')\n",
    "# plt.legend()\n",
    "ax.set_ylabel('Average Firing Rate (sp/s)')\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "plt.axvline(0, color = 'k',linestyle = '--')    \n",
    "\n",
    "\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "plt.axvline(0, color = 'k',linestyle = '--')    \n",
    "plt.xlim([-300, 600])\n",
    "plt.ylim([6.5, 13.2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_psth.pdf',dpi = 'figure')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Kinematics\n",
    "# var = 'speed'\n",
    "# fig, ax = plt.subplots(figsize=(10,6))\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# count = 0\n",
    "# for _, trial in active_df.groupby('trial_id'):\n",
    "#     if len(trial[var]) == len(x_axis):\n",
    "#         plt.plot(x_axis,trial[var], color='k', linewidth=0.5)\n",
    "#         count+=1\n",
    "# print(count,'active trials')\n",
    "# # count = 0\n",
    "# # for _, trial in passive_df.groupby('trial_id'):\n",
    "# #     if len(trial[var]) == len(x_axis):\n",
    "# #         plt.plot(x_axis, trial[var], color='red', linewidth=0.5)\n",
    "# #         count+=1\n",
    "# # print(count,'passive trials')\n",
    "\n",
    "# plt.xlabel('Time after movement onset (ms)')\n",
    "# plt.ylabel('Hand speed (cm/s)')\n",
    "# plt.axvline(0, color = 'k',linestyle = '--')\n",
    "# # plt.title('Speed aligned to move_onset')\n",
    "# # plt.axvline(120, color = 'k',linestyle = '--')\n",
    "# plt.xlim([-2000,500])\n",
    "# # plt.ylim([-3,100])\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(figDir + monkey + '_speed_whole.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CD/FB subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_10ms\n",
    "# x_field = 'PCA_40'\n",
    "x_field = 'spikes'\n",
    "# x_field = 'proj_out'\n",
    "dim = dataset.data[x_field].shape[1]\n",
    "\n",
    "align_range = (-100, 1500)\n",
    "align_field = 'move_onset_time'\n",
    "mask = active_mask\n",
    "active_df = dataset.make_trial_data(align_field=align_field, align_range=align_range, ignored_trials=~mask)\n",
    "n_trials = active_n_trials\n",
    "X = active_df[x_field].to_numpy().reshape((-1, dim))\n",
    "print(X.shape)\n",
    "mean = np.nanmean(X,axis=0)\n",
    "std = np.nanstd(X,axis=0)\n",
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find CD axes\n",
    "align_range = (-100,0)\n",
    "align_field = 'move_onset_time'\n",
    "\n",
    "mask = active_mask\n",
    "active_df = dataset.make_trial_data(align_field=align_field, align_range=align_range, ignored_trials=~mask)\n",
    "cond_dict = active_cond_dict\n",
    "n_trials = active_n_trials\n",
    "X = active_df[x_field].to_numpy().reshape((-1, dim))\n",
    "X = (X - mean)/std\n",
    "act_trial_spikes = X.reshape((n_trials, -1, dim))\n",
    "print(act_trial_spikes.shape)\n",
    "act_trial_mean_activity = np.mean(act_trial_spikes, axis=1)\n",
    "\n",
    "dirs = [dataset.trial_info[dataset.trial_info.trial_id == i]['cond_dir'] for i in active_df.trial_id]\n",
    "cos_x = np.array([round(math.cos(math.radians(i)),3) for i in dirs])\n",
    "sin_y = np.array([round(math.sin(math.radians(i)),3) for i in dirs])\n",
    "cos_sin = np.array([cos_x, sin_y]).T\n",
    "print(cos_sin.shape)\n",
    "\n",
    "# dirs = [dataset.trial_info[dataset.trial_info.trial_id == i]['cond_dir'] for i in active_df.trial_id]\n",
    "# filter = np.array([x%90==0 for x in dirs])\n",
    "# n_trials = len(filter)\n",
    "# act_trial_mean_activity = act_trial_mean_activity[filter[0::act_trial_spikes.shape[1]].squeeze(),:]\n",
    "# print(act_trial_mean_activity.shape)\n",
    "\n",
    "# cos_x = np.array([round(math.cos(math.radians(i)),3) for i in dirs])[filter.squeeze()]\n",
    "# sin_y = np.array([round(math.sin(math.radians(i)),3) for i in dirs])[filter.squeeze()]\n",
    "# cos_sin = np.array([cos_x, sin_y]).T\n",
    "# print(cos_sin.shape)\n",
    "\n",
    "# n_trials = act_trial_mean_activity.shape[0]\n",
    "# cond_dict = cond_dict[filter[0::act_trial_spikes.shape[1]].squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_proj_matrix_sparse(A,reg=1e-10):\n",
    "#     if A.ndim == 1:\n",
    "#         A = A.reshape(-1, 1)\n",
    "#     return A @ np.linalg.pinv(A.T @ A + reg * np.eye(A.shape[1])) @ A.T\n",
    "# def calc_proj_sparse(R, w,reg=1e-10):\n",
    "#     \"\"\" Returns projection of R(ates) onto the space defined by w \"\"\"\n",
    "#     P = calc_proj_matrix_sparse(w,reg=reg)\n",
    "#     return P@R.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "X = act_trial_mean_activity\n",
    "\n",
    "y=np.array(cos_x).reshape((n_trials,-1,1))[:,0,:]\n",
    "X_proc = X\n",
    "axes_list_x = nans([N,y.shape[1],dim])\n",
    "r2_cv_list_x = nans([N])\n",
    "r2_list_x = nans([N])\n",
    "for i in range(N):\n",
    "    reg = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    # reg = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    axes_list_x[i,:,:] = reg.best_estimator_.coef_\n",
    "    r2_list_x[i] = reg.best_estimator_.score(X_proc, y)\n",
    "    weights = reg.best_estimator_.coef_\n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True,random_state = 42)   \n",
    "    true_concat = nans([n_trials,1])\n",
    "    pred_concat = nans([n_trials,1])\n",
    "    trial_save_idx = 0\n",
    "    for training_set, test_set in skf.split(range(0,n_trials),cond_dict):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test = X_proc[training_set,:],X_proc[test_set,:]\n",
    "        y_train, y_test = y[training_set],y[test_set]\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        # lr = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_test_predicted = lr.predict(X_test)\n",
    "\n",
    "        n = y_test_predicted.shape[0]\n",
    "        true_concat[trial_save_idx:trial_save_idx+n] = y_test\n",
    "        pred_concat[trial_save_idx:trial_save_idx+n] = y_test_predicted.reshape(len(y_test_predicted),1)\n",
    "        trial_save_idx += n\n",
    "    sses =get_sses_pred(true_concat,pred_concat)\n",
    "    sses_mean=get_sses_mean(true_concat)\n",
    "    R2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "    r2_cv_list_x[i] = R2\n",
    "    X_proc = X_proc - calc_proj(X_proc,weights.T).T\n",
    "    # X_proc = X_proc - calc_proj_sparse(X_proc,weights.T).T\n",
    "\n",
    "X_proc = X\n",
    "y = np.array(sin_y).reshape((n_trials,-1,1))[:,0,:]\n",
    "axes_list_y = nans([N,y.shape[1],dim])\n",
    "r2_cv_list_y = nans([N])\n",
    "r2_list_y = nans([N])\n",
    "for i in range(N):\n",
    "    reg = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    # reg = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    axes_list_y[i,:,:] = reg.best_estimator_.coef_\n",
    "    r2_list_y[i] = reg.best_estimator_.score(X_proc, y)\n",
    "    weights = reg.best_estimator_.coef_\n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True,random_state = 42)   \n",
    "    true_concat = nans([n_trials,1])\n",
    "    pred_concat = nans([n_trials,1])\n",
    "    trial_save_idx = 0\n",
    "    for training_set, test_set in skf.split(range(0,n_trials),cond_dict):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test = X_proc[training_set,:],X_proc[test_set,:]\n",
    "        y_train, y_test = y[training_set],y[test_set]\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        # lr = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_test_predicted = lr.predict(X_test)\n",
    "\n",
    "        n = y_test_predicted.shape[0]\n",
    "        true_concat[trial_save_idx:trial_save_idx+n] = y_test\n",
    "        pred_concat[trial_save_idx:trial_save_idx+n] = y_test_predicted.reshape(len(y_test_predicted),1)\n",
    "        trial_save_idx += n\n",
    "    sses =get_sses_pred(true_concat,pred_concat)\n",
    "    sses_mean=get_sses_mean(true_concat)\n",
    "    R2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "    r2_cv_list_y[i] = R2\n",
    "    X_proc = X_proc - calc_proj(X_proc,weights.T).T\n",
    "    # X_proc = X_proc - calc_proj_sparse(X_proc,weights.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(N)+1,r2_cv_list_x)\n",
    "plt.xlabel('N')\n",
    "plt.title('x-dir')\n",
    "plt.ylabel('R2')\n",
    "print(r2_cv_list_x)\n",
    "plt.show()\n",
    "plt.plot(np.arange(N)+1,r2_cv_list_y)\n",
    "plt.xlabel('N')\n",
    "plt.title('y-dir')\n",
    "plt.ylabel('R2')\n",
    "print(r2_cv_list_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.0\n",
    "print(axes_list_x.squeeze()[r2_cv_list_x>thresh,:].shape)\n",
    "print(axes_list_y.squeeze()[r2_cv_list_y>thresh,:].shape)\n",
    "CD_axes = np.vstack((axes_list_x.squeeze()[r2_cv_list_x>thresh,:],axes_list_y.squeeze()[r2_cv_list_y>thresh,:]))\n",
    "CD_axes.shape\n",
    "\n",
    "all_data = dataset_10ms.data[x_field].to_numpy()\n",
    "proj_data = all_data @ CD_axes.T\n",
    "print(proj_data.shape)\n",
    "\n",
    "dataset_10ms.add_continuous_data(proj_data,'v6_alt_zscore_unsmoothed100_CD_proj_'+x_field)\n",
    "\n",
    "# reg = 'Lasso'\n",
    "# dataset_10ms.add_continuous_data(proj_data,'CD_proj_'+x_field+reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find FB axes\n",
    "# proj_out_CD = True\n",
    "# align_range = (200, 400)\n",
    "# align_field = 'move_onset_time'\n",
    "# cond_dict = active_cond_dict\n",
    "\n",
    "proj_out_CD = False\n",
    "align_range = (-100, 0)\n",
    "align_field = 'move_offset_time'\n",
    "cond_dict = active_cond_dict_offset\n",
    "\n",
    "mask = active_mask\n",
    "active_df = dataset.make_trial_data(align_field=align_field, align_range=align_range, ignored_trials=~mask)\n",
    "n_trials = active_df['trial_id'].nunique()\n",
    "X = active_df[x_field].to_numpy().reshape((-1, dim))\n",
    "X = (X - mean)/std\n",
    "act_trial_spikes = X.reshape((n_trials, -1, dim))\n",
    "print(act_trial_spikes.shape)\n",
    "act_trial_mean_activity = np.mean(act_trial_spikes, axis=1)\n",
    "\n",
    "dirs = [dataset.trial_info[dataset.trial_info.trial_id == i]['cond_dir'] for i in active_df.trial_id]\n",
    "cos_x = [round(math.cos(math.radians(i)),3) for i in dirs]\n",
    "sin_y = [round(math.sin(math.radians(i)),3) for i in dirs]\n",
    "cos_sin = np.array([cos_x, sin_y]).T\n",
    "print(cos_sin.shape)\n",
    "\n",
    "# dirs = [dataset.trial_info[dataset.trial_info.trial_id == i]['cond_dir'] for i in active_df.trial_id]\n",
    "# filter = np.array([x%90==0 for x in dirs])\n",
    "# n_trials = len(filter)\n",
    "# act_trial_mean_activity = act_trial_mean_activity[filter[0::act_trial_spikes.shape[1]].squeeze(),:]\n",
    "# print(act_trial_mean_activity.shape)\n",
    "\n",
    "# cos_x = np.array([round(math.cos(math.radians(i)),3) for i in dirs])[filter.squeeze()]\n",
    "# sin_y = np.array([round(math.sin(math.radians(i)),3) for i in dirs])[filter.squeeze()]\n",
    "# cos_sin = np.array([cos_x, sin_y]).T\n",
    "# print(cos_sin.shape)\n",
    "\n",
    "# n_trials = act_trial_mean_activity.shape[0]\n",
    "# cond_dict = cond_dict[filter[0::act_trial_spikes.shape[1]].squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "X = act_trial_mean_activity\n",
    "if proj_out_CD:\n",
    "    X =  X - calc_proj(X,CD_axes.T).T\n",
    "\n",
    "y=np.array(cos_x).reshape((n_trials,-1,1))[:,0,:]\n",
    "X_proc = X\n",
    "axes_list_x = nans([N,y.shape[1],dim])\n",
    "r2_cv_list_x = nans([N])\n",
    "r2_list_x = nans([N])\n",
    "for i in range(N):\n",
    "    reg = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    # reg = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    axes_list_x[i,:,:] = reg.best_estimator_.coef_\n",
    "    r2_list_x[i] = reg.best_estimator_.score(X_proc, y)\n",
    "    weights = reg.best_estimator_.coef_\n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True,random_state = 42)   \n",
    "    true_concat = nans([n_trials,1])\n",
    "    pred_concat = nans([n_trials,1])\n",
    "    trial_save_idx = 0\n",
    "    for training_set, test_set in skf.split(range(0,n_trials)):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test = X_proc[training_set,:],X_proc[test_set,:]\n",
    "        y_train, y_test = y[training_set],y[test_set]\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        # lr = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_test_predicted = lr.predict(X_test)\n",
    "\n",
    "        n = y_test_predicted.shape[0]\n",
    "        true_concat[trial_save_idx:trial_save_idx+n] = y_test\n",
    "        pred_concat[trial_save_idx:trial_save_idx+n] = y_test_predicted.reshape(len(y_test_predicted),1)\n",
    "        trial_save_idx += n\n",
    "    sses =get_sses_pred(true_concat,pred_concat)\n",
    "    sses_mean=get_sses_mean(true_concat)\n",
    "    R2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "    r2_cv_list_x[i] = R2\n",
    "    X_proc = X_proc - calc_proj(X_proc,weights.T).T\n",
    "\n",
    "X_proc = X\n",
    "y = np.array(sin_y).reshape((n_trials,-1,1))[:,0,:]\n",
    "axes_list_y = nans([N,y.shape[1],dim])\n",
    "r2_cv_list_y = nans([N])\n",
    "r2_list_y = nans([N])\n",
    "for i in range(N):\n",
    "    reg = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    # reg = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    axes_list_y[i,:,:] = reg.best_estimator_.coef_\n",
    "    r2_list_y[i] = reg.best_estimator_.score(X_proc, y)\n",
    "    weights = reg.best_estimator_.coef_\n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True,random_state = 42)   \n",
    "    true_concat = nans([n_trials,1])\n",
    "    pred_concat = nans([n_trials,1])\n",
    "    trial_save_idx = 0\n",
    "    for training_set, test_set in skf.split(range(0,n_trials)):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test = X_proc[training_set,:],X_proc[test_set,:]\n",
    "        y_train, y_test = y[training_set],y[test_set]\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        # lr = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_test_predicted = lr.predict(X_test)\n",
    "\n",
    "        n = y_test_predicted.shape[0]\n",
    "        true_concat[trial_save_idx:trial_save_idx+n] = y_test\n",
    "        pred_concat[trial_save_idx:trial_save_idx+n] = y_test_predicted.reshape(len(y_test_predicted),1)\n",
    "        trial_save_idx += n\n",
    "    sses =get_sses_pred(true_concat,pred_concat)\n",
    "    sses_mean=get_sses_mean(true_concat)\n",
    "    R2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "    r2_cv_list_y[i] = R2\n",
    "    X_proc = X_proc - calc_proj(X_proc,weights.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(N)+1,r2_cv_list_x)\n",
    "plt.xlabel('N')\n",
    "plt.title('x-dir')\n",
    "plt.ylabel('R2')\n",
    "print(r2_cv_list_x)\n",
    "plt.show()\n",
    "plt.plot(np.arange(N)+1,r2_cv_list_y)\n",
    "plt.xlabel('N')\n",
    "plt.title('y-dir')\n",
    "plt.ylabel('R2')\n",
    "print(r2_cv_list_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.0\n",
    "print(axes_list_x.squeeze()[r2_cv_list_x>thresh,:].shape)\n",
    "print(axes_list_y.squeeze()[r2_cv_list_y>thresh,:].shape)\n",
    "FB_axes = np.vstack((axes_list_x.squeeze()[r2_cv_list_x>thresh,:],axes_list_y.squeeze()[r2_cv_list_y>thresh,:]))\n",
    "FB_axes.shape\n",
    "\n",
    "# # thresh = 0.08\n",
    "# # print(axes_list_x.squeeze()[r2_list_x>thresh,:].shape)\n",
    "# # print(axes_list_y.squeeze()[r2_list_y>thresh,:].shape)\n",
    "# # FB_axes_alt = np.vstack((axes_list_x.squeeze()[r2_list_x>thresh,:],axes_list_y.squeeze()[r2_list_y>thresh,:]))\n",
    "# # FB_axes_alt.shape\n",
    "\n",
    "all_data = dataset_10ms.data[x_field].to_numpy()\n",
    "proj_data = all_data @ FB_axes.T\n",
    "print(proj_data.shape)\n",
    "\n",
    "dataset_10ms.add_continuous_data(proj_data,'v6_alt_zscore_unsmoothed100_FB_proj_'+x_field)\n",
    "\n",
    "# reg='Lasso'\n",
    "# dataset_10ms.add_continuous_data(proj_data,'FB_proj_'+x_field+reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find FB? axes\n",
    "proj_out_CDFB = True\n",
    "align_range = (200, 400)\n",
    "align_field = 'move_onset_time'\n",
    "cond_dict = active_cond_dict\n",
    "\n",
    "mask = active_mask\n",
    "active_df = dataset.make_trial_data(align_field=align_field, align_range=align_range, ignored_trials=~mask)\n",
    "n_trials = active_df['trial_id'].nunique()\n",
    "X = active_df[x_field].to_numpy().reshape((-1, dim))\n",
    "# X = (X - mean)/std\n",
    "act_trial_spikes = X.reshape((n_trials, -1, dim))\n",
    "print(act_trial_spikes.shape)\n",
    "act_trial_mean_activity = np.mean(act_trial_spikes, axis=1)\n",
    "\n",
    "dirs = [dataset.trial_info[dataset.trial_info.trial_id == i]['cond_dir'] for i in active_df.trial_id]\n",
    "cos_x = [round(math.cos(math.radians(i)),3) for i in dirs]\n",
    "sin_y = [round(math.sin(math.radians(i)),3) for i in dirs]\n",
    "cos_sin = np.array([cos_x, sin_y]).T\n",
    "print(cos_sin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "X = act_trial_mean_activity\n",
    "if proj_out_CDFB:\n",
    "    CDFB_axes = np.vstack([CD_axes, FB_axes])\n",
    "    X =  X - calc_proj(X,CDFB_axes.T).T\n",
    "\n",
    "y=np.array(cos_x).reshape((n_trials,-1,1))[:,0,:]\n",
    "X_proc = X\n",
    "axes_list_x = nans([N,y.shape[1],dim])\n",
    "r2_cv_list_x = nans([N])\n",
    "r2_list_x = nans([N])\n",
    "for i in range(N):\n",
    "    reg = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    # reg = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    axes_list_x[i,:,:] = reg.best_estimator_.coef_\n",
    "    r2_list_x[i] = reg.best_estimator_.score(X_proc, y)\n",
    "    weights = reg.best_estimator_.coef_\n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True,random_state = 42)   \n",
    "    true_concat = nans([n_trials,1])\n",
    "    pred_concat = nans([n_trials,1])\n",
    "    trial_save_idx = 0\n",
    "    for training_set, test_set in skf.split(range(0,n_trials)):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test = X_proc[training_set,:],X_proc[test_set,:]\n",
    "        y_train, y_test = y[training_set],y[test_set]\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        # lr = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_test_predicted = lr.predict(X_test)\n",
    "\n",
    "        n = y_test_predicted.shape[0]\n",
    "        true_concat[trial_save_idx:trial_save_idx+n] = y_test\n",
    "        pred_concat[trial_save_idx:trial_save_idx+n] = y_test_predicted.reshape(len(y_test_predicted),1)\n",
    "        trial_save_idx += n\n",
    "    sses =get_sses_pred(true_concat,pred_concat)\n",
    "    sses_mean=get_sses_mean(true_concat)\n",
    "    R2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "    r2_cv_list_x[i] = R2\n",
    "    X_proc = X_proc - calc_proj(X_proc,weights.T).T\n",
    "\n",
    "X_proc = X\n",
    "y = np.array(sin_y).reshape((n_trials,-1,1))[:,0,:]\n",
    "axes_list_y = nans([N,y.shape[1],dim])\n",
    "r2_cv_list_y = nans([N])\n",
    "r2_list_y = nans([N])\n",
    "for i in range(N):\n",
    "    reg = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    # reg = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    axes_list_y[i,:,:] = reg.best_estimator_.coef_\n",
    "    r2_list_y[i] = reg.best_estimator_.score(X_proc, y)\n",
    "    weights = reg.best_estimator_.coef_\n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True,random_state = 42)   \n",
    "    true_concat = nans([n_trials,1])\n",
    "    pred_concat = nans([n_trials,1])\n",
    "    trial_save_idx = 0\n",
    "    for training_set, test_set in skf.split(range(0,n_trials)):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test = X_proc[training_set,:],X_proc[test_set,:]\n",
    "        y_train, y_test = y[training_set],y[test_set]\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        # lr = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_test_predicted = lr.predict(X_test)\n",
    "\n",
    "        n = y_test_predicted.shape[0]\n",
    "        true_concat[trial_save_idx:trial_save_idx+n] = y_test\n",
    "        pred_concat[trial_save_idx:trial_save_idx+n] = y_test_predicted.reshape(len(y_test_predicted),1)\n",
    "        trial_save_idx += n\n",
    "    sses =get_sses_pred(true_concat,pred_concat)\n",
    "    sses_mean=get_sses_mean(true_concat)\n",
    "    R2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "    r2_cv_list_y[i] = R2\n",
    "    X_proc = X_proc - calc_proj(X_proc,weights.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(N)+1,r2_cv_list_x)\n",
    "plt.xlabel('N')\n",
    "plt.title('x-dir')\n",
    "plt.ylabel('R2')\n",
    "print(r2_cv_list_x)\n",
    "plt.show()\n",
    "plt.plot(np.arange(N)+1,r2_cv_list_y)\n",
    "plt.xlabel('N')\n",
    "plt.title('y-dir')\n",
    "plt.ylabel('R2')\n",
    "print(r2_cv_list_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.0\n",
    "print(axes_list_x.squeeze()[r2_cv_list_x>thresh,:].shape)\n",
    "print(axes_list_y.squeeze()[r2_cv_list_y>thresh,:].shape)\n",
    "FBq_axes = np.vstack((axes_list_x.squeeze()[r2_cv_list_x>thresh,:],axes_list_y.squeeze()[r2_cv_list_y>thresh,:]))\n",
    "FBq_axes.shape\n",
    "\n",
    "\n",
    "all_data = dataset_10ms.data[x_field].to_numpy()\n",
    "proj_data = all_data @ FBq_axes.T\n",
    "print(proj_data.shape)\n",
    "\n",
    "dataset_10ms.add_continuous_data(proj_data,'v6_unsmoothed100_FB?_proj_'+x_field)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.data.keys().unique(0))\n",
    "CD_proj = np.array(dataset_10ms.data['v6_alt_zscore_unsmoothed100_CD_proj_'+x_field])\n",
    "print(CD_proj.shape)\n",
    "FB_proj = np.array(dataset_10ms.data['v6_alt_zscore_unsmoothed100_FB_proj_'+x_field])\n",
    "print(FB_proj.shape)\n",
    "CD_FB_proj  = np.hstack([CD_proj,FB_proj])\n",
    "print(CD_FB_proj.shape)\n",
    "dataset_10ms.add_continuous_data(CD_FB_proj,'v6_alt_zscore_unsmoothed100_CD_FB_proj_'+x_field)\n",
    "\n",
    "# # all_data = dataset_10ms.data[x_field].to_numpy()\n",
    "# # CD_proj_out_data = all_data - calc_proj(all_data,CD_axes.T).T\n",
    "# # print(CD_proj_out_data.shape)\n",
    "# # dataset_10ms.add_continuous_data(CD_proj_out_data,'CD_proj_out_'+x_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CD_axes_Lasso = CD_axes\n",
    "# FB_axes_Lasso = FB_axes\n",
    "\n",
    "# CD_axes_Ridge = CD_axes\n",
    "# FB_axes_Ridge = FB_axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(monkey+'_v6_alt_zscore_unsmoothed100_cdfb_weights_'+x_field, CD_axes = CD_axes, FB_axes = FB_axes) \n",
    "np.savez(monkey+'_v6_alt_zscore_unsmoothed100_cdfb_data_'+x_field, \\\n",
    "        CD_FB_proj = dataset.data['v6_alt_zscore_unsmoothed100_CD_FB_proj_'+x_field].to_numpy(), \\\n",
    "        FB_proj = dataset.data['v6_alt_zscore_unsmoothed100_FB_proj_'+x_field].to_numpy(),\n",
    "        CD_proj = dataset.data['v6_alt_zscore_unsmoothed100_CD_proj_'+x_field].to_numpy())\n",
    "\n",
    "# np.savez(monkey+'_v6_qsignal_unsmoothed100_cdfb_weights_'+x_field, CD_axes = CD_axes, FB_axes = FB_axes, FBq_axes = FBq_axes) \n",
    "# np.savez(monkey+'_v6_qsignal_unsmoothed100_cdfb_data_'+x_field, \\\n",
    "#         FBq_proj = dataset.data['v6_unsmoothed100_FB?_proj_'+x_field].to_numpy(), \\\n",
    "#         FB_proj = dataset.data['v6_unsmoothed100_FB_proj_'+x_field].to_numpy(),\n",
    "#         CD_proj = dataset.data['v6_unsmoothed100_CD_proj_'+x_field].to_numpy(),\n",
    "#         CD_FB_proj = dataset.data['v6_unsmoothed100_CD_FB_proj_'+x_field].to_numpy())\n",
    "\n",
    "# np.savez(monkey+'_cdfb_weights_'+x_field, CD_axes_Lasso = CD_axes_Lasso, FB_axes_Lasso = FB_axes_Lasso,\\\n",
    "#          CD_axes_Ridge = CD_axes_Ridge, FB_axes_Ridge = FB_axes_Ridge) \n",
    "# np.savez(monkey+'_cdfb_data_'+x_field, \\\n",
    "#         CD_FB_proj_Lasso = dataset.data['CD_FB_proj_'+x_field+'Lasso'].to_numpy(), \\\n",
    "#         FB_proj_Lasso = dataset.data['FB_proj_'+x_field+'Lasso'].to_numpy(),\\\n",
    "#         CD_proj_Lasso = dataset.data['CD_proj_'+x_field+'Lasso'].to_numpy(),\\\n",
    "#         CD_FB_proj_Ridge = dataset.data['CD_FB_proj_'+x_field+'Ridge'].to_numpy(), \\\n",
    "#         FB_proj_Ridge = dataset.data['FB_proj_'+x_field+'Ridge'].to_numpy(),\n",
    "#         CD_proj_Ridge = dataset.data['CD_proj_'+x_field+'Ridge'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = dataset_10ms.data['spikes_smth_40'].to_numpy()\n",
    "CD_proj = all_data @ CD_axes.T\n",
    "print(CD_proj.shape)\n",
    "dataset_10ms.add_continuous_data(CD_proj,'v6_smoothed100_CD_proj_'+x_field)\n",
    "\n",
    "FB_proj = all_data @ FB_axes.T\n",
    "print(FB_proj.shape)\n",
    "dataset_10ms.add_continuous_data(FB_proj,'v6_smoothed100_FB_proj_'+x_field)\n",
    "\n",
    "FBq_proj = all_data @ FBq_axes.T\n",
    "print(FBq_proj.shape)\n",
    "dataset_10ms.add_continuous_data(FBq_proj,'v6_smoothed100_FB?_proj_'+x_field)\n",
    "\n",
    "CD_FB_proj  = np.hstack([CD_proj,FB_proj])\n",
    "print(CD_FB_proj.shape)\n",
    "dataset_10ms.add_continuous_data(CD_FB_proj,'v6_smoothed100_CD_FB_proj_'+x_field)\n",
    "\n",
    "np.savez(monkey+'_v6_qsignal_smoothed100_cdfb_data_'+x_field, \\\n",
    "        CD_FB_proj = dataset_10ms.data['v6_smoothed100_CD_FB_proj_'+x_field].to_numpy(), \\\n",
    "        FB_proj = dataset_10ms.data['v6_smoothed100_FB_proj_'+x_field].to_numpy(),\n",
    "        CD_proj = dataset_10ms.data['v6_smoothed100_CD_proj_'+x_field].to_numpy(),\n",
    "        FBq_proj = dataset_10ms.data['v6_smoothed100_FB?_proj_'+x_field].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = CD_axes.T\n",
    "Y = FB_axes.T\n",
    "angles = principal_angles(X, Y)\n",
    "print(\"Principal angles\", np.degrees(angles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FB_axes_plot = FB_axes\n",
    "angDist_array = nans([len(CD_axes),len(FB_axes_plot)])\n",
    "for i in range(len(CD_axes)):\n",
    "    for j in range(len(FB_axes_plot)):\n",
    "        angDist_array[i,j] = math.degrees(angle_between(CD_axes[i,:],FB_axes_plot[j,:]))\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "im = ax.imshow(angDist_array)\n",
    "ax.set_xlabel('Feedback axes')\n",
    "ax.set_ylabel('CD axes')\n",
    "ax.set_xticks(np.arange(len(FB_axes_plot)))\n",
    "ax.set_yticks(np.arange(len(CD_axes)))\n",
    "\n",
    "for i in range(len(CD_axes)):\n",
    "    for j in range(len(FB_axes_plot)):\n",
    "        text = ax.text(j, i, str(int(angDist_array[i, j])),\n",
    "                        ha=\"center\", va=\"center\", color=\"w\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_cdfb_degrees_pc.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_10ms\n",
    "x_field = 'spikes'\n",
    "dim = dataset.data[x_field].shape[1]\n",
    "align_range = (-100, 1500)\n",
    "align_field = 'move_onset_time'\n",
    "mask = active_mask\n",
    "active_df = dataset.make_trial_data(align_field=align_field, align_range=align_range, ignored_trials=~mask)\n",
    "n_trials = active_n_trials\n",
    "X = active_df[x_field].to_numpy().reshape((-1, dim))\n",
    "print(X.shape)\n",
    "mean = np.nanmean(X,axis=0)\n",
    "std = np.nanstd(X,axis=0)\n",
    "mean.shape\n",
    "#CD\n",
    "align_range = (-100,0)\n",
    "align_field = 'move_onset_time'\n",
    "mask = active_mask\n",
    "active_df = dataset.make_trial_data(align_field=align_field, align_range=align_range, ignored_trials=~mask)\n",
    "cond_dict = active_cond_dict\n",
    "n_trials = active_n_trials\n",
    "X = active_df[x_field].to_numpy().reshape((-1, dim))\n",
    "X = (X - mean)/std\n",
    "act_trial_spikes = X.reshape((n_trials, -1, dim))\n",
    "print(act_trial_spikes.shape)\n",
    "act_trial_mean_activity = np.mean(act_trial_spikes, axis=1)\n",
    "\n",
    "\n",
    "dirs = [dataset.trial_info[dataset.trial_info.trial_id == i]['cond_dir'] for i in active_df.trial_id]\n",
    "cos_x = np.array([round(math.cos(math.radians(i)),3) for i in dirs])\n",
    "sin_y = np.array([round(math.sin(math.radians(i)),3) for i in dirs])\n",
    "cos_sin = np.array([cos_x, sin_y]).T\n",
    "print(cos_sin.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "X = act_trial_mean_activity\n",
    "\n",
    "y=np.array(cos_x).reshape((n_trials,-1,1))[:,0,:]\n",
    "X_proc = X\n",
    "axes_list_x = nans([N,y.shape[1],dim])\n",
    "r2_cv_list_x = nans([N])\n",
    "r2_list_x = nans([N])\n",
    "for i in range(N):\n",
    "    reg = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    # reg = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    axes_list_x[i,:,:] = reg.best_estimator_.coef_\n",
    "    r2_list_x[i] = reg.best_estimator_.score(X_proc, y)\n",
    "    weights = reg.best_estimator_.coef_\n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True,random_state = 42)   \n",
    "    true_concat = nans([n_trials,1])\n",
    "    pred_concat = nans([n_trials,1])\n",
    "    trial_save_idx = 0\n",
    "    for training_set, test_set in skf.split(range(0,n_trials),cond_dict):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test = X_proc[training_set,:],X_proc[test_set,:]\n",
    "        y_train, y_test = y[training_set],y[test_set]\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        # lr = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_test_predicted = lr.predict(X_test)\n",
    "\n",
    "        n = y_test_predicted.shape[0]\n",
    "        true_concat[trial_save_idx:trial_save_idx+n] = y_test\n",
    "        pred_concat[trial_save_idx:trial_save_idx+n] = y_test_predicted.reshape(len(y_test_predicted),1)\n",
    "        trial_save_idx += n\n",
    "    sses =get_sses_pred(true_concat,pred_concat)\n",
    "    sses_mean=get_sses_mean(true_concat)\n",
    "    R2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "    r2_cv_list_x[i] = R2\n",
    "    X_proc = X_proc - calc_proj(X_proc,weights.T).T\n",
    "    # X_proc = X_proc - calc_proj_sparse(X_proc,weights.T).T\n",
    "\n",
    "X_proc = X\n",
    "y = np.array(sin_y).reshape((n_trials,-1,1))[:,0,:]\n",
    "axes_list_y = nans([N,y.shape[1],dim])\n",
    "r2_cv_list_y = nans([N])\n",
    "r2_list_y = nans([N])\n",
    "for i in range(N):\n",
    "    reg = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    # reg = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "    axes_list_y[i,:,:] = reg.best_estimator_.coef_\n",
    "    r2_list_y[i] = reg.best_estimator_.score(X_proc, y)\n",
    "    weights = reg.best_estimator_.coef_\n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True,random_state = 42)   \n",
    "    true_concat = nans([n_trials,1])\n",
    "    pred_concat = nans([n_trials,1])\n",
    "    trial_save_idx = 0\n",
    "    for training_set, test_set in skf.split(range(0,n_trials),cond_dict):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test = X_proc[training_set,:],X_proc[test_set,:]\n",
    "        y_train, y_test = y[training_set],y[test_set]\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        # lr = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_test_predicted = lr.predict(X_test)\n",
    "\n",
    "        n = y_test_predicted.shape[0]\n",
    "        true_concat[trial_save_idx:trial_save_idx+n] = y_test\n",
    "        pred_concat[trial_save_idx:trial_save_idx+n] = y_test_predicted.reshape(len(y_test_predicted),1)\n",
    "        trial_save_idx += n\n",
    "    sses =get_sses_pred(true_concat,pred_concat)\n",
    "    sses_mean=get_sses_mean(true_concat)\n",
    "    R2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "    r2_cv_list_y[i] = R2\n",
    "    X_proc = X_proc - calc_proj(X_proc,weights.T).T\n",
    "thresh = 0.0\n",
    "print(axes_list_x.squeeze()[r2_cv_list_x>thresh,:].shape)\n",
    "print(axes_list_y.squeeze()[r2_cv_list_y>thresh,:].shape)\n",
    "CD_axes = np.vstack((axes_list_x.squeeze()[r2_cv_list_x>thresh,:],axes_list_y.squeeze()[r2_cv_list_y>thresh,:]))\n",
    "CD_axes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FB_axes_list = []\n",
    "\n",
    "# align_range_start = np.arange(-100, 1500,100)\n",
    "# align_range_end = np.arange(0, 1600,100)\n",
    "\n",
    "align_range_start = np.arange(-1600, 0, 100)\n",
    "align_range_end = np.arange(-1500, 100,100)\n",
    "\n",
    "\n",
    "for i in range(len(align_range_start)):\n",
    "    align_field = 'move_offset_time'\n",
    "    mask = active_mask\n",
    "    align_range = (align_range_start[i], align_range_end[i])\n",
    "    active_df = dataset.make_trial_data(align_field=align_field, align_range=align_range, ignored_trials=~mask)\n",
    "    cond_dict = active_cond_dict\n",
    "    n_trials = active_df['trial_id'].nunique()\n",
    "    X = active_df[x_field].to_numpy().reshape((-1, dim))\n",
    "    X = (X - mean)/std\n",
    "    act_trial_spikes = X.reshape((n_trials, -1, dim))\n",
    "    act_trial_mean_activity = np.mean(act_trial_spikes, axis=1)\n",
    "\n",
    "    dirs = [dataset.trial_info[dataset.trial_info.trial_id == i]['cond_dir'] for i in active_df.trial_id]\n",
    "    cos_x = [round(math.cos(math.radians(i)),3) for i in dirs]\n",
    "    sin_y = [round(math.sin(math.radians(i)),3) for i in dirs]\n",
    "    cos_sin = np.array([cos_x, sin_y]).T\n",
    "    print(cos_sin.shape)\n",
    "\n",
    "    N = 10\n",
    "    X = act_trial_mean_activity\n",
    "    y=np.array(cos_x).reshape((n_trials,-1,1))[:,0,:]\n",
    "    X_proc = X\n",
    "    axes_list_x = nans([N,y.shape[1],dim])\n",
    "    r2_cv_list_x = nans([N])\n",
    "    r2_list_x = nans([N])\n",
    "    for i in range(N):\n",
    "        reg = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "        # reg = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "        axes_list_x[i,:,:] = reg.best_estimator_.coef_\n",
    "        r2_list_x[i] = reg.best_estimator_.score(X_proc, y)\n",
    "        weights = reg.best_estimator_.coef_\n",
    "        skf = StratifiedKFold(n_splits=3,shuffle=True,random_state = 42)   \n",
    "        true_concat = nans([n_trials,1])\n",
    "        pred_concat = nans([n_trials,1])\n",
    "        trial_save_idx = 0\n",
    "        for training_set, test_set in skf.split(range(0,n_trials),cond_dict):\n",
    "            #split training and testing by trials\n",
    "            X_train, X_test = X_proc[training_set,:],X_proc[test_set,:]\n",
    "            y_train, y_test = y[training_set],y[test_set]\n",
    "            lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "            # lr = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "            lr.fit(X_train, y_train)\n",
    "            y_test_predicted = lr.predict(X_test)\n",
    "\n",
    "            n = y_test_predicted.shape[0]\n",
    "            true_concat[trial_save_idx:trial_save_idx+n] = y_test\n",
    "            pred_concat[trial_save_idx:trial_save_idx+n] = y_test_predicted.reshape(len(y_test_predicted),1)\n",
    "            trial_save_idx += n\n",
    "        sses =get_sses_pred(true_concat,pred_concat)\n",
    "        sses_mean=get_sses_mean(true_concat)\n",
    "        R2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "        r2_cv_list_x[i] = R2\n",
    "        X_proc = X_proc - calc_proj(X_proc,weights.T).T\n",
    "\n",
    "    X_proc = X\n",
    "    y = np.array(sin_y).reshape((n_trials,-1,1))[:,0,:]\n",
    "    axes_list_y = nans([N,y.shape[1],dim])\n",
    "    r2_cv_list_y = nans([N])\n",
    "    r2_list_y = nans([N])\n",
    "    for i in range(N):\n",
    "        reg = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "        # reg = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)}).fit(X_proc, y)\n",
    "        axes_list_y[i,:,:] = reg.best_estimator_.coef_\n",
    "        r2_list_y[i] = reg.best_estimator_.score(X_proc, y)\n",
    "        weights = reg.best_estimator_.coef_\n",
    "        skf = StratifiedKFold(n_splits=3,shuffle=True,random_state = 42)   \n",
    "        true_concat = nans([n_trials,1])\n",
    "        pred_concat = nans([n_trials,1])\n",
    "        trial_save_idx = 0\n",
    "        for training_set, test_set in skf.split(range(0,n_trials),cond_dict):\n",
    "            #split training and testing by trials\n",
    "            X_train, X_test = X_proc[training_set,:],X_proc[test_set,:]\n",
    "            y_train, y_test = y[training_set],y[test_set]\n",
    "            lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "            # lr = GridSearchCV(Lasso(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "            lr.fit(X_train, y_train)\n",
    "            y_test_predicted = lr.predict(X_test)\n",
    "\n",
    "            n = y_test_predicted.shape[0]\n",
    "            true_concat[trial_save_idx:trial_save_idx+n] = y_test\n",
    "            pred_concat[trial_save_idx:trial_save_idx+n] = y_test_predicted.reshape(len(y_test_predicted),1)\n",
    "            trial_save_idx += n\n",
    "        sses =get_sses_pred(true_concat,pred_concat)\n",
    "        sses_mean=get_sses_mean(true_concat)\n",
    "        R2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "        r2_cv_list_y[i] = R2\n",
    "        X_proc = X_proc - calc_proj(X_proc,weights.T).T\n",
    "    thresh = 0.0\n",
    "    print(axes_list_x.squeeze()[r2_cv_list_x>thresh,:].shape)\n",
    "    print(axes_list_y.squeeze()[r2_cv_list_y>thresh,:].shape)\n",
    "    FB_axes = np.vstack((axes_list_x.squeeze()[r2_cv_list_x>thresh,:],axes_list_y.squeeze()[r2_cv_list_y>thresh,:]))\n",
    "    print(FB_axes.shape)\n",
    "    FB_axes_list.append(FB_axes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_angles_list = []\n",
    "for i in range(len(align_range_start)):\n",
    "    FB_axes = FB_axes_list[i]\n",
    "    X = CD_axes.T\n",
    "    Y = FB_axes.T\n",
    "    angles = principal_angles(X, Y)\n",
    "    print(\"Principal angles\", np.degrees(angles))\n",
    "    principal_angles_list.append(np.degrees(angles)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(align_range_start, principal_angles_list,\"-o\",color = 'k')\n",
    "plt.xlim([-200, 1600])\n",
    "# plt.xlim([-1700, 0])\n",
    "plt.ylim(([-5, 80]))\n",
    "plt.axhline([75.37946756],linestyle='--')\n",
    "plt.xlabel('Time after move onset (ms)')\n",
    "plt.ylabel('Principal angle (deg)')\n",
    "plt.title('z-score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FB_axes_plot = FB_axes\n",
    "# angDist_array = nans([len(FB_axes_plot),len(FB_axes_plot)])\n",
    "# for i in range(len(FB_axes_plot)):\n",
    "#     for j in range(len(FB_axes_plot)):\n",
    "#         angDist_array[i,j] = math.degrees(angle_between(FB_axes_plot[i,:],FB_axes_plot[j,:]))\n",
    "# fig, ax = plt.subplots(figsize=(6, 6))\n",
    "# im = ax.imshow(angDist_array)\n",
    "# ax.set_xlabel('FB axes')\n",
    "# ax.set_ylabel('FB axes')\n",
    "# ax.set_xticks(np.arange(len(FB_axes_plot)))\n",
    "# ax.set_yticks(np.arange(len(FB_axes_plot)))\n",
    "# for i in range(len(FB_axes_plot)):\n",
    "#     for j in range(len(FB_axes_plot)):\n",
    "#         text = ax.text(j, i, str(int(angDist_array[i, j])),\n",
    "#                         ha=\"center\", va=\"center\", color=\"w\", fontsize=14)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# angDist_array = nans([len(CD_axes),len(CD_axes)])\n",
    "# for i in range(len(CD_axes)):\n",
    "#     for j in range(len(CD_axes)):\n",
    "#         angDist_array[i,j] = math.degrees(angle_between(CD_axes[i,:],CD_axes[j,:]))\n",
    "# fig, ax = plt.subplots(figsize=(4, 4))\n",
    "# im = ax.imshow(angDist_array)\n",
    "# ax.set_xlabel('CD axes')\n",
    "# ax.set_ylabel('CD axes')\n",
    "# ax.set_xticks(np.arange(len(CD_axes)))\n",
    "# ax.set_yticks(np.arange(len(CD_axes)))\n",
    "# for i in range(len(CD_axes)):\n",
    "#     for j in range(len(CD_axes)):\n",
    "#         text = ax.text(j, i, str(int(angDist_array[i, j])),\n",
    "#                         ha=\"center\", va=\"center\", color=\"w\", fontsize=14)\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'spikes'\n",
    "data = np.load(monkey+'_v6_unsmoothed100_cdfb_weights_'+x_field+'.npz')\n",
    "data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = dataset_10ms.data['spikes_smth_40'].to_numpy()\n",
    "CD_proj = all_data @ data['CD_axes'].T\n",
    "# CD_proj = all_data @ CD_axes.T\n",
    "print(CD_proj.shape)\n",
    "dataset_10ms.add_continuous_data(CD_proj,'v6_smoothed100_CD_proj_'+x_field)\n",
    "\n",
    "FB_proj = all_data @ data['FB_axes'].T\n",
    "print(FB_proj.shape)\n",
    "dataset_10ms.add_continuous_data(FB_proj,'v6_smoothed100_FB_proj_'+x_field)\n",
    "\n",
    "CD_FB_proj  = np.hstack([CD_proj,FB_proj])\n",
    "print(CD_FB_proj.shape)\n",
    "dataset_10ms.add_continuous_data(CD_FB_proj,'v6_smoothed100_CD_FB_proj_'+x_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez(monkey+'_v6_smoothed100_cdfb_data_'+x_field, \\\n",
    "#         CD_FB_proj = dataset_10ms.data['v6_smoothed100_CD_FB_proj_'+x_field].to_numpy(), \\\n",
    "#         FB_proj = dataset_10ms.data['v6_smoothed100_FB_proj_'+x_field].to_numpy(),\n",
    "#         CD_proj = dataset_10ms.data['v6_smoothed100_CD_proj_'+x_field].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_10ms\n",
    "x_field = 'spikes'\n",
    "data = np.load(monkey+'_v6_alt_smoothed100_cdfb_data_'+x_field+'.npz')\n",
    "data.files\n",
    "dataset.add_continuous_data(data['CD_FB_proj'],'alt_smooth_CD_FB_proj')\n",
    "dataset.add_continuous_data(data['CD_proj'],'alt_smooth_CD_proj')\n",
    "dataset.add_continuous_data(data['FB_proj'],'alt_smooth_FB_proj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'spikes'\n",
    "data = np.load(monkey+'_v6_smoothed100_cdfb_data_'+x_field+'.npz')\n",
    "data.files\n",
    "dataset.add_continuous_data(data['CD_FB_proj'],'smooth_CD_FB_proj')\n",
    "dataset.add_continuous_data(data['CD_proj'],'smooth_CD_proj')\n",
    "dataset.add_continuous_data(data['FB_proj'],'smooth_FB_proj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_field = 'spikes'\n",
    "# data = np.load(monkey+'_v6_alt_unsmoothed100_cdfb_data_'+x_field+'.npz')\n",
    "# data.files\n",
    "# dataset.add_continuous_data(data['CD_FB_proj'],'alt_unsmooth_CD_FB_proj')\n",
    "# dataset.add_continuous_data(data['CD_proj'],'alt_unsmooth_CD_proj')\n",
    "# dataset.add_continuous_data(data['FB_proj'],'alt_unsmooth_FB_proj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA projections over trial, for different reaching directions\n",
    "plot_dir = np.array([0,45,90,135,180,225,270,315]) \n",
    "directions = np.array([0,45,90,135,180,225,270,315])\n",
    "# plot_dir = np.array([0,90,180,270]) \n",
    "# directions = np.array([0,90,180,270])\n",
    "cmap = plt.get_cmap('coolwarm',len(plot_dir))\n",
    "custom_palette = [mpl.colors.rgb2hex(cmap(i)) for i in range(len(plot_dir))]\n",
    "plot_field = 'v6_alt_zscore_unsmoothed100_CD_FB_proj_'+x_field\n",
    "# plot_field = 'alt_smooth_CD_FB_proj'\n",
    "N = dataset_10ms.data[plot_field].shape[1]\n",
    "order = range(N)\n",
    "\n",
    "pred_range = (-200, 1100)\n",
    "trial_mask = active_mask\n",
    "cond_dict = active_cond_dict\n",
    "n_timepoints = int((pred_range[1] - pred_range[0])/dataset_10ms.bin_width)\n",
    "data = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=pred_range, ignored_trials=~trial_mask, allow_overlap=True)\n",
    "n_trials = data['trial_id'].nunique()\n",
    "trials_pca = nans([n_trials,n_timepoints,N])\n",
    "i = 0\n",
    "for idx, trial in data.groupby('trial_id'):\n",
    "    trials_pca[i,:,:]=trial[plot_field].to_numpy()\n",
    "    i+=1\n",
    "print(trials_pca.shape)\n",
    "\n",
    "x_axis = np.arange(pred_range[0], pred_range[1], dataset_10ms.bin_width)\n",
    "\n",
    "# define some useful time points\n",
    "move_idx=0\n",
    "ret_idx = 500\n",
    "\n",
    "plot_dims = N\n",
    "\n",
    "fig,ax=plt.subplots(plot_dims,1,figsize=(10,N+4))\n",
    "for i in range(plot_dims):\n",
    "    for j in range(len(plot_dir)):\n",
    "        color = custom_palette[j]\n",
    "        dir_idx = np.argwhere(directions == plot_dir[j])[0]\n",
    "        cond_mean_proj = np.mean(trials_pca[np.argwhere(cond_dict==dir_idx).flatten(),:,:], axis = 0)[:,order[i]] \n",
    "        pca_mean = np.mean(data[plot_field].to_numpy(),axis = 0)[order[i]] \n",
    "        ax[i].plot(x_axis,cond_mean_proj - pca_mean,linewidth=2.25,color = color,label = plot_dir[j])\n",
    "        \n",
    "        ax[i].axvline(move_idx, color='k',linewidth = 1)\n",
    "        ax[i].axvline(ret_idx, color='k',linewidth = 1)\n",
    "        \n",
    "        ax[i].set_xlim([-200,1000])\n",
    "        # ax[i].set_ylim([-15, 15])\n",
    "        ax[i].axhline(0,color ='k',ls = '--')\n",
    "        if i<plot_dims-1:\n",
    "            ax[i].set_xticks([])\n",
    "        else:\n",
    "            ax[i].set_xlabel('Time after movement onset (ms)')\n",
    "            \n",
    "        ax[i].set_yticks([])\n",
    "        ax[i].set_ylabel('Dim. '+str(i+1))\n",
    "\n",
    "    ax[0].set_title('Active trials')\n",
    "     \n",
    "plt.legend(bbox_to_anchor = (1, 1), loc = 'upper left')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_alt_cdfb_active_smooth.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA projections over trial, for different reaching directions\n",
    "pred_range = (-100, 600)\n",
    "trial_mask = passive_mask\n",
    "cond_dict = passive_cond_dict\n",
    "n_timepoints = int((pred_range[1] - pred_range[0])/dataset_10ms.bin_width)\n",
    "data = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=pred_range, ignored_trials=~trial_mask, allow_overlap=True)\n",
    "n_trials = data['trial_id'].nunique()\n",
    "trials_pca = nans([n_trials,n_timepoints,N])\n",
    "i = 0\n",
    "for idx, trial in data.groupby('trial_id'):\n",
    "    trials_pca[i,:,:]=trial[plot_field].to_numpy()\n",
    "    i+=1\n",
    "print(trials_pca.shape)\n",
    "\n",
    "x_axis = np.arange(pred_range[0], pred_range[1], dataset_10ms.bin_width)\n",
    "\n",
    "# define some useful time points\n",
    "move_idx=0\n",
    "ret_idx = 120\n",
    "\n",
    "plot_dims = N\n",
    "\n",
    "fig,ax=plt.subplots(plot_dims,1,figsize=(10,N+4))\n",
    "for i in range(plot_dims):\n",
    "    for j in range(len(plot_dir)):\n",
    "        color = custom_palette[j]\n",
    "        dir_idx = np.argwhere(directions == plot_dir[j])[0]\n",
    "        cond_mean_proj = np.mean(trials_pca[np.argwhere(cond_dict==dir_idx).flatten(),:,:], axis = 0)[:,order[i]] \n",
    "        pca_mean = np.mean(data[plot_field].to_numpy(),axis = 0)[order[i]]\n",
    "        ax[i].plot(x_axis,cond_mean_proj - pca_mean,linewidth=2.25,color = color,label = plot_dir[j])\n",
    "        \n",
    "        ax[i].axvline(move_idx, color='k',linewidth = 1)\n",
    "        # ax[i].axvline(120, color='k',linewidth = 1)\n",
    "        ax[i].axvline(ret_idx, color='k',linewidth = 1)\n",
    "        ax[i].set_xlim([-100,500])\n",
    "        # ax[i].set_ylim([-15, 15])\n",
    "        ax[i].axhline(0,color ='k',ls = '--')\n",
    "        if i<plot_dims-1:\n",
    "            ax[i].set_xticks([])\n",
    "        else:\n",
    "            ax[i].set_xlabel('Time after movement onset (ms)')\n",
    "\n",
    "        ax[i].set_yticks([])\n",
    "        ax[i].set_ylabel('Dim. '+str(i+1))\n",
    "\n",
    "    ax[0].set_title('Passive trials')\n",
    "\n",
    "plt.legend(bbox_to_anchor = (1, 1), loc = 'upper left')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_alt_cdfb_passive_smooth.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA projections over trial, for different reaching directions\n",
    "pred_range = (-100, 1100)\n",
    "trial_mask = nan_mask\n",
    "# cond_dict = nan_bump_cond_dict\n",
    "cond_dict = nan_cond_dict\n",
    "n_timepoints = int((pred_range[1] - pred_range[0])/dataset_10ms.bin_width)\n",
    "data = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=pred_range, ignored_trials=~trial_mask, allow_overlap=True)\n",
    "n_trials = data['trial_id'].nunique()\n",
    "trials_pca = nans([n_trials,n_timepoints,N])\n",
    "i = 0\n",
    "for idx, trial in data.groupby('trial_id'):\n",
    "    trials_pca[i,:,:]=trial[plot_field].to_numpy()\n",
    "    i+=1\n",
    "print(trials_pca.shape)\n",
    "\n",
    "x_axis = np.arange(pred_range[0], pred_range[1], dataset_10ms.bin_width)\n",
    "\n",
    "# define some useful time points\n",
    "move_idx=0\n",
    "ret_idx = 200\n",
    "\n",
    "plot_dims = N\n",
    "\n",
    "fig,ax=plt.subplots(plot_dims,1,figsize=(10,N+4))\n",
    "for i in range(plot_dims):\n",
    "    for j in range(len(plot_dir)):\n",
    "        color = custom_palette[j]\n",
    "        dir_idx = np.argwhere(directions == plot_dir[j])[0]\n",
    "        cond_mean_proj = np.mean(trials_pca[np.argwhere(cond_dict==dir_idx).flatten(),:,:], axis = 0)[:,order[i]] \n",
    "        pca_mean = np.mean(data[plot_field].to_numpy(),axis = 0)[order[i]]\n",
    "        ax[i].plot(x_axis,cond_mean_proj - pca_mean,linewidth=2.25,color = color,label = plot_dir[j])\n",
    "        \n",
    "        ax[i].axvline(move_idx, color='k',linewidth = .5)\n",
    "        ax[i].axvline(ret_idx, color='k',linewidth = .5)\n",
    "        ax[i].set_xlim([-100,1000])\n",
    "        # ax[i].set_ylim([-15, 15])\n",
    "        ax[i].axhline(0,color ='k',ls = '--')\n",
    "        if i<plot_dims-1:\n",
    "            ax[i].set_xticks([])\n",
    "        else:\n",
    "            ax[i].set_xlabel('Time after movement onset (ms)')\n",
    "            \n",
    "        ax[i].set_yticks([])\n",
    "        ax[i].set_ylabel('Dim. '+str(i+1))\n",
    "\n",
    "    ax[0].set_title('Active-bump trials ' + plot_field)\n",
    "    \n",
    "plt.legend(bbox_to_anchor = (1, 1), loc = 'upper left')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_cdfb_passive_pc.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_field = 'CD_proj_'+x_field\n",
    "x_name = '0000'\n",
    "y_name = '0001'\n",
    "\n",
    "# Active, 2D plot\n",
    "unique_conditions = [(False, 0.0), (False, 45.0), (False, 90.0), (False, 135.0),\n",
    "                     (False, 180.0), (False, 225.0), (False, 270.0), (False, 315.0)]\n",
    "# unique_conditions = [(False, 0.0),  (False, 90.0), (False, 180.0), (False, 270.0)]\n",
    "\n",
    "# Initialize figure\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "ax_0 = fig.add_subplot(1,5,1)\n",
    "ax_1 = fig.add_subplot(1, 5,2)\n",
    "ax_2 = fig.add_subplot(1, 5,3)\n",
    "ax_3 = fig.add_subplot(1, 5,4)\n",
    "ax_4 = fig.add_subplot(1, 5,5)\n",
    "# xlim = [-1.5, 1.5]\n",
    "# ylim = [-1.5, 1.5]\n",
    "\n",
    "for cond in unique_conditions:\n",
    "    # Filter out invalid trials (labeled 'none') and trials in other conditions\n",
    "    cond_mask = (dataset.trial_info['ctr_hold_bump']==cond[0]) & \\\n",
    "                (dataset.trial_info['cond_dir']%360==cond[1]) & \\\n",
    "                (dataset.trial_info.split != 'none')\n",
    "# cond_mask = (np.isnan(dataset.trial_info['ctr_hold_bump'])) & \\\n",
    "\n",
    "    # Extract relevant portion of selected trials\n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(-200, 0), ignored_trials=~cond_mask)\n",
    "    ax_0.plot(cond_data.groupby('align_time').mean()[plot_field][x_name].to_numpy(),cond_data.groupby('align_time').mean()[plot_field][y_name].to_numpy(),color=plt.cm.hsv(cond[1] / 360))\n",
    "    # for idx, trial in cond_data.groupby('trial_id'):\n",
    "    #     ax_0.plot(trial[plot_field][x_name], trial[plot_field][y_name], color=plt.cm.hsv(cond[1] / 360), linewidth=0.5)\n",
    "        # ax_0.set_xlim(xlim)\n",
    "        # ax_0.set_ylim(ylim)\n",
    "\n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(0, 500), ignored_trials=~cond_mask)\n",
    "    ax_1.plot(cond_data.groupby('align_time').mean()[plot_field][x_name].to_numpy(),cond_data.groupby('align_time').mean()[plot_field][y_name].to_numpy(),color=plt.cm.hsv(cond[1] / 360))\n",
    "    # for idx, trial in cond_data.groupby('trial_id'):\n",
    "    #     ax_1.plot(trial[plot_field][x_name], trial[plot_field][y_name], color=plt.cm.hsv(cond[1] / 360), linewidth=0.5)\n",
    "        # ax_1.set_xlim(xlim)\n",
    "        # ax_1.set_ylim(ylim)\n",
    "    \n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(500, 1000), ignored_trials=~cond_mask)\n",
    "    ax_2.plot(cond_data.groupby('align_time').mean()[plot_field][x_name].to_numpy(),cond_data.groupby('align_time').mean()[plot_field][y_name].to_numpy(),color=plt.cm.hsv(cond[1] / 360))\n",
    "    # for idx, trial in cond_data.groupby('trial_id'):\n",
    "    #     ax_2.plot(trial[plot_field][x_name], trial[plot_field][y_name], color=plt.cm.hsv(cond[1] / 360), linewidth=0.5)\n",
    "        # ax_2.set_xlim(xlim)\n",
    "        # ax_2.set_ylim(ylim)\n",
    "            \n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(1000, 1500), ignored_trials=~cond_mask)\n",
    "    ax_3.plot(cond_data.groupby('align_time').mean()[plot_field][x_name].to_numpy(),cond_data.groupby('align_time').mean()[plot_field][y_name].to_numpy(),color=plt.cm.hsv(cond[1] / 360))\n",
    "    # for idx, trial in cond_data.groupby('trial_id'):\n",
    "    #     ax_3.plot(trial[plot_field][x_name], trial[plot_field][y_name], color=plt.cm.hsv(cond[1] / 360), linewidth=0.5)\n",
    "        # ax_3.set_xlim(xlim)\n",
    "        # ax_3.set_ylim(ylim)\n",
    "\n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(1500, 2000), ignored_trials=~cond_mask)\n",
    "    ax_4.plot(cond_data.groupby('align_time').mean()[plot_field][x_name].to_numpy(),cond_data.groupby('align_time').mean()[plot_field][y_name].to_numpy(),color=plt.cm.hsv(cond[1] / 360))\n",
    "    # for idx, trial in cond_data.groupby('trial_id'):\n",
    "    #     ax_4.plot(trial[plot_field][x_name], trial[plot_field][y_name], color=plt.cm.hsv(cond[1] / 360), linewidth=0.5)\n",
    "        # ax_4.set_xlim(xlim)\n",
    "        # ax_4.set_ylim(ylim)\n",
    "            \n",
    "# Add labels\n",
    "ax_0.set_title('-200 to 0')\n",
    "ax_1.set_title('0 to 500')\n",
    "ax_2.set_title('500 to 1000')\n",
    "ax_3.set_title('1000 to 1500')\n",
    "ax_4.set_title('1500 to 2000')\n",
    "\n",
    "# ax_0.axis(\"off\")\n",
    "# ax_1.axis(\"off\")\n",
    "# ax_2.axis(\"off\")\n",
    "# ax_3.axis(\"off\")\n",
    "# ax_4.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_active_traj.pdf',dpi = 'figure')\n",
    "# plt.suptitle('Active Reach Trajectories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passive\n",
    "\n",
    "unique_conditions = [(True, 0.0), (True, 45.0), (True, 90.0), (True, 135.0),\n",
    "                     (True, 180.0), (True, 225.0), (True, 270.0), (True, 315.0)]\n",
    "# unique_conditions = [(True, 0.0),  (True, 90.0), (True, 180.0), (True, 270.0)]\n",
    "\n",
    "# Initialize figure\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax_0 = fig.add_subplot(1,3,1)\n",
    "ax_1 = fig.add_subplot(1, 3,2)\n",
    "ax_2 = fig.add_subplot(1, 3,3)\n",
    "# xlim = [-1.5, 1.5]\n",
    "# ylim = [-1.5, 1.5]\n",
    "\n",
    "for cond in unique_conditions:\n",
    "    # Filter out invalid trials (labeled 'none') and trials in other conditions\n",
    "    cond_mask = (dataset.trial_info['ctr_hold_bump']==cond[0]) & \\\n",
    "                (dataset.trial_info['cond_dir']%360==cond[1]) & \\\n",
    "                (dataset.trial_info.split != 'none')\n",
    "# cond_mask = (np.isnan(dataset.trial_info['ctr_hold_bump'])) & \\\n",
    "\n",
    "    # Extract relevant portion of selected trials\n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(-200, 0), ignored_trials=~cond_mask)\n",
    "    ax_0.plot(cond_data.groupby('align_time').mean()[plot_field][x_name].to_numpy(),cond_data.groupby('align_time').mean()[plot_field][y_name].to_numpy(),color=plt.cm.hsv(cond[1] / 360))\n",
    "    # for idx, trial in cond_data.groupby('trial_id'):\n",
    "    #     ax_0.plot(trial[plot_field][x_name], trial[plot_field][y_name], color=plt.cm.hsv(cond[1] / 360), linewidth=0.5)\n",
    "        # ax_0.set_xlim(xlim)\n",
    "        # ax_0.set_ylim(ylim)\n",
    "    \n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(0, 200), ignored_trials=~cond_mask)\n",
    "    ax_1.plot(cond_data.groupby('align_time').mean()[plot_field][x_name].to_numpy(),cond_data.groupby('align_time').mean()[plot_field][y_name].to_numpy(),color=plt.cm.hsv(cond[1] / 360))\n",
    "    # for idx, trial in cond_data.groupby('trial_id'):\n",
    "    #     ax_1.plot(trial[plot_field][x_name], trial[plot_field][y_name], color=plt.cm.hsv(cond[1] / 360), linewidth=0.5)\n",
    "        # ax_1.set_xlim(xlim)\n",
    "        # ax_1.set_ylim(ylim)\n",
    "\n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(200, 500), ignored_trials=~cond_mask)\n",
    "    ax_2.plot(cond_data.groupby('align_time').mean()[plot_field][x_name].to_numpy(),cond_data.groupby('align_time').mean()[plot_field][y_name].to_numpy(),color=plt.cm.hsv(cond[1] / 360))\n",
    "    # for idx, trial in cond_data.groupby('trial_id'):\n",
    "    #     ax_2.plot(trial[plot_field][x_name], trial[plot_field][y_name], color=plt.cm.hsv(cond[1] / 360), linewidth=0.5)\n",
    "        # ax_2.set_xlim(xlim)\n",
    "        # ax_2.set_ylim(ylim)\n",
    "            \n",
    "# Add labels\n",
    "ax_0.set_title('-200 to 0')\n",
    "ax_1.set_title('0 to 200')\n",
    "ax_2.set_title('200 to 500')\n",
    "\n",
    "# ax_0.axis(\"off\")\n",
    "# ax_1.axis(\"off\")\n",
    "# ax_2.axis(\"off\")\n",
    "# ax_3.axis(\"off\")\n",
    "# ax_4.axis(\"off\")\n",
    "\n",
    "figDir = '/Users/sherryan/area2_population_analysis/figures_plus/'\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_active_traj.pdf',dpi = 'figure')\n",
    "# plt.suptitle('Active Reach Trajectories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reaching directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldername = \"~/area2_population_analysis/s1-kinematics/actpas_NWB/\"\n",
    "# monkey = \"Han_20171207\"\n",
    "# filename = foldername + monkey + \"_COactpas_TD_offset6.nwb\"\n",
    "\n",
    "# monkey = \"Chips_20170913\"\n",
    "# filename = foldername + monkey + \"_COactpas_TD.nwb\"\n",
    "\n",
    "monkey = 'Duncan_20190710'\n",
    "filename = foldername + monkey + \"_COactpas_offset6.nwb\"\n",
    "\n",
    "dataset_50ms = NWBDataset(filename, split_heldout=False)\n",
    "dataset_50ms.resample(10)\n",
    "bin_width = dataset_50ms.bin_width\n",
    "print(bin_width)\n",
    "\n",
    "# dataset_50ms.resample(10)\n",
    "# bin_width = dataset_50ms.bin_width\n",
    "# print(bin_width)\n",
    "\n",
    "# filename = '/Users/sherryan/area2_population_analysis/s1-kinematics/'+monkey+'_COactpas_with_emg_TD.mat'\n",
    "# mat = scipy.io.loadmat(filename)\n",
    "# EMG = mat['trial_data']['emg'][0,0]\n",
    "# dataset_50ms.add_continuous_data(EMG,'EMG')\n",
    "\n",
    "# dataset_50ms.add_continuous_data(dataset_10ms.data.PCA_40.to_numpy(),'PCA_40')\n",
    "# dataset_50ms.add_continuous_data(dataset_10ms.data.spikes_smth_40_oneside.to_numpy(),'spikes_smth_40_oneside')\n",
    "# dataset_50ms.add_continuous_data(dataset_10ms.data.muscle_PCA.to_numpy(),'muscle_PCA')\n",
    "# dataset_50ms.add_continuous_data(dataset_10ms.data.joint_PCA.to_numpy(),'joint_PCA')\n",
    "\n",
    "# x_field = 'spikes'\n",
    "# data = np.load(monkey+'_unsmoothed50_cdfb_data_'+x_field+'.npz')\n",
    "# data.files\n",
    "# dataset_50ms.add_continuous_data(data['CD_FB_proj'],'CD_FB_proj')\n",
    "# dataset_50ms.add_continuous_data(data['CD_proj'],'CD_proj')\n",
    "# dataset_50ms.add_continuous_data(data['FB_proj'],'FB_proj')\n",
    "# dataset_50ms.resample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'spikes'\n",
    "data = np.load(monkey+'_v6_unsmoothed100_cdfb_data_'+x_field+'.npz')\n",
    "data.files\n",
    "dataset_50ms.add_continuous_data(data['CD_FB_proj'],'CD_FB_proj')\n",
    "dataset_50ms.add_continuous_data(data['CD_proj'],'CD_proj')\n",
    "dataset_50ms.add_continuous_data(data['FB_proj'],'FB_proj')\n",
    "# dataset_50ms.resample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procX_train_test(X,y,training_set,test_set):\n",
    "    X_train = X[training_set,:]\n",
    "    X_test = X[test_set,:]\n",
    "    y_train = y[training_set,:]\n",
    "    y_test = y[test_set,:]    \n",
    "    \n",
    "    X_train_mean=np.nanmean(X_train,axis=0)\n",
    "    X_train_std=np.nanstd(X_train,axis=0)  \n",
    "    X_train_std[X_train_std==0] = 1\n",
    "\n",
    "\n",
    "    X_train=(X_train-X_train_mean)/X_train_std\n",
    "    X_test=(X_test-X_train_mean)/X_train_std\n",
    " \n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial_mask = active_mask\n",
    "n_trials = dataset_50ms.trial_info.shape[0]\n",
    "print(n_trials,'total trials')\n",
    "n_neurons = dataset_50ms.data.spikes.shape[1]\n",
    "print(n_neurons,'neurons')\n",
    "\n",
    "#make dictionary for trial condition (reaching directions) for Stratified CV\n",
    "dataset = dataset_50ms\n",
    "active_mask = (dataset.trial_info.ctr_hold_bump==0) & (dataset.trial_info['split'] != 'none')\n",
    "passive_mask = (dataset.trial_info.ctr_hold_bump==1) & (dataset.trial_info['split'] != 'none')\n",
    "nan_mask = (np.isnan(dataset.trial_info.ctr_hold_bump)) & (dataset.trial_info['split'] != 'none')\n",
    "nan_against_mask = (np.isnan(dataset.trial_info.ctr_hold_bump)) & (dataset.trial_info['bump_dir']%360 == (dataset.trial_info['cond_dir']+180)%360) & (dataset.trial_info['split'] != 'none')\n",
    "nan_assist_mask = (np.isnan(dataset.trial_info.ctr_hold_bump)) & (dataset.trial_info['bump_dir']%360 == dataset.trial_info['cond_dir']%360) & (dataset.trial_info['split'] != 'none')\n",
    "nan_disturb_mask = (np.isnan(dataset.trial_info.ctr_hold_bump)) & (dataset.trial_info['bump_dir']%360 != dataset.trial_info['cond_dir']%360) & (dataset.trial_info['bump_dir']%360 != (dataset.trial_info['cond_dir']+180)%360) & (dataset.trial_info['split'] != 'none')\n",
    "\n",
    "all_mask = (dataset.trial_info['split'] != 'none')\n",
    "\n",
    "trial_mask = all_mask\n",
    "valid_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(valid_n_trials,'valid trials')\n",
    "\n",
    "trial_mask = active_mask\n",
    "active_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "active_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(active_n_trials,'active trials')\n",
    "\n",
    "trial_mask = passive_mask\n",
    "passive_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "passive_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(passive_n_trials,'passive trials')\n",
    "\n",
    "trial_mask = nan_mask\n",
    "nan_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "nan_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(nan_n_trials,'reach bump trials')\n",
    "\n",
    "trial_mask = nan_against_mask\n",
    "nan_against_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "nan_against_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(nan_against_n_trials,'reach bump against trials')\n",
    "\n",
    "trial_mask = nan_assist_mask\n",
    "nan_assist_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "nan_assist_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(nan_assist_n_trials,'reach bump assist trials')\n",
    "\n",
    "trial_mask = nan_disturb_mask\n",
    "nan_disturb_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "nan_disturb_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(nan_disturb_n_trials,'reach bump disturb trials')\n",
    "\n",
    "active_cond_dir_idx = []\n",
    "passive_cond_dir_idx = []\n",
    "nan_cond_dir_idx = []\n",
    "nan_bump_cond_dir_idx = []\n",
    "nan_against_cond_dir_idx = []\n",
    "nan_assist_cond_dir_idx = []\n",
    "nan_disturb_bump_dir_idx = []\n",
    "nan_disturb_cond_dir_idx = []\n",
    "\n",
    "for direction in [0,45,90,135,180,225,270,315]:\n",
    "# for direction in [0,90,180,270]:\n",
    "    active_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & (dataset.trial_info['ctr_hold_bump'] == 0) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "    passive_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & (dataset.trial_info['ctr_hold_bump'] == 1) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "    nan_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & (np.isnan(dataset.trial_info.ctr_hold_bump)) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "    nan_bump_cond_dir_idx.append(np.where((dataset.trial_info['bump_dir']%360 == direction) & (np.isnan(dataset.trial_info.ctr_hold_bump)) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "    nan_against_cond_dir_idx.append(np.where((dataset.trial_info['bump_dir']%360 == direction) & ((dataset.trial_info['cond_dir']+180)%360 == direction) & (np.isnan(dataset.trial_info.ctr_hold_bump)) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "    nan_assist_cond_dir_idx.append(np.where((dataset.trial_info['bump_dir']%360 == direction) & (dataset.trial_info['cond_dir']%360 == direction) & (np.isnan(dataset.trial_info.ctr_hold_bump)) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "    nan_disturb_bump_dir_idx.append(np.where((dataset.trial_info['bump_dir']%360 == direction) & ((dataset.trial_info['cond_dir']+180)%360 != direction) & ((dataset.trial_info['cond_dir'])%360 != direction) & \\\n",
    "           (np.isnan(dataset.trial_info.ctr_hold_bump)) & (dataset.trial_info['split'] != 'none'))[0])\n",
    "    nan_disturb_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & ((dataset.trial_info['bump_dir']+180)%360 != direction) & ((dataset.trial_info['bump_dir'])%360 != direction) & \\\n",
    "           (np.isnan(dataset.trial_info.ctr_hold_bump)) & (dataset.trial_info['split'] != 'none'))[0])\n",
    "    \n",
    "active_cond_dict = nans([active_n_trials])\n",
    "i = 0\n",
    "for idx in active_trials_idx:\n",
    "    for cond in range(0,len(active_cond_dir_idx)):\n",
    "        if idx in active_cond_dir_idx[cond]:\n",
    "            active_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(active_cond_dict)\n",
    "print(len(active_cond_dict))\n",
    "\n",
    "passive_cond_dict = nans([passive_n_trials])\n",
    "i = 0\n",
    "for idx in passive_trials_idx:\n",
    "    for cond in range(0,len(passive_cond_dir_idx)):\n",
    "        if idx in passive_cond_dir_idx[cond]:\n",
    "            passive_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(passive_cond_dict)\n",
    "print(len(passive_cond_dict))\n",
    "\n",
    "nan_cond_dict = nans([nan_n_trials])\n",
    "i = 0\n",
    "for idx in nan_trials_idx:\n",
    "    for cond in range(0,len(nan_cond_dir_idx)):\n",
    "        if idx in nan_cond_dir_idx[cond]:\n",
    "            nan_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(nan_cond_dict)\n",
    "print(len(nan_cond_dict))\n",
    "\n",
    "nan_bump_cond_dict = nans([nan_n_trials])\n",
    "i = 0\n",
    "for idx in nan_trials_idx:\n",
    "    for cond in range(0,len(nan_bump_cond_dir_idx)):\n",
    "        if idx in nan_bump_cond_dir_idx[cond]:\n",
    "            nan_bump_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(nan_bump_cond_dict)\n",
    "print(len(nan_bump_cond_dict))\n",
    "\n",
    "nan_against_cond_dict = nans([nan_against_n_trials])\n",
    "i = 0\n",
    "for idx in nan_against_trials_idx:\n",
    "    for cond in range(0,len(nan_against_cond_dir_idx)):\n",
    "        if idx in nan_against_cond_dir_idx[cond]:\n",
    "            nan_against_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(nan_against_cond_dict)\n",
    "print(len(nan_against_cond_dict))\n",
    "\n",
    "nan_assist_cond_dict = nans([nan_assist_n_trials])\n",
    "i = 0\n",
    "for idx in nan_assist_trials_idx:\n",
    "    for cond in range(0,len(nan_assist_cond_dir_idx)):\n",
    "        if idx in nan_assist_cond_dir_idx[cond]:\n",
    "            nan_assist_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(nan_assist_cond_dict)\n",
    "print(len(nan_assist_cond_dict))\n",
    "\n",
    "nan_disturb_cond_dict = nans([nan_disturb_n_trials])\n",
    "i = 0\n",
    "for idx in nan_disturb_trials_idx:\n",
    "    for cond in range(0,len(nan_disturb_cond_dir_idx)):\n",
    "        if idx in nan_disturb_cond_dir_idx[cond]:\n",
    "            nan_disturb_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(nan_disturb_cond_dict)\n",
    "print(len(nan_disturb_cond_dict))\n",
    "\n",
    "nan_disturb_bump_cond_dict = nans([nan_disturb_n_trials])\n",
    "i = 0\n",
    "for idx in nan_disturb_trials_idx:\n",
    "    for cond in range(0,len(nan_disturb_bump_dir_idx)):\n",
    "        if idx in nan_disturb_bump_dir_idx[cond]:\n",
    "            nan_disturb_bump_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(nan_disturb_bump_cond_dict)\n",
    "print(len(nan_disturb_bump_cond_dict))\n",
    "\n",
    "\n",
    "if monkey == 'Duncan_20190710':\n",
    "    active_df = dataset.make_trial_data(align_field='move_onset_time', align_range = (-100,100), ignored_trials = ~active_mask)\n",
    "    del_indices = list(set(active_trials_idx) - set(active_df['trial_id'].unique()))\n",
    "    print('was',active_n_trials,'active trials')\n",
    "    active_n_trials = active_n_trials - len(list(set(active_trials_idx) - set(active_df['trial_id'].unique())))\n",
    "    active_cond_dict_onset = np.delete(active_cond_dict,np.where(np.isin(active_trials_idx, del_indices)))\n",
    "    print('now',active_n_trials,'active trials')\n",
    "    print(len(active_cond_dict_onset))\n",
    "\n",
    "    passive_df = dataset.make_trial_data(align_field='move_onset_time', align_range = (-100,100), ignored_trials = ~passive_mask)\n",
    "    del_indices = list(set(passive_trials_idx) - set(passive_df['trial_id'].unique()))\n",
    "    print('was',passive_n_trials,'passive trials')\n",
    "    passive_n_trials = passive_n_trials - len(list(set(passive_trials_idx) - set(passive_df['trial_id'].unique())))\n",
    "    passive_cond_dict = np.delete(passive_cond_dict,np.where(np.isin(passive_trials_idx, del_indices)))\n",
    "    print('now',passive_n_trials,'passive trials')\n",
    "\n",
    "    print(len(passive_cond_dict))\n",
    "\n",
    "    # nan_df = dataset.make_trial_data(align_field='move_onset_time', align_range = (-100,100), ignored_trials = ~nan_mask)\n",
    "    # del_indices = list(set(nan_trials_idx) - set(nan_df['trial_id'].unique()))\n",
    "    # print('was',nan_n_trials,'nan trials')\n",
    "    # nan_n_trials = nan_n_trials - len(list(set(nan_trials_idx) - set(nan_df['trial_id'].unique())))\n",
    "    # nan_cond_dict = np.delete(nan_cond_dict,np.where(np.isin(nan_trials_idx, del_indices)))\n",
    "    # nan_bump_cond_dict = np.delete(nan_bump_cond_dict,np.where(np.isin(nan_trials_idx, del_indices)))\n",
    "    # print('now',nan_n_trials,'nan trials')\n",
    "    # print(len(nan_bump_cond_dict))\n",
    "\n",
    "active_df = dataset.make_trial_data(align_field='move_offset_time', align_range = (-100,0), ignored_trials = ~active_mask)\n",
    "del_indices = list(set(active_trials_idx) - set(active_df['trial_id'].unique()))\n",
    "print('was',active_n_trials,'active trials')\n",
    "active_cond_dict_offset = np.delete(active_cond_dict, np.where(np.isin(active_trials_idx, del_indices))[0])\n",
    "print('now')\n",
    "print(len(active_cond_dict_offset))\n",
    "if monkey == 'Duncan_20190710':\n",
    "    active_cond_dict = active_cond_dict_onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dims = 20\n",
    "active_mask = (dataset_50ms.trial_info.ctr_hold_bump==0) & (dataset_50ms.trial_info.split != 'none')\n",
    "passive_mask = (dataset_50ms.trial_info.ctr_hold_bump==1) & (dataset_50ms.trial_info.split != 'none')\n",
    "\n",
    "all_data = np.array(dataset_50ms.data.spikes)\n",
    "print(all_data.shape)\n",
    "if not np.isnan(all_data).any():\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(all_data)\n",
    "    pca = PCA(n_components=n_dims,random_state = 42)\n",
    "    PCA_data = pca.fit_transform(X)\n",
    "print(PCA_data.shape)\n",
    "dataset_50ms.add_continuous_data(PCA_data,'20PC')\n",
    "print('PCA total var explained:',sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reach-bump trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bump align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'spikes'\n",
    "data = np.load(monkey+'_v6_smoothed100_cdfb_data_'+x_field+'.npz')\n",
    "data.files\n",
    "dataset.add_continuous_data(data['CD_FB_proj'],'smooth_CD_FB_proj')\n",
    "dataset.add_continuous_data(data['CD_proj'],'smooth_CD_proj')\n",
    "dataset.add_continuous_data(data['FB_proj'],'smooth_FB_proj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA projections over trial, for different reaching directions\n",
    "plot_dir = np.array([0,45,90,135,180,225,270,315]) \n",
    "directions = np.array([0,45,90,135,180,225,270,315])\n",
    "\n",
    "cmap = plt.get_cmap('coolwarm',len(plot_dir))\n",
    "custom_palette = [mpl.colors.rgb2hex(cmap(i)) for i in range(len(plot_dir))]\n",
    "plot_field = 'smooth_CD_FB_proj'\n",
    "N = dataset_50ms.data[plot_field].shape[1]\n",
    "order = range(N)\n",
    "\n",
    "pred_range = (-300, 1000)\n",
    "\n",
    "# mask = nan_mask\n",
    "# n_trials = nan_n_trials\n",
    "# cond_dict = nan_cond_dict\n",
    "\n",
    "mask = nan_assist_mask\n",
    "n_trials = nan_assist_n_trials\n",
    "cond_dict = nan_assist_cond_dict\n",
    "\n",
    "# mask = nan_against_mask\n",
    "# n_trials = nan_against_n_trials\n",
    "# cond_dict = nan_against_cond_dict\n",
    "\n",
    "# mask = nan_disturb_mask\n",
    "# n_trials = nan_disturb_n_trials\n",
    "# cond_dict = nan_disturb_cond_dict\n",
    "# cond_dict = nan_disturb_bump_cond_dict\n",
    "\n",
    "n_timepoints = int((pred_range[1] - pred_range[0])/dataset_50ms.bin_width)\n",
    "data = dataset_50ms.make_trial_data(align_field='bump_time', align_range=pred_range, ignored_trials=~mask)\n",
    "n_trials = data['trial_id'].nunique()\n",
    "trials_pca = nans([n_trials,n_timepoints,N])\n",
    "i = 0\n",
    "for idx, trial in data.groupby('trial_id'):\n",
    "    trials_pca[i,:,:]=trial[plot_field].to_numpy()\n",
    "    i+=1\n",
    "print(trials_pca.shape)\n",
    "\n",
    "x_axis = np.arange(pred_range[0], pred_range[1], dataset_50ms.bin_width)\n",
    "\n",
    "# define some useful time points\n",
    "move_idx=0\n",
    "ret_idx = 500\n",
    "\n",
    "plot_dims = N\n",
    "\n",
    "fig,ax=plt.subplots(plot_dims,1,figsize=(10,N+4))\n",
    "for i in range(plot_dims):\n",
    "    for j in range(len(plot_dir)):\n",
    "        color = custom_palette[j]\n",
    "        dir_idx = np.argwhere(directions == plot_dir[j])[0]\n",
    "        cond_mean_proj = np.mean(trials_pca[np.argwhere(cond_dict==dir_idx).flatten(),:,:], axis = 0)[:,order[i]] \n",
    "        pca_mean = np.mean(data[plot_field].to_numpy(),axis = 0)[order[i]] \n",
    "        ax[i].plot(x_axis,cond_mean_proj - pca_mean,linewidth=2.25,color = color,label = plot_dir[j])\n",
    "        \n",
    "        ax[i].axvline(move_idx, color='k',linewidth = 1)\n",
    "        # ax[i].axvline(ret_idx, color='k',linewidth = 1)\n",
    "        \n",
    "        ax[i].set_xlim([-200,1000])\n",
    "        ax[i].set_ylim([-1, 1])\n",
    "        ax[i].axhline(0,color ='k',ls = '--')\n",
    "        if i<plot_dims-1:\n",
    "            ax[i].set_xticks([])\n",
    "        else:\n",
    "            ax[i].set_xlabel('Time after bump onset (ms)')\n",
    "            \n",
    "        ax[i].set_yticks([])\n",
    "        ax[i].set_ylabel('Dim. '+str(i+1))\n",
    "\n",
    "    ax[0].set_title('Assist trials (bump sorted)')\n",
    "     \n",
    "plt.legend(bbox_to_anchor = (1, 1), loc = 'upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_cdfb_reachbump_assist_smooth.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 20\n",
    "pred_range = (-300, 1000)\n",
    "x_field = 'CD_FB_proj'\n",
    "# mask = nan_mask\n",
    "# n_trials = nan_n_trials\n",
    "# cond_dict = nan_cond_dict\n",
    "\n",
    "# mask = nan_assist_mask\n",
    "# n_trials = nan_assist_n_trials\n",
    "# cond_dict = nan_assist_cond_dict\n",
    "\n",
    "# mask = nan_against_mask\n",
    "# n_trials = nan_against_n_trials\n",
    "# cond_dict = nan_against_cond_dict\n",
    "\n",
    "mask = nan_disturb_mask\n",
    "n_trials = nan_disturb_n_trials\n",
    "# cond_dict = nan_disturb_cond_dict\n",
    "cond_dict = nan_disturb_bump_cond_dict\n",
    "\n",
    "\n",
    "dim = dataset.data[x_field].shape[1]\n",
    "nan_df = dataset_50ms.make_trial_data(align_field='bump_time', align_range=pred_range, ignored_trials=~mask)\n",
    "\n",
    "dirs = [dataset_50ms.trial_info[dataset_50ms.trial_info.trial_id == i]['bump_dir'] for i in nan_df.trial_id]\n",
    "# dirs = [dataset_50ms.trial_info[dataset_50ms.trial_info.trial_id == i]['cond_dir'] for i in nan_df.trial_id]\n",
    "\n",
    "cos_x = [round(math.cos(math.radians(i)),3) for i in dirs]\n",
    "sin_y = [round(math.sin(math.radians(i)),3) for i in dirs]\n",
    "cos_sin = np.array([cos_x, sin_y]).T\n",
    "nan_trial_PCA = nan_df[x_field].to_numpy().reshape((n_trials, -1, dim))\n",
    "print(nan_trial_PCA.shape)\n",
    "nan_trial_ang = cos_sin.reshape((n_trials, -1, 2))\n",
    "# act_trial_ang = np.array(cos_x).reshape((n_trials,-1,1))\n",
    "# act_trial_ang = np.array(sin_y).reshape((n_trials,-1,1))\n",
    "print(nan_trial_ang.shape)\n",
    "n_bins = nan_trial_PCA.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bump_align_nan_r2_arr = nans([n_bins, n_splits])\n",
    "bump_coefs_arr = nans([n_bins, 2, dim])\n",
    "for i in range(n_bins):\n",
    "    X = nan_trial_PCA[:,i,:]\n",
    "    Y = nan_trial_ang[:,i,:]\n",
    "    std=np.nanstd(X,axis=0)  \n",
    "    std[std==0] = 1\n",
    "    X_proc = (X - np.nanmean(X,axis=0))/std\n",
    "    lr_all = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "    lr_all.fit(X_proc, Y)\n",
    "    bump_coefs_arr[i,:,:] = lr_all.best_estimator_.coef_\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits,test_size=0.2)\n",
    "    for j, (training_set, test_set) in enumerate(sss.split(range(0,n_trials),cond_dict)):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test, y_train, y_test = procX_train_test(X,Y,training_set,test_set)\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_test)\n",
    "        # Combined R² over both components\n",
    "        ss_res_combined = np.sum((y_test - y_pred) ** 2)\n",
    "        ss_tot_combined = np.sum((y_test - np.mean(y_test, axis=0)) ** 2)\n",
    "        r2_combined = 1 - ss_res_combined / ss_tot_combined\n",
    "        bump_align_nan_r2_arr[i,j] = r2_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(nan_trial_ang)\n",
    "shuffle_r2_arr = nans([n_bins, n_splits])\n",
    "for i in range(n_bins):\n",
    "    X = nan_trial_PCA[:,i,:]\n",
    "    Y = nan_trial_ang[:,i,:]\n",
    "    std=np.nanstd(X,axis=0)  \n",
    "    std[std==0] = 1\n",
    "    X_proc = (X - np.nanmean(X,axis=0))/std\n",
    "    lr_all = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "    lr_all.fit(X_proc, Y)\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits,test_size=0.2)\n",
    "    for j, (training_set, test_set) in enumerate(sss.split(range(0,n_trials),cond_dict)):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test, y_train, y_test = procX_train_test(X,Y,training_set,test_set)\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_test)\n",
    "        # Combined R² over both components\n",
    "        ss_res_combined = np.sum((y_test - y_pred) ** 2)\n",
    "        ss_tot_combined = np.sum((y_test - np.mean(y_test, axis=0)) ** 2)\n",
    "        r2_combined = 1 - ss_res_combined / ss_tot_combined\n",
    "        shuffle_r2_arr[i,j] = r2_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_nan_bump_r2 = bump_align_nan_r2_arr\n",
    "# shuffle_r2 = shuffle_r2_arr\n",
    "# cd_nan_bump_r2 = bump_align_nan_r2_arr\n",
    "# fb_nan_bump_r2 = bump_align_nan_r2_arr\n",
    "cd_fb_nan_bump_r2 = bump_align_nan_r2_arr\n",
    "cd_fb_bump_coefs_arr = bump_coefs_arr\n",
    "\n",
    "\n",
    "np.savez(monkey+'_v6_cdfb_reachbump_bumpalign_disturb_bumpdir_r2_unsmoothed', \n",
    "         cd_nan_bump_r2 = cd_nan_bump_r2, cd_fb_bump_coefs_arr = cd_fb_bump_coefs_arr,\\\n",
    "         fb_nan_bump_r2 = fb_nan_bump_r2, cd_fb_nan_bump_r2 = cd_fb_nan_bump_r2,\\\n",
    "         neural_nan_bump_r2 = neural_nan_bump_r2,\\\n",
    "         shuffle_r2=shuffle_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey = 'Duncan_20190710'\n",
    "data = np.load(monkey+'_v6_cdfb_reachbump_bumpalign_disturb_bumpdir_r2_unsmoothed.npz')\n",
    "data.files\n",
    "cd_nan_bump_r2 = data['cd_nan_bump_r2'] \n",
    "fb_nan_bump_r2=data['fb_nan_bump_r2'] \n",
    "cd_fb_nan_bump_r2=data['cd_fb_nan_bump_r2'] \n",
    "neural_nan_bump_r2=data['neural_nan_bump_r2'] \n",
    "shuffle_r2=data['shuffle_r2'] \n",
    "cd_fb_bump_coefs_arr = data['cd_fb_bump_coefs_arr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_axis = np.arange(-300, 1000, 10) \n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# Plot function with mean and shaded std\n",
    "def plot_with_shade(x, data, color, label, alpha=1, linestyle='-'):\n",
    "    mean = np.mean(data, axis=1)\n",
    "    std = np.std(data, axis=1)\n",
    "    ax.plot(x, mean, linestyle + 'o', color=color, alpha = alpha, label=label)\n",
    "    ax.fill_between(x, mean - std, mean + std, color=color, alpha=0.3)\n",
    "\n",
    "# Plotting each dataset\n",
    "plot_with_shade(lag_axis, cd_nan_bump_r2, 'green', 'CD')\n",
    "plot_with_shade(lag_axis, fb_nan_bump_r2, 'magenta', 'FB')\n",
    "plot_with_shade(lag_axis, cd_fb_nan_bump_r2, 'brown', 'CD+FB')\n",
    "plot_with_shade(lag_axis, neural_nan_bump_r2, 'grey', 'Neural', alpha=0.3,linestyle='--')\n",
    "plot_with_shade(lag_axis, shuffle_r2, 'k', 'shuffle',alpha=0.1,linestyle='--')\n",
    "\n",
    "\n",
    "\n",
    "# Axes labels and aesthetics\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "plt.ylabel('R²')\n",
    "plt.legend(fontsize=8)\n",
    "plt.xlim([-310, 1010])\n",
    "plt.ylim([-0.1, 1.0])\n",
    "plt.axvline(0, color='k', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.title('bump dir decoding')\n",
    "# Save and show\n",
    "plt.savefig(figDir + monkey + '_cdfb_dir_r2_reachbump_bumpalign_disturb_bumpdir.pdf', dpi='figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(lag_axis,bump_coefs_arr[:,0,3:7],label='FB-x',color='magenta')\n",
    "ax.plot(lag_axis,bump_coefs_arr[:,0,0:2],label='CD-x',color='green')\n",
    "ax.plot(lag_axis,bump_coefs_arr[:,0,7:11],label='FB-y',color='grey')\n",
    "ax.plot(lag_axis,bump_coefs_arr[:,0,2],label='CD-y',color='k')\n",
    "# plt.legend()\n",
    "plt.title('bump x-dir decoder weights')\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "# Create custom legend handles\n",
    "custom_lines = [\n",
    "    Line2D([0], [0], color='magenta', lw=2, label='FB-x'),\n",
    "    Line2D([0], [0], color='green', lw=2, label='CD-x'),\n",
    "    Line2D([0], [0], color='grey', lw=2, label='FB-y'),\n",
    "    Line2D([0], [0], color='k', lw=2, label='CD-y'),\n",
    "]\n",
    "plt.axvline(0, color='k', linestyle='--')\n",
    "# Add custom legend\n",
    "ax.legend(handles=custom_lines, fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(lag_axis,bump_coefs_arr[:,1,3:7],label='FB-x',color='grey')\n",
    "ax.plot(lag_axis,bump_coefs_arr[:,1,0:2],label='CD-x',color='k')\n",
    "ax.plot(lag_axis,bump_coefs_arr[:,1,7:11],label='FB-y',color='magenta')\n",
    "ax.plot(lag_axis,bump_coefs_arr[:,1,2],label='CD-y',color='green')\n",
    "# plt.legend()\n",
    "plt.title('bump y-dir decoder weights')\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "# Create custom legend handles\n",
    "custom_lines = [\n",
    "    Line2D([0], [0], color='grey', lw=2, label='FB-x'),\n",
    "    Line2D([0], [0], color='k', lw=2, label='CD-x'),\n",
    "    Line2D([0], [0], color='magenta', lw=2, label='FB-y'),\n",
    "    Line2D([0], [0], color='green', lw=2, label='CD-y'),\n",
    "]\n",
    "plt.axvline(0, color='k', linestyle='--')\n",
    "# Add custom legend\n",
    "ax.legend(handles=custom_lines, fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### move align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 20\n",
    "pred_range = (-300, 1000)\n",
    "x_field = 'spikes'\n",
    "mask = nan_mask\n",
    "n_trials = nan_n_trials\n",
    "cond_dict = nan_cond_dict\n",
    "dim = dataset.data[x_field].shape[1]\n",
    "nan_df = dataset_50ms.make_trial_data(align_field='move_onset_time', align_range=pred_range, ignored_trials=~mask)\n",
    "\n",
    "dirs = [dataset_50ms.trial_info[dataset_50ms.trial_info.trial_id == i]['cond_dir'] for i in nan_df.trial_id]\n",
    "cos_x = [round(math.cos(math.radians(i)),3) for i in dirs]\n",
    "sin_y = [round(math.sin(math.radians(i)),3) for i in dirs]\n",
    "cos_sin = np.array([cos_x, sin_y]).T\n",
    "nan_trial_PCA = nan_df[x_field].to_numpy().reshape((n_trials, -1, dim))\n",
    "print(nan_trial_PCA.shape)\n",
    "nan_trial_ang = cos_sin.reshape((n_trials, -1, 2))\n",
    "# act_trial_ang = np.array(cos_x).reshape((n_trials,-1,1))\n",
    "# act_trial_ang = np.array(sin_y).reshape((n_trials,-1,1))\n",
    "print(nan_trial_ang.shape)\n",
    "n_bins = nan_trial_PCA.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_cond_r2_arr = nans([n_bins, n_splits])\n",
    "for i in range(n_bins):\n",
    "    X = nan_trial_PCA[:,i,:]\n",
    "    Y = nan_trial_ang[:,i,:]\n",
    "    std=np.nanstd(X,axis=0)  \n",
    "    std[std==0] = 1\n",
    "    X_proc = (X - np.nanmean(X,axis=0))/std\n",
    "    lr_all = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "    lr_all.fit(X_proc, Y)\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits)\n",
    "    for j, (training_set, test_set) in enumerate(sss.split(range(0,n_trials),cond_dict)):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test, y_train, y_test = procX_train_test(X,Y,training_set,test_set)\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_test)\n",
    "        # Combined R² over both components\n",
    "        ss_res_combined = np.sum((y_test - y_pred) ** 2)\n",
    "        ss_tot_combined = np.sum((y_test - np.mean(y_test, axis=0)) ** 2)\n",
    "        r2_combined = 1 - ss_res_combined / ss_tot_combined\n",
    "        nan_cond_r2_arr[i,j] = r2_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(nan_trial_ang)\n",
    "shuffle_r2_arr = nans([n_bins, n_splits])\n",
    "for i in range(n_bins):\n",
    "    X = nan_trial_PCA[:,i,:]\n",
    "    Y = nan_trial_ang[:,i,:]\n",
    "    std=np.nanstd(X,axis=0)  \n",
    "    std[std==0] = 1\n",
    "    X_proc = (X - np.nanmean(X,axis=0))/std\n",
    "    lr_all = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "    lr_all.fit(X_proc, Y)\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits)\n",
    "    for j, (training_set, test_set) in enumerate(sss.split(range(0,n_trials),cond_dict)):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test, y_train, y_test = procX_train_test(X,Y,training_set,test_set)\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_test)\n",
    "        # Combined R² over both components\n",
    "        ss_res_combined = np.sum((y_test - y_pred) ** 2)\n",
    "        ss_tot_combined = np.sum((y_test - np.mean(y_test, axis=0)) ** 2)\n",
    "        r2_combined = 1 - ss_res_combined / ss_tot_combined\n",
    "        shuffle_r2_arr[i,j] = r2_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_dict = nan_bump_cond_dict\n",
    "dirs = [dataset_50ms.trial_info[dataset_50ms.trial_info.trial_id == i]['bump_dir'] for i in nan_df.trial_id]\n",
    "cos_x = [round(math.cos(math.radians(i)),3) for i in dirs]\n",
    "sin_y = [round(math.sin(math.radians(i)),3) for i in dirs]\n",
    "cos_sin = np.array([cos_x, sin_y]).T\n",
    "nan_trial_ang = cos_sin.reshape((n_trials, -1, 2))\n",
    "print(nan_trial_ang.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_bump_r2_arr = nans([n_bins,n_splits])\n",
    "for i in range(n_bins):\n",
    "    X = nan_trial_PCA[:,i,:]\n",
    "    Y = nan_trial_ang[:,i,:]\n",
    "    std=np.nanstd(X,axis=0)  \n",
    "    std[std==0] = 1\n",
    "    X_proc = (X - np.nanmean(X,axis=0))/std\n",
    "    lr_all = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "    lr_all.fit(X_proc, Y)\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits)\n",
    "    for j, (training_set, test_set) in enumerate(sss.split(range(0,n_trials),cond_dict)):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test, y_train, y_test = procX_train_test(X,Y,training_set,test_set)\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_test)\n",
    "        # Combined R² over both components\n",
    "        ss_res_combined = np.sum((y_test - y_pred) ** 2)\n",
    "        ss_tot_combined = np.sum((y_test - np.mean(y_test, axis=0)) ** 2)\n",
    "        r2_combined = 1 - ss_res_combined / ss_tot_combined\n",
    "        nan_bump_r2_arr[i,j] = r2_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd_nan_cond_r2 = nan_cond_r2_arr\n",
    "# cd_nan_bump_r2 = nan_bump_r2_arr\n",
    "# fb_nan_cond_r2 = nan_cond_r2_arr\n",
    "# fb_nan_bump_r2 = nan_bump_r2_arr\n",
    "# cd_fb_nan_cond_r2 = nan_cond_r2_arr\n",
    "# cd_fb_nan_bump_r2 = nan_bump_r2_arr\n",
    "# neural_pc_nan_cond_r2 = nan_cond_r2_arr\n",
    "# neural_pc_nan_bump_r2 = nan_bump_r2_arr\n",
    "neural_nan_cond_r2 = nan_cond_r2_arr\n",
    "neural_nan_bump_r2 = nan_bump_r2_arr\n",
    "shuffle_r2 = shuffle_r2_arr\n",
    "\n",
    "np.savez(monkey+'_v6_cdfb_reachbump_dir_r2_unsmoothed100', \n",
    "         cd_nan_cond_r2 = cd_nan_cond_r2, cd_nan_bump_r2 = cd_nan_bump_r2, \\\n",
    "         fb_nan_cond_r2 = fb_nan_cond_r2,fb_nan_bump_r2 = fb_nan_bump_r2, \\\n",
    "         cd_fb_nan_cond_r2 = cd_fb_nan_cond_r2,cd_fb_nan_bump_r2 = cd_fb_nan_bump_r2,\\\n",
    "         neural_pc_nan_cond_r2 = neural_pc_nan_cond_r2, neural_pc_nan_bump_r2 = neural_pc_nan_bump_r2,\\\n",
    "         neural_nan_cond_r2 = neural_nan_cond_r2, neural_nan_bump_r2 = neural_nan_bump_r2,\\\n",
    "         shuffle_r2=shuffle_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey = 'Duncan_20190710'\n",
    "data = np.load(monkey+'_v6_cdfb_reachbump_dir_r2_unsmoothed100.npz')\n",
    "data.files\n",
    "cd_act_r2 = data['cd_nan_cond_r2'] \n",
    "cd_pas_r2=data['cd_nan_bump_r2'] \n",
    "fb_act_r2=data['fb_nan_cond_r2'] \n",
    "fb_pas_r2=data['fb_nan_bump_r2'] \n",
    "cd_fb_act_r2=data['cd_fb_nan_cond_r2'] \n",
    "cd_fb_pas_r2=data['cd_fb_nan_bump_r2'] \n",
    "neural_spikes_act_r2=data['neural_nan_cond_r2'] \n",
    "neural_spikes_pas_r2=data['neural_nan_bump_r2'] \n",
    "shuffle_r2 = data['shuffle_r2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_axis = np.arange(-300, 1000, 50) + 25\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# Plot function with mean and shaded std\n",
    "def plot_with_shade(x, data, color, label, alpha=1, linestyle='-'):\n",
    "    mean = np.mean(data, axis=1)\n",
    "    std = np.std(data, axis=1)\n",
    "    ax.plot(x, mean, linestyle + 'o', color=color, alpha = alpha, label=label)\n",
    "    ax.fill_between(x, mean - std, mean + std, color=color, alpha=0.3)\n",
    "\n",
    "# Plotting each dataset\n",
    "plot_with_shade(lag_axis, cd_act_r2, 'green', 'CD')\n",
    "plot_with_shade(lag_axis, fb_act_r2, 'magenta', 'FB')\n",
    "plot_with_shade(lag_axis, cd_fb_act_r2, 'brown', 'CD_FB')\n",
    "plot_with_shade(lag_axis, neural_spikes_act_r2, 'grey', 'Neural', alpha=0.3,linestyle='--')\n",
    "plot_with_shade(lag_axis, shuffle_r2, 'k', 'shuffle', alpha=0.1,linestyle='--')\n",
    "\n",
    "\n",
    "# Axes labels and aesthetics\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "plt.ylabel('R²')\n",
    "plt.legend(fontsize=8)\n",
    "plt.xlim([-210, 1010])\n",
    "plt.ylim([-0.1, 1.0])\n",
    "plt.axvline(0, color='k', linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show\n",
    "plt.savefig(figDir + monkey + '_cdfb_dir_r2_reachbump_cond.pdf', dpi='figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_axis = np.arange(-300, 1000, 50) + 25\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# Plot function with mean and shaded std\n",
    "def plot_with_shade(x, data, color, label, alpha=1, linestyle='-'):\n",
    "    mean = np.mean(data, axis=1)\n",
    "    std = np.std(data, axis=1)\n",
    "    ax.plot(x, mean, linestyle + 'o', color=color, alpha = alpha, label=label)\n",
    "    ax.fill_between(x, mean - std, mean + std, color=color, alpha=0.3)\n",
    "\n",
    "# Plotting each dataset\n",
    "plot_with_shade(lag_axis, cd_pas_r2, 'green', 'CD')\n",
    "plot_with_shade(lag_axis, fb_pas_r2, 'magenta', 'FB')\n",
    "plot_with_shade(lag_axis, cd_fb_pas_r2, 'brown', 'CD_FB')\n",
    "plot_with_shade(lag_axis, neural_spikes_pas_r2, 'grey', 'Neural', alpha=0.3,linestyle='--')\n",
    "plot_with_shade(lag_axis, shuffle_r2, 'k', 'shuffle', alpha=0.1,linestyle='--')\n",
    "\n",
    "\n",
    "# Axes labels and aesthetics\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "plt.ylabel('R²')\n",
    "plt.legend(fontsize=8)\n",
    "plt.xlim([-210, 1010])\n",
    "plt.ylim([-0.1, 1.0])\n",
    "plt.axvline(0, color='k', linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show\n",
    "plt.savefig(figDir + monkey + '_cdfb_dir_r2_reachbump_bump.pdf', dpi='figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active and Passive trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_range = (-300, 1000)\n",
    "x_field = 'spikes'\n",
    "mask = active_mask\n",
    "n_trials = active_n_trials\n",
    "cond_dict = active_cond_dict\n",
    "dim = dataset_50ms.data[x_field].shape[1]\n",
    "active_df = dataset_50ms.make_trial_data(align_field='move_onset_time', align_range=pred_range, ignored_trials=~mask)\n",
    "dirs = [dataset_50ms.trial_info[dataset_50ms.trial_info.trial_id == i]['cond_dir'] for i in active_df.trial_id]\n",
    "cos_x = [round(math.cos(math.radians(i)),3) for i in dirs]\n",
    "sin_y = [round(math.sin(math.radians(i)),3) for i in dirs]\n",
    "cos_sin = np.array([cos_x, sin_y]).T\n",
    "act_trial_PCA = active_df[x_field].to_numpy().reshape((n_trials, -1, dim))\n",
    "print(act_trial_PCA.shape)\n",
    "act_trial_ang = cos_sin.reshape((n_trials, -1, 2))\n",
    "# act_trial_ang = np.array(cos_x).reshape((n_trials,-1,1))\n",
    "# act_trial_ang = np.array(sin_y).reshape((n_trials,-1,1))\n",
    "\n",
    "print(act_trial_ang.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # decoders angle\n",
    "# dataset = dataset_50ms\n",
    "# x_field = 'PCA_40'\n",
    "# y_field ='hand_acc'\n",
    "\n",
    "# lag_axis = np.arange(-200,1000,50)+50\n",
    "# lag_axis = np.arange(-200,1000,50)+50\n",
    "\n",
    "# act_t_label = lag_axis\n",
    "# act_X_coef_array = act_coefs_arr[:,0,:]\n",
    "# angDist_array = nans([len(act_t_label),len(act_t_label)])\n",
    "\n",
    "# for i in range(len(act_t_label)):\n",
    "#     for j in range(len(act_t_label)):\n",
    "#         angDist_array[i,j] = math.degrees(angle_between(act_X_coef_array[i,:],act_X_coef_array[j,:]))\n",
    "# fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# im = ax.imshow(angDist_array)\n",
    "# ax.set_xlabel('Active time (ms)')\n",
    "# ax.set_ylabel('Active time (ms)')\n",
    "\n",
    "# ax.set_xticks(np.arange(len(act_t_label)))\n",
    "# ax.set_yticks(np.arange(len(act_t_label)))\n",
    "# ax.set_xticklabels(labels=act_t_label,size=8)\n",
    "# ax.set_yticklabels(labels=act_t_label,size=8)\n",
    "\n",
    "# ax.set_title(\"Angle between CD+FB weights\")\n",
    "\n",
    "# for i in range(len(act_t_label)):\n",
    "#     for j in range(len(act_t_label)):\n",
    "#         text = ax.text(j, i, str(int(angDist_array[i, j])),\n",
    "#                         ha=\"center\", va=\"center\", color=\"w\", fontsize=14)\n",
    "# plt.tight_layout()\n",
    "# # figDir = '/Users/sherryan/area2_population_analysis/figures_plus/'\n",
    "# # plt.savefig(figDir + monkey + '_decoder_angle.pdf', dpi = 'figure')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = act_trial_PCA.shape[1]\n",
    "n_splits = 20\n",
    "act_coefs_arr = nans([n_bins, 2, dim])\n",
    "act_offset_arr = nans([n_bins, 2])\n",
    "act_r2_arr = nans([n_bins, n_splits])\n",
    "act_r2_xy_arr = nans([n_bins,2, n_splits])\n",
    "\n",
    "for i in range(n_bins):\n",
    "    X = act_trial_PCA[:,i,:]\n",
    "    Y = act_trial_ang[:,i,:]\n",
    "    std=np.nanstd(X,axis=0)  \n",
    "    std[std==0] = 1\n",
    "    X_proc = (X - np.nanmean(X,axis=0))/std\n",
    "    lr_all = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "    lr_all.fit(X_proc, Y)\n",
    "    act_coefs_arr[i,:,:] = lr_all.best_estimator_.coef_\n",
    "    act_offset_arr[i,:] = lr_all.best_estimator_.intercept_\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits)\n",
    "    # true_concat = nans([n_trials,2])\n",
    "    # pred_concat = nans([n_trials,2])\n",
    "    # trial_save_idx = 0\n",
    "    for j, (training_set, test_set) in enumerate(sss.split(range(0,n_trials),cond_dict)):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test, y_train, y_test = procX_train_test(X,Y,training_set,test_set)\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_test)\n",
    "        # Separate R² for each dimension (x and y)\n",
    "        r2_x = 1 - np.sum((y_test[:, 0] - y_pred[:, 0]) ** 2) / np.sum((y_test[:, 0] - np.mean(y_test[:, 0])) ** 2)\n",
    "        r2_y = 1 - np.sum((y_test[:, 1] - y_pred[:, 1]) ** 2) / np.sum((y_test[:, 1] - np.mean(y_test[:, 1])) ** 2)\n",
    "        act_r2_xy_arr[i,:,j] = [r2_x, r2_y]\n",
    "\n",
    "        # Combined R² over both components\n",
    "        ss_res_combined = np.sum((y_test - y_pred) ** 2)\n",
    "        ss_tot_combined = np.sum((y_test - np.mean(y_test, axis=0)) ** 2)\n",
    "        r2_combined = 1 - ss_res_combined / ss_tot_combined\n",
    "        act_r2_arr[i,j] = r2_combined\n",
    "        # n = y_test_predicted.shape[0]\n",
    "        # true_concat[trial_save_idx:trial_save_idx+n,:] = y_test\n",
    "        # pred_concat[trial_save_idx:trial_save_idx+n,:] = y_test_predicted\n",
    "        # trial_save_idx += n\n",
    "    # for j in range(true_concat.shape[1]):\n",
    "    #     sses =get_sses_pred(true_concat[:,j],pred_concat[:,j])\n",
    "    #     sses_mean=get_sses_mean(true_concat[:,j])\n",
    "    #     act_r2_xy_arr[i,j] =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "    # sses =get_sses_pred(true_concat,pred_concat)\n",
    "    # sses_mean=get_sses_mean(true_concat)\n",
    "    # R2 =1-np.sum(sses)/np.sum(sses_mean)     \n",
    "    # act_r2_arr[i] = R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = act_trial_PCA.shape[1]\n",
    "np.random.shuffle(act_trial_ang)\n",
    "act_shuffle_r2_arr = nans([n_bins, n_splits])\n",
    "for i in range(n_bins):\n",
    "    X = act_trial_PCA[:,i,:]\n",
    "    Y = act_trial_ang[:,i,:]\n",
    "    std=np.nanstd(X,axis=0)  \n",
    "    std[std==0] = 1\n",
    "    X_proc = (X - np.nanmean(X,axis=0))/std\n",
    "    lr_all = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "    lr_all.fit(X_proc, Y)\n",
    "    act_coefs_arr[i,:,:] = lr_all.best_estimator_.coef_\n",
    "    act_offset_arr[i,:] = lr_all.best_estimator_.intercept_\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits)\n",
    "    for j, (training_set, test_set) in enumerate(sss.split(range(0,n_trials),cond_dict)):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test, y_train, y_test = procX_train_test(X,Y,training_set,test_set)\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_test)\n",
    "        # Combined R² over both components\n",
    "        ss_res_combined = np.sum((y_test - y_pred) ** 2)\n",
    "        ss_tot_combined = np.sum((y_test - np.mean(y_test, axis=0)) ** 2)\n",
    "        r2_combined = 1 - ss_res_combined / ss_tot_combined\n",
    "        act_shuffle_r2_arr[i,j] = r2_combined\n",
    "\n",
    "# for i in range(n_bins):\n",
    "#     X = act_trial_PCA[:,i,:]\n",
    "#     Y = act_trial_ang[:,i,:]\n",
    "#     std=np.nanstd(X,axis=0)  \n",
    "#     std[std==0] = 1\n",
    "#     X_proc = (X - np.nanmean(X,axis=0))/std\n",
    "#     lr_all = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "#     lr_all.fit(X_proc, Y)\n",
    "#     kf = KFold(n_splits=5,shuffle=True,random_state = 42)\n",
    "#     true_concat = nans([n_trials,2])\n",
    "#     pred_concat = nans([n_trials,2])\n",
    "#     trial_save_idx = 0\n",
    "#     for training_set, test_set in kf.split(range(0,n_trials)):\n",
    "#         #split training and testing by trials\n",
    "#         X_train, X_test, y_train, y_test = procX_train_test(X,Y,training_set,test_set)\n",
    "#         lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "#         lr.fit(X_train, y_train)\n",
    "#         y_test_predicted = lr.predict(X_test)\n",
    "#         n = y_test_predicted.shape[0]\n",
    "#         true_concat[trial_save_idx:trial_save_idx+n,:] = y_test\n",
    "#         pred_concat[trial_save_idx:trial_save_idx+n,:] = y_test_predicted\n",
    "#         trial_save_idx += n\n",
    "#     sses =get_sses_pred(true_concat,pred_concat)\n",
    "#     sses_mean=get_sses_mean(true_concat)\n",
    "#     R2 =1-np.sum(sses)/np.sum(sses_mean)     \n",
    "#     act_shuffle_r2_arr[i] = R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_range = (-300, 1000)\n",
    "mask = passive_mask\n",
    "n_trials = passive_n_trials\n",
    "cond_dict = passive_cond_dict\n",
    "passive_df = dataset_50ms.make_trial_data(align_field='move_onset_time', align_range=pred_range, ignored_trials=~mask)\n",
    "dirs = [dataset_50ms.trial_info[dataset_50ms.trial_info.trial_id == i]['cond_dir'] for i in passive_df.trial_id]\n",
    "cos_x = [round(math.cos(math.radians(i)),3) for i in dirs]\n",
    "sin_y = [round(math.sin(math.radians(i)),3) for i in dirs]\n",
    "cos_sin = np.array([cos_x, sin_y]).T\n",
    "pas_trial_PCA = passive_df[x_field].to_numpy().reshape((n_trials, -1, dim))\n",
    "print(pas_trial_PCA.shape)\n",
    "pas_trial_ang = cos_sin.reshape((n_trials, -1, 2))\n",
    "# pas_trial_ang = np.array(cos_x).reshape((n_trials,-1,1))\n",
    "# pas_trial_ang = np.array(sin_y).reshape((n_trials,-1,1))\n",
    "\n",
    "print(pas_trial_ang.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas_coefs_arr = nans([n_bins, 2, dim])\n",
    "pas_offset_arr = nans([n_bins, 2])\n",
    "pas_r2_arr = nans([n_bins,n_splits])\n",
    "pas_r2_xy_arr = nans([n_bins,2,n_splits])\n",
    "\n",
    "for i in range(n_bins):\n",
    "    X = pas_trial_PCA[:,i,:]\n",
    "    Y = pas_trial_ang[:,i,:]\n",
    "    std=np.nanstd(X,axis=0)  \n",
    "    std[std==0] = 1\n",
    "    X_proc = (X - np.nanmean(X,axis=0))/std\n",
    "    lr_all = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "    lr_all.fit(X_proc, Y)\n",
    "    pas_coefs_arr[i,:,:] = lr_all.best_estimator_.coef_\n",
    "    pas_offset_arr[i,:] = lr_all.best_estimator_.intercept_\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits)\n",
    "    for j, (training_set, test_set) in enumerate(sss.split(range(0,n_trials),cond_dict)):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test, y_train, y_test = procX_train_test(X,Y,training_set,test_set)\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_test)\n",
    "        # Separate R² for each dimension (x and y)\n",
    "        r2_x = 1 - np.sum((y_test[:, 0] - y_pred[:, 0]) ** 2) / np.sum((y_test[:, 0] - np.mean(y_test[:, 0])) ** 2)\n",
    "        r2_y = 1 - np.sum((y_test[:, 1] - y_pred[:, 1]) ** 2) / np.sum((y_test[:, 1] - np.mean(y_test[:, 1])) ** 2)\n",
    "        pas_r2_xy_arr[i,:,j] = [r2_x, r2_y]\n",
    "        # Combined R² over both components\n",
    "        ss_res_combined = np.sum((y_test - y_pred) ** 2)\n",
    "        ss_tot_combined = np.sum((y_test - np.mean(y_test, axis=0)) ** 2)\n",
    "        r2_combined = 1 - ss_res_combined / ss_tot_combined\n",
    "        pas_r2_arr[i,j] = r2_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.shuffle(pas_trial_ang)\n",
    "# pas_shuffle_r2_arr = nans([n_bins])\n",
    "# for i in range(n_bins):\n",
    "#     X = pas_trial_PCA[:,i,:]\n",
    "#     Y = pas_trial_ang[:,i,:]\n",
    "#     X_proc = (X - np.nanmean(X,axis=0))/np.nanstd(X,axis=0)\n",
    "#     lr_all = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)})\n",
    "#     lr_all.fit(X_proc, Y)\n",
    "#     kf = KFold(n_splits=5,shuffle=True,random_state = 42)\n",
    "#     true_concat = nans([n_trials,2])\n",
    "#     pred_concat = nans([n_trials,2])\n",
    "#     trial_save_idx = 0\n",
    "#     for training_set, test_set in kf.split(range(0,n_trials)):\n",
    "#         #split training and testing by trials\n",
    "#         X_train, X_test, y_train, y_test = procX_train_test(X,Y,training_set,test_set)\n",
    "#         lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-3, 3, 7)}) \n",
    "#         lr.fit(X_train, y_train)\n",
    "#         y_test_predicted = lr.predict(X_test)\n",
    "\n",
    "#         n = y_test_predicted.shape[0]\n",
    "#         true_concat[trial_save_idx:trial_save_idx+n,:] = y_test\n",
    "#         pred_concat[trial_save_idx:trial_save_idx+n,:] = y_test_predicted\n",
    "#         trial_save_idx += n\n",
    "\n",
    "#     sses =get_sses_pred(true_concat,pred_concat)\n",
    "#     sses_mean=get_sses_mean(true_concat)\n",
    "#     R2 =1-np.sum(sses)/np.sum(sses_mean)     \n",
    "#     pas_shuffle_r2_arr[i] = R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd_act_r2 = act_r2_arr\n",
    "# cd_pas_r2 = pas_r2_arr\n",
    "# fb_act_r2 = act_r2_arr\n",
    "# fb_pas_r2 = pas_r2_arr\n",
    "# cd_fb_act_r2 = act_r2_arr\n",
    "# cd_fb_pas_r2 = pas_r2_arr\n",
    "# neural_pc_act_r2 = act_r2_arr\n",
    "# neural_pc_pas_r2 = pas_r2_arr\n",
    "\n",
    "# neural_spikes_act_r2 = act_r2_arr\n",
    "# neural_spikes_pas_r2 = pas_r2_arr\n",
    "# emg_act_r2 = act_r2_arr\n",
    "# emg_pas_r2 = pas_r2_arr\n",
    "# muscle_pc_act_r2 = act_r2_arr\n",
    "# muscle_pc_pas_r2 = pas_r2_arr\n",
    "# joint_pc_act_r2 = act_r2_arr\n",
    "# joint_pc_pas_r2 = pas_r2_arr\n",
    "\n",
    "\n",
    "# np.savez(monkey+'_v6_cdfb_dir_r2_unsmoothed100', \n",
    "#          cd_act_r2 = cd_act_r2, cd_pas_r2 = cd_pas_r2, \\\n",
    "#          fb_act_r2 = fb_act_r2,fb_pas_r2 = fb_pas_r2, \\\n",
    "#          cd_fb_act_r2 = cd_fb_act_r2,cd_fb_pas_r2 = cd_fb_pas_r2,\\\n",
    "#          neural_pc_act_r2 = neural_pc_act_r2, neural_pc_pas_r2 = neural_pc_pas_r2)\n",
    "\n",
    "np.savez(monkey+'_spikes_dir_r2_unsmoothed', \n",
    "         act_r2_arr = act_r2_arr, pas_r2_arr = pas_r2_arr, act_shuffle_r2_arr=act_shuffle_r2_arr)\n",
    "\n",
    "\n",
    "        #  neural_pc_act_r2 = neural_pc_act_r2, neural_pc_pas_r2 = neural_pc_pas_r2)\n",
    "        #  neural_spikes_act_r2 = neural_spikes_act_r2, neural_spikes_pas_r2 = neural_spikes_pas_r2,\\\n",
    "        #  emg_act_r2 = emg_act_r2, emg_pas_r2 = emg_pas_r2,\\\n",
    "        #  muscle_pc_act_r2 = act_r2_arr, muscle_pc_pas_r2 = pas_r2_arr,\\\n",
    "        #  joint_pc_act_r2 = act_r2_arr, joint_pc_pas_r2 = pas_r2_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monkey = \"Han_20171207\"\n",
    "monkey = 'Duncan_20190710'\n",
    "\n",
    "data = np.load(monkey+'_v6_cdfb_dir_r2_unsmoothed100.npz')\n",
    "data.files\n",
    "cd_act_r2 = data['cd_act_r2'] \n",
    "cd_pas_r2=data['cd_pas_r2'] \n",
    "fb_act_r2=data['fb_act_r2'] \n",
    "fb_pas_r2=data['fb_pas_r2'] \n",
    "cd_fb_act_r2=data['cd_fb_act_r2'] \n",
    "cd_fb_pas_r2=data['cd_fb_pas_r2'] \n",
    "data = np.load(monkey+'_spikes_dir_r2_unsmoothed.npz')\n",
    "neural_spikes_act_r2=data['act_r2_arr'] \n",
    "neural_spikes_pas_r2=data['pas_r2_arr'] \n",
    "shuffle_r2 = data['act_shuffle_r2_arr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_axis = np.arange(-300, 1000, 50) + 25\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# Plot function with mean and shaded std\n",
    "def plot_with_shade(x, data, color, label, alpha=1, linestyle='-'):\n",
    "    mean = np.mean(data, axis=1)\n",
    "    std = np.std(data, axis=1)\n",
    "    ax.plot(x, mean, linestyle + 'o', color=color, alpha = alpha, label=label)\n",
    "    ax.fill_between(x, mean - std, mean + std, color=color, alpha=0.3)\n",
    "\n",
    "# Plotting each dataset\n",
    "plot_with_shade(lag_axis, cd_act_r2, 'green', 'CD')\n",
    "plot_with_shade(lag_axis, fb_act_r2, 'magenta', 'FB')\n",
    "plot_with_shade(lag_axis, cd_fb_act_r2, 'brown', 'CD_FB')\n",
    "plot_with_shade(lag_axis, neural_spikes_act_r2, 'grey', 'Neural', alpha=0.3,linestyle='--')\n",
    "plot_with_shade(lag_axis, shuffle_r2, 'k', 'shuffle', alpha=0.1,linestyle='--')\n",
    "\n",
    "\n",
    "# Axes labels and aesthetics\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "plt.ylabel('R²')\n",
    "plt.legend(fontsize=8)\n",
    "plt.xlim([-210, 1010])\n",
    "plt.ylim([-0.1, 1.0])\n",
    "plt.axvline(0, color='k', linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show\n",
    "plt.savefig(figDir + monkey + '_cdfb_dir_r2_active.pdf', dpi='figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_axis = np.arange(-300, 1000, 50) + 25\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# Plotting each dataset\n",
    "plot_with_shade(lag_axis, cd_pas_r2, 'green', 'CD')\n",
    "plot_with_shade(lag_axis, fb_pas_r2, 'magenta', 'FB')\n",
    "plot_with_shade(lag_axis, cd_fb_pas_r2, 'brown', 'CD_FB')\n",
    "plot_with_shade(lag_axis, neural_spikes_pas_r2, 'grey', 'Neural', alpha=0.3,linestyle='--')\n",
    "plot_with_shade(lag_axis, shuffle_r2, 'k', 'shuffle', alpha=0.1,linestyle='--')\n",
    "\n",
    "\n",
    "# Axes labels and aesthetics\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "plt.ylabel('R²')\n",
    "plt.legend(fontsize=8)\n",
    "plt.xlim([-210, 1010])\n",
    "plt.ylim([-0.1, 1.0])\n",
    "plt.axvline(0, color='k', linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show\n",
    "plt.savefig(figDir + monkey + '_cdfb_dir_r2_passive.pdf', dpi='figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_pc_act_r2 = act_r2_arr\n",
    "# neural_pc_pas_r2 = pas_r2_arr\n",
    "# neural_pc_act_xy_r2 = act_r2_xy_arr\n",
    "# neural_pc_pas_xy_r2 = pas_r2_xy_arr\n",
    "# vel_act_r2 = act_r2_arr\n",
    "# vel_pas_r2 = pas_r2_arr\n",
    "# vel_act_xy_r2 = act_r2_xy_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_axis = np.arange(-300,1000,50)+25\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(lag_axis,neural_spikes_act_r2, \"o\",color = 'k', label = 'Active',markersize=10)\n",
    "plt.plot(lag_axis,neural_spikes_pas_r2, \"o\",color = 'red', label = 'Passive',markersize=10)\n",
    "\n",
    "# plt.plot(lag_axis,neural_pc_act_r2, \"-o\",color = 'k', label = 'Active, 20PC')\n",
    "# plt.plot(lag_axis,neural_pc_act_xy_r2[:,0], \"-o\",label = 'Active, 20PC,x-dir')\n",
    "# plt.plot(lag_axis,neural_pc_act_xy_r2[:,1], \"-o\", label = 'Active, 20PC,y-dir')\n",
    "# plt.plot(lag_axis,vel_act_r2,\"-o\", color = 'gray',label = 'Active, velocity')\n",
    "# plt.plot(lag_axis,vel_act_xy_r2[:,0], \"-o\",color = 'gray',label = 'Active, velocity,x-dir')\n",
    "# plt.plot(lag_axis,vel_act_xy_r2[:,1], \"-o\", color = 'gray',label = 'Active, velocity,y-dir')\n",
    "\n",
    "# plt.plot(lag_axis,neural_pc_pas_r2,\"-o\", color = 'red', label = 'Passive, 20PC')\n",
    "# plt.plot(lag_axis,pas_shuffle_r2_arr, color = 'red', ls='--',label = 'Passive shuffle')\n",
    "# plt.plot(lag_axis,vel_pas_r2,\"-o\", color = 'gray',label = 'Passive, velocity')\n",
    "\n",
    "plt.plot(lag_axis,act_shuffle_r2_arr,  \"-\",color = 'grey', label = 'shuffle')\n",
    "\n",
    "plt.xlabel('Time after movement onset (ms)'); plt.ylabel('Cross-validated R$^2$'); \n",
    "# plt.title('Direction r2')\n",
    "# plt.legend(fontsize=8)\n",
    "plt.xlim([-300, 610])\n",
    "plt.ylim([-0.1, 1])\n",
    "\n",
    "plt.axvline(0, color = 'k',linestyle = '--')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_direction_r2.pdf',dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-lag decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_10ms\n",
    "dataset.data.keys().unique(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'spikes'\n",
    "data = np.load(monkey+'_v6_smoothed100_cdfb_data_'+x_field+'.npz')\n",
    "data.files\n",
    "dataset.add_continuous_data(data['CD_FB_proj'],'CD_FB_proj')\n",
    "dataset.add_continuous_data(data['CD_proj'],'CD_proj')\n",
    "dataset.add_continuous_data(data['FB_proj'],'FB_proj')\n",
    "dataset.data.keys().unique(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data.CD_proj.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'spikes'\n",
    "data = np.load(monkey+'_v6_qsignal_smoothed100_cdfb_data_'+x_field+'.npz')\n",
    "data.files\n",
    "dataset.add_continuous_data(data['CD_FB_proj'],'CD_FB_proj')\n",
    "dataset.add_continuous_data(data['CD_proj'],'CD_proj')\n",
    "dataset.add_continuous_data(data['FB_proj'],'FB_proj')\n",
    "dataset.add_continuous_data(data['FBq_proj'],'FBq_proj')\n",
    "dataset.data.keys().unique(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = (False, 180.0)\n",
    "cond_mask = (dataset_10ms.trial_info['ctr_hold_bump'] == cond[0]) & \\\n",
    "            (dataset_10ms.trial_info['cond_dir']%360==cond[1]) & \\\n",
    "            (dataset_10ms.trial_info.split != 'none')\n",
    "align_range = (-200, 1000)\n",
    "cond_data = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=align_range, ignored_trials=~cond_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_x, cd_y, fb_x, fb_y, vx, vy, neural = [],[],[],[],[],[],[]\n",
    "for idx, trial in cond_data.groupby('trial_id'):\n",
    "    neural.append(trial['spikes_smth_40'].to_numpy()[:,:])\n",
    "    cd_x.append(trial['CD_proj'].to_numpy()[:,:2])\n",
    "    cd_y.append(trial['CD_proj'].to_numpy()[:,2])\n",
    "    fb_x.append(trial['FB_proj'].to_numpy()[:,:4])\n",
    "    fb_y.append(trial['FB_proj'].to_numpy()[:,4:])\n",
    "    vx.append(trial['hand_vel'].to_numpy()[:,0])\n",
    "    vy.append(trial['hand_vel'].to_numpy()[:,1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.arctan2(np.array(vy), np.array(vx))           \n",
    "cos_theta = np.cos(theta)           \n",
    "sin_theta = np.sin(theta)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_axis = np.arange(align_range[0],align_range[1],dataset_10ms.bin_width)\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "neural_var = np.sum(np.var(np.array(neural),axis=0),axis=1)\n",
    "\n",
    "# plt.plot(t_axis,np.sum(np.var(np.array(cd_x),axis=0),axis=1),color='green', label='CD_x')\n",
    "# plt.plot(t_axis,np.var(np.array(cd_y),axis=0),color='k',label='CD_y')\n",
    "# plt.plot(t_axis,np.sum(np.var(np.array(fb_x),axis=0),axis=1),color='magenta', label='FB_x')\n",
    "# plt.plot(t_axis,np.sum(np.var(np.array(fb_y),axis=0),axis=1),color='grey',label='FB_y')\n",
    "\n",
    "plt.plot(t_axis,np.sum(np.var(np.array(cd_x),axis=0),axis=1)/neural_var,color='green', label='CD_x')\n",
    "plt.plot(t_axis,np.var(np.array(cd_y),axis=0)/neural_var,color='k',label='CD_y')\n",
    "plt.plot(t_axis,np.sum(np.var(np.array(fb_x),axis=0),axis=1)/neural_var,color='magenta', label='FB_x')\n",
    "plt.plot(t_axis,np.sum(np.var(np.array(fb_y),axis=0),axis=1)/neural_var,color='grey',label='FB_y')\n",
    "# plt.plot(t_axis,neural_var,color='red',label='neural')\n",
    "\n",
    "# plt.plot(t_axis,np.var(np.array(vel_x),axis=0),color='blue', label='vel_x')\n",
    "# plt.plot(t_axis,np.var(np.array(vel_y),axis=0),color='yellow',label='vel_y')\n",
    "plt.legend()\n",
    "plt.title('180 deg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.plot(t_axis,np.var(np.array(vx),axis=0),color='blue', label='vel_x')\n",
    "plt.plot(t_axis,np.var(np.array(vy),axis=0),color='yellow',label='vel_y')\n",
    "plt.legend()\n",
    "plt.title('270 deg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State space plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 16 conditions, in the format (ctr_hold_bump, cond_dir)\n",
    "active_unique_conditions = [(False, 0.0), (False, 45.0), (False, 90.0), (False, 135.0),\n",
    "                     (False, 180.0), (False, 225.0), (False, 270.0), (False, 315.0)]\n",
    "passive_unique_conditions = [(True, 0.0), (True, 45.0), (True, 90.0), (True, 135.0),\n",
    "                     (True, 180.0), (True, 225.0), (True, 270.0), (True, 315.0)]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax_0 = fig.add_subplot(221) \n",
    "ax_1 = fig.add_subplot(222, projection='3d')\n",
    "\n",
    "var1_name = ['FB_proj']\n",
    "indices = [0,1,3]\n",
    "align_range = (-100, 200)\n",
    "t0_index = np.argwhere(np.arange(align_range[0], align_range[1],dataset.bin_width)==0)[0,0]\n",
    "\n",
    "var_name = ['hand_vel']\n",
    "for cond in active_unique_conditions:\n",
    "    # Filter out invalid trials (labeled 'none') and trials in other conditions\n",
    "    cond_mask = (dataset.trial_info['ctr_hold_bump'] == cond[0]) & \\\n",
    "                (dataset.trial_info['cond_dir']%360==cond[1]) & \\\n",
    "                (dataset.trial_info.split != 'none')\n",
    "\n",
    "    # Extract relevant portion of selected trials\n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=align_range, ignored_trials=~cond_mask)\n",
    "    all_x, all_y = [], []\n",
    "    for idx, trial in cond_data.groupby('trial_id'):\n",
    "        all_x.append(trial[var_name].to_numpy()[:,0])\n",
    "        all_y.append(trial[var_name].to_numpy()[:,1])\n",
    "    avg_x = np.mean(np.vstack(all_x), axis=0)\n",
    "    avg_y = np.mean(np.vstack(all_y), axis=0)\n",
    "\n",
    "    ax_0.plot(avg_x, avg_y,color=plt.cm.hsv(cond[1] / 360), linewidth=1.5)\n",
    "    ax_0.scatter(avg_x[t0_index], avg_y[t0_index],  color=plt.cm.hsv(cond[1] / 360), edgecolor='black', s=80, zorder=3)\n",
    "    arrow_idx = -5\n",
    "    ax_0.quiver(avg_x[arrow_idx], avg_y[arrow_idx], \n",
    "        avg_x[arrow_idx+1] - avg_x[arrow_idx], \n",
    "        avg_y[arrow_idx+1] - avg_y[arrow_idx], \n",
    "        angles='xy', scale_units='xy', scale=1, color=plt.cm.hsv(cond[1] / 360), linewidth=3)\n",
    "        \n",
    "for cond in active_unique_conditions:\n",
    "    # Filter out invalid trials (labeled 'none') and trials in other conditions\n",
    "    cond_mask = (dataset.trial_info['ctr_hold_bump'] == cond[0]) & \\\n",
    "                (dataset.trial_info['cond_dir']%360==cond[1]) & \\\n",
    "                (dataset.trial_info.split != 'none')\n",
    "    \n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=align_range, ignored_trials=~cond_mask)\n",
    "    \n",
    "    all_x, all_y, all_z = [], [], []\n",
    "    for idx, trial in cond_data.groupby('trial_id'):\n",
    "        all_x.append(trial[var1_name].to_numpy()[:,indices[0]])\n",
    "        all_y.append(trial[var1_name].to_numpy()[:,indices[1]])\n",
    "        all_z.append(trial[var1_name].to_numpy()[:,indices[2]])\n",
    "    \n",
    "    avg_x = np.mean(np.vstack(all_x), axis=0)\n",
    "    avg_y = np.mean(np.vstack(all_y), axis=0)\n",
    "    avg_z = np.mean(np.vstack(all_z), axis=0)\n",
    "        \n",
    "    ax_1.plot(avg_x, avg_y, avg_z, color=plt.cm.hsv(cond[1] / 360), linewidth=1.5)\n",
    "    ax_1.scatter(avg_x[t0_index], avg_y[t0_index], avg_z[t0_index], color=plt.cm.hsv(cond[1] / 360), edgecolor='black', s=80, zorder=3)\n",
    "    arrow_idx = -5\n",
    "    ax_1.quiver(avg_x[arrow_idx], avg_y[arrow_idx], avg_z[arrow_idx], \n",
    "        avg_x[arrow_idx+1] - avg_x[arrow_idx], \n",
    "        avg_y[arrow_idx+1] - avg_y[arrow_idx], \n",
    "        avg_z[arrow_idx+1] - avg_z[arrow_idx], \n",
    "        color=plt.cm.hsv(cond[1] / 360), linewidth=3, arrow_length_ratio=1)\n",
    "\n",
    "# ax_2 = fig.add_subplot(223) \n",
    "ax_2 = fig.add_subplot(223, sharex=ax_0, sharey=ax_0) \n",
    "ax_3 = fig.add_subplot(224, projection='3d')\n",
    "\n",
    "align_range = (-100, 120)\n",
    "t0_index = np.argwhere(np.arange(align_range[0], align_range[1],dataset.bin_width)==0)[0,0]\n",
    "\n",
    "for cond in passive_unique_conditions:\n",
    "    # Filter out invalid trials (labeled 'none') and trials in other conditions\n",
    "    cond_mask = (dataset.trial_info['ctr_hold_bump'] == cond[0]) & \\\n",
    "                (dataset.trial_info['cond_dir']%360==cond[1]) & \\\n",
    "                (dataset.trial_info.split != 'none')\n",
    "\n",
    "    # Extract relevant portion of selected trials\n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=align_range, ignored_trials=~cond_mask)\n",
    "    all_x, all_y = [], []\n",
    "    for idx, trial in cond_data.groupby('trial_id'):\n",
    "        all_x.append(trial[var_name].to_numpy()[:,0])\n",
    "        all_y.append(trial[var_name].to_numpy()[:,1])\n",
    "    avg_x = np.mean(np.vstack(all_x), axis=0)\n",
    "    avg_y = np.mean(np.vstack(all_y), axis=0)\n",
    "\n",
    "    ax_2.plot(avg_x, avg_y,color=plt.cm.hsv(cond[1] / 360), linewidth=1.5)\n",
    "    ax_2.scatter(avg_x[t0_index], avg_y[t0_index],  color=plt.cm.hsv(cond[1] / 360), edgecolor='black', s=80, zorder=3)\n",
    "    arrow_idx = -5\n",
    "    ax_2.quiver(avg_x[arrow_idx], avg_y[arrow_idx], \n",
    "        avg_x[arrow_idx+1] - avg_x[arrow_idx], \n",
    "        avg_y[arrow_idx+1] - avg_y[arrow_idx], \n",
    "        angles='xy', scale_units='xy', scale=1, color=plt.cm.hsv(cond[1] / 360), linewidth=3)\n",
    "\n",
    "for cond in passive_unique_conditions:\n",
    "    # Filter out invalid trials (labeled 'none') and trials in other conditions\n",
    "    cond_mask = (dataset.trial_info['ctr_hold_bump'] == cond[0]) & \\\n",
    "                (dataset.trial_info['cond_dir']%360==cond[1]) & \\\n",
    "                (dataset.trial_info.split != 'none')\n",
    "    \n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=align_range, ignored_trials=~cond_mask)\n",
    "    \n",
    "    all_x, all_y, all_z = [], [], []\n",
    "    for idx, trial in cond_data.groupby('trial_id'):\n",
    "        all_x.append(trial[var1_name].to_numpy()[:,indices[0]])\n",
    "        all_y.append(trial[var1_name].to_numpy()[:,indices[1]])\n",
    "        all_z.append(trial[var1_name].to_numpy()[:,indices[2]])\n",
    "    \n",
    "    avg_x = np.mean(np.vstack(all_x), axis=0)\n",
    "    avg_y = np.mean(np.vstack(all_y), axis=0)\n",
    "    avg_z = np.mean(np.vstack(all_z), axis=0)\n",
    "        \n",
    "    ax_3.plot(avg_x, avg_y, avg_z, color=plt.cm.hsv(cond[1] / 360), linewidth=1.5)\n",
    "    ax_3.scatter(avg_x[t0_index], avg_y[t0_index], avg_z[t0_index], color=plt.cm.hsv(cond[1] / 360), edgecolor='black', s=80, zorder=3)\n",
    "    arrow_idx = -5\n",
    "    ax_3.quiver(avg_x[arrow_idx], avg_y[arrow_idx], avg_z[arrow_idx], \n",
    "        avg_x[arrow_idx+1] - avg_x[arrow_idx], \n",
    "        avg_y[arrow_idx+1] - avg_y[arrow_idx], \n",
    "        avg_z[arrow_idx+1] - avg_z[arrow_idx], \n",
    "        color=plt.cm.hsv(cond[1] / 360), linewidth=3, arrow_length_ratio=1)\n",
    "x_min = min(ax_1.get_xlim()[0], ax_3.get_xlim()[0])\n",
    "x_max = max(ax_1.get_xlim()[1], ax_3.get_xlim()[1])\n",
    "y_min = min(ax_1.get_ylim()[0], ax_3.get_ylim()[0])\n",
    "y_max = max(ax_1.get_ylim()[1], ax_3.get_ylim()[1])\n",
    "z_min = min(ax_1.get_zlim()[0], ax_3.get_zlim()[0])\n",
    "z_max = max(ax_1.get_zlim()[1], ax_3.get_zlim()[1])\n",
    "\n",
    "# Apply the same limits to both 3D plots\n",
    "ax_1.set_xlim(x_min, x_max)\n",
    "ax_1.set_ylim(y_min, y_max)\n",
    "ax_1.set_zlim(z_min, z_max)\n",
    "\n",
    "ax_3.set_xlim(x_min, x_max)\n",
    "ax_3.set_ylim(y_min, y_max)\n",
    "ax_3.set_zlim(z_min, z_max)    \n",
    "    \n",
    "ax_1.set_xticks([]); ax_1.set_yticks([]); ax_1.set_zticks([]); \n",
    "ax_3.set_xticks([]); ax_3.set_yticks([]); ax_3.set_zticks([]); \n",
    "ax_0.set_xticks([]); ax_0.set_yticks([]); \n",
    "ax_2.set_xticks([]); ax_2.set_yticks([]);\n",
    "ax_0.grid(False); ax_1.grid(False);  ax_2.grid(False) ; ax_3.grid(False)\n",
    "# ax_1.set_xlabel('X')\n",
    "# ax_1.set_ylabel('Y')\n",
    "# ax_1.set_zlabel('Z')\n",
    "\n",
    "ax_0.spines['top'].set_visible(False); ax_0.spines['right'].set_visible(False); ax_0.spines['bottom'].set_visible(False); ax_0.spines['left'].set_visible(False)\n",
    "ax_2.spines['top'].set_visible(False); ax_2.spines['right'].set_visible(False); ax_2.spines['bottom'].set_visible(False); ax_2.spines['left'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_fb_traj_200120.pdf',dpi = 'figure')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_10ms\n",
    "dataset.data.keys().unique(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'spikes'\n",
    "data = np.load(monkey+'_v6_alt_unsmoothed100_cdfb_weights_'+x_field+'.npz')\n",
    "data.files\n",
    "data['CD_axes'].shape\n",
    "\n",
    "# data = np.load(monkey+'_v6_unsmoothed100_cdfb_weights_'+x_field+'.npz')\n",
    "# data.files\n",
    "# data['CD_axes'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dims = 3 \n",
    "all_data = np.array(dataset_10ms.data.spikes_smth_40)\n",
    "print(all_data.shape)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(all_data)\n",
    "pca = PCA(n_components=n_dims,random_state = 42)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project all neural activity data into PC space\n",
    "PCA_data = pca.transform(X)\n",
    "print(PCA_data.shape)\n",
    "dataset_10ms.add_continuous_data(PCA_data,'PCA')\n",
    "print('PCA total var explained:',sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = pca.transform(np.vstack([data['CD_axes'][2],data['CD_axes'][3]]))\n",
    "v1 /= np.linalg.norm(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2 = pca.transform(np.vstack([data['FB_axes'][0],data['FB_axes'][1]]))\n",
    "v2 /= np.linalg.norm(v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "%matplotlib widget\n",
    "\n",
    "active_unique_conditions = [(False, 0.0), (False, 45.0), (False, 90.0), (False, 135.0),\n",
    "                     (False, 180.0), (False, 225.0), (False, 270.0), (False, 315.0)]\n",
    "cmap = plt.get_cmap('coolwarm',len(active_unique_conditions))\n",
    "custom_palette = [mpl.colors.rgb2hex(cmap(i)) for i in range(len(active_unique_conditions))]\n",
    "\n",
    "align_range = (-100, 500)\n",
    "t0_index = np.argwhere(np.arange(align_range[0], align_range[1],dataset.bin_width)==0)[0,0]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "var_name = ['PCA']\n",
    "j=0\n",
    "for cond in active_unique_conditions:\n",
    "    # Filter out invalid trials (labeled 'none') and trials in other conditions\n",
    "    cond_mask = (dataset.trial_info['ctr_hold_bump'] == cond[0]) & \\\n",
    "                (dataset.trial_info['cond_dir']%360==cond[1]) & \\\n",
    "                (dataset.trial_info.split != 'none')\n",
    "    \n",
    "    cond_data = dataset.make_trial_data(align_field='move_onset_time', align_range=align_range, ignored_trials=~cond_mask)\n",
    "    \n",
    "    all_x, all_y, all_z = [], [], []\n",
    "    for idx, trial in cond_data.groupby('trial_id'):\n",
    "        all_x.append(trial[var_name].to_numpy()[:,0])\n",
    "        all_y.append(trial[var_name].to_numpy()[:,1])\n",
    "        all_z.append(trial[var_name].to_numpy()[:,2])\n",
    "    \n",
    "    avg_x = np.mean(np.vstack(all_x), axis=0)\n",
    "    avg_y = np.mean(np.vstack(all_y), axis=0)\n",
    "    avg_z = np.mean(np.vstack(all_z), axis=0)\n",
    "        \n",
    "    # Plot trajectories before and after time zero in different colors\n",
    "    ax.plot(avg_x[:t0_index], avg_y[:t0_index], avg_z[:t0_index], color=custom_palette[j], linewidth=1.5, label=f'Before Time Zero - {cond[1]}°')\n",
    "    ax.plot(avg_x[t0_index:], avg_y[t0_index:], avg_z[t0_index:], color=custom_palette[j], linewidth=1.5, linestyle='--', label=f'After Time Zero - {cond[1]}°')\n",
    "\n",
    "    # Scatter plot to mark the position at time zero\n",
    "    ax.scatter(avg_x[t0_index], avg_y[t0_index], avg_z[t0_index], color=custom_palette[j], s=40, zorder=3)\n",
    "    j+=1\n",
    "    \n",
    "u = np.linspace(-1, 1, 10)\n",
    "v = np.linspace(-1, 1, 10)\n",
    "U, V = np.meshgrid(u, v)\n",
    "\n",
    "# Calculate the range of the neural data\n",
    "min_x, max_x = np.min(avg_x), np.max(avg_x)\n",
    "min_y, max_y = np.min(avg_y), np.max(avg_y)\n",
    "min_z, max_z = np.min(avg_z), np.max(avg_z)\n",
    "\n",
    "# Scale factors (optional, adjust based on how you want the planes to appear)\n",
    "scale_factor_cd = [10,5,5]\n",
    "# scale_factor_cd = [10,10,10]\n",
    "trans_cd = [0,1,-2]\n",
    "# trans_cd = [0,0,0]\n",
    "\n",
    "scale_factor_fb = [10,10,10]\n",
    "trans_fb = [5,5,0]\n",
    "\n",
    "# Plot CD plane (scaled and translated)\n",
    "X_cd = U * v1[0,0] + V * v1[1,0]\n",
    "Y_cd = U * v1[0,1] + V * v1[1,1]\n",
    "Z_cd = U * v1[0,2] + V * v1[1,2]\n",
    "\n",
    "ax.plot_surface(X_cd * scale_factor_cd[0] + trans_cd[0], \n",
    "                Y_cd * scale_factor_cd[1] + trans_cd[1], \n",
    "                Z_cd * scale_factor_cd[2] + trans_cd[2], \n",
    "                color='green', alpha=0.2)   \n",
    "\n",
    "# Plot FB plane (scaled and translated)\n",
    "X_fb = U * v2[0,0] + V * v2[1,0]\n",
    "Y_fb = U * v2[0,1] + V * v2[1,1]\n",
    "Z_fb = U * v2[0,2] + V * v2[1,2]\n",
    "\n",
    "ax.plot_surface(X_fb * scale_factor_fb[0] + trans_fb[0], \n",
    "                Y_fb * scale_factor_fb[1] + trans_fb[1], \n",
    "                Z_fb * scale_factor_fb[2] + trans_fb[2], \n",
    "                color='magenta', alpha=0.2)\n",
    "# params = [1.558,-59.99]\n",
    "\n",
    "# params = [22.597402597402642, -82.20779220779221]\n",
    "params = [29.610389610389664, -99.35064935064926]\n",
    "ax.view_init(elev=params[0], azim=params[1])\n",
    "# Labels and title\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "ax.set_xlim(-5, 10)\n",
    "ax.set_ylim(-7, 7)\n",
    "ax.set_zlim(-7, 7)\n",
    "# Remove the grid\n",
    "ax.grid(False)\n",
    "\n",
    "# Remove the axis labels\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_zticks([])\n",
    "# ax.set_title('Neural Trajectories and CD Subspace')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# print(ax.elev, ax.azim) #in separate cell\n",
    "fig.savefig(figDir+monkey+\"_trajectory_3d_plot.pdf\", format=\"pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ax.elev, ax.azim) #in separate cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ax.elev, ax.azim) #in separate cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Straight decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'spikes_smth_40'\n",
    "y_field ='hand_vel'\n",
    "lag_axis = np.arange(-300,320,20)\n",
    "# pred_range = (-100, 1000)\n",
    "# trial_mask = active_mask\n",
    "# cond_dict = active_cond_dict\n",
    "pred_range = (-100, 120)\n",
    "trial_mask = passive_mask\n",
    "cond_dict = passive_cond_dict\n",
    "dataset=dataset_10ms\n",
    "dim = dataset.data[x_field].shape[1]\n",
    "print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 20\n",
    "r2_array_MC = nans([len(lag_axis),n_splits]); \n",
    "r2_feature_array = nans([len(lag_axis),n_splits,dataset.data[y_field].shape[1]])\n",
    "coef_array = nans([len(lag_axis),dataset.data[y_field].shape[1],dim])\n",
    "for i in range(len(lag_axis)):\n",
    "    lag = lag_axis[i]\n",
    "    print(int(i/len(lag_axis)*100),'%')\n",
    "    r2, coef,_,vel_df,r2_arr = fit_and_predict_MC(dataset, trial_mask, 'move_onset_time',pred_range, lag, x_field, y_field,cond_dict)\n",
    "    r2_array_MC[i,:] = r2; r2_feature_array[i,:,:] = r2_arr\n",
    "    coef_array[i,:,:] = coef\n",
    "time_max = lag_axis[np.argmax(np.mean(r2_array_MC,axis=1))]\n",
    "print(np.max(np.mean(r2_array_MC,axis=1)))\n",
    "print(time_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_r2_cd_only = r2_feature_array[:,:,0]\n",
    "# y_r2_cd_only = r2_feature_array[:,:,1]\n",
    "# r2_cd_only = r2_array_MC\n",
    "# tmax_cd_only = time_max\n",
    "\n",
    "# x_r2_fb_only = r2_feature_array[:,:,0]\n",
    "# y_r2_fb_only = r2_feature_array[:,:,1]\n",
    "# r2_fb_only = r2_array_MC\n",
    "# tmax_fb_only = time_max\n",
    "\n",
    "# x_r2_cd_fb = r2_feature_array[:,:,0]\n",
    "# y_r2_cd_fb = r2_feature_array[:,:,1]\n",
    "# r2_cd_fb = r2_array_MC\n",
    "# tmax_cd_fb = time_max\n",
    "\n",
    "x_r2_nrn = r2_feature_array[:,:,0]\n",
    "y_r2_nrn = r2_feature_array[:,:,1]\n",
    "r2_nrn = r2_array_MC\n",
    "tmax_nrn = time_max\n",
    "\n",
    "# x_r2_pc = r2_feature_array[:,:,0]\n",
    "# y_r2_pc = r2_feature_array[:,:,1]\n",
    "# r2_pc = r2_array_MC\n",
    "# tmax_pc = time_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(monkey+'_MC_smooth40_spikes_'+y_field +'_pas_r2s', \\\n",
    "         x_r2_cd_only = x_r2_cd_only, y_r2_cd_only = y_r2_cd_only, r2_cd_only = r2_cd_only, \\\n",
    "         x_r2_fb_only = x_r2_fb_only, y_r2_fb_only = y_r2_fb_only, r2_fb_only = r2_fb_only,\\\n",
    "         x_r2_cd_fb = x_r2_cd_fb, y_r2_cd_fb = y_r2_cd_fb, r2_cd_fb = r2_cd_fb,\\\n",
    "         x_r2_nrn = x_r2_nrn, y_r2_nrn = y_r2_nrn, r2_nrn = r2_nrn,\\\n",
    "         x_r2_pc = x_r2_pc, y_r2_pc = y_r2_pc, r2_pc = r2_pc)\n",
    "\n",
    "# monkey = \"Han_20171207\"\n",
    "# monkey = 'Duncan_20190710'\n",
    "\n",
    "data = np.load(monkey+'_MC_smooth40_spikes_hand_vel_pas_r2s.npz')\n",
    "data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw = 3\n",
    "lag_axis = np.arange(-300, 320, 20)\n",
    "fig, ax = plt.subplots(figsize=(5.5, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# CD only\n",
    "r2_cd = data['r2_cd_only']  # shape [n_lags, n_splits]\n",
    "mean_cd = np.nanmean(r2_cd, axis=1)\n",
    "std_cd = np.nanstd(r2_cd, axis=1)\n",
    "plt.plot(lag_axis, mean_cd, linewidth=lw, color='green', label='cd')\n",
    "plt.fill_between(lag_axis, mean_cd - std_cd, mean_cd + std_cd, color='green', alpha=0.3)\n",
    "\n",
    "print(np.nanmax(mean_cd))\n",
    "print(lag_axis[np.nanargmax(mean_cd)])\n",
    "\n",
    "# FB only\n",
    "r2_fb = data['r2_fb_only']\n",
    "mean_fb = np.nanmean(r2_fb, axis=1)\n",
    "std_fb = np.nanstd(r2_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_fb, linewidth=lw, color='magenta', label='fb')\n",
    "plt.fill_between(lag_axis, mean_fb - std_fb, mean_fb + std_fb, color='magenta', alpha=0.3)\n",
    "\n",
    "print(np.nanmax(mean_fb))\n",
    "print(lag_axis[np.nanargmax(mean_fb)])\n",
    "\n",
    "# CD + FB\n",
    "r2_cd_fb = data['r2_cd_fb']\n",
    "mean_cd_fb = np.nanmean(r2_cd_fb, axis=1)\n",
    "std_cd_fb = np.nanstd(r2_cd_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_cd_fb, linewidth=lw, color='brown', label='cd+fb')\n",
    "plt.fill_between(lag_axis, mean_cd_fb - std_cd_fb, mean_cd_fb + std_cd_fb, color='brown', alpha=0.3)\n",
    "print(np.nanmax(mean_cd_fb))\n",
    "print(lag_axis[np.nanargmax(mean_cd_fb)])\n",
    "\n",
    "# All nrns\n",
    "r2_nrn = data['r2_nrn']\n",
    "mean_nrn = np.nanmean(r2_nrn, axis=1)\n",
    "std_nrn = np.nanstd(r2_nrn, axis=1)\n",
    "plt.plot(lag_axis, mean_nrn, linewidth=lw, linestyle = '--',color='grey', label='neurons', alpha=0.3)\n",
    "plt.fill_between(lag_axis, mean_nrn - std_nrn, mean_nrn + std_nrn, color='grey', alpha=0.3)\n",
    "print(np.nanmax(mean_nrn))\n",
    "print(lag_axis[np.nanargmax(mean_nrn)])\n",
    "\n",
    "# 20 PC\n",
    "r2_pc = data['r2_pc']\n",
    "mean_pc = np.nanmean(r2_pc, axis=1)\n",
    "std_pc = np.nanstd(r2_pc, axis=1)\n",
    "plt.plot(lag_axis, mean_pc, linewidth=lw, linestyle = '--',color='lightgrey', label='PCs', alpha=0.5)\n",
    "plt.fill_between(lag_axis, mean_pc - std_pc, mean_pc + std_pc, color='lightgrey', alpha=0.5)\n",
    "print(np.nanmax(mean_pc))\n",
    "print(lag_axis[np.nanargmax(mean_pc)])\n",
    "plt.axvline(0, color = 'k', linestyle='--')\n",
    "\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R²')\n",
    "plt.ylim([-0.1, 0.85])\n",
    "# plt.legend(fontsize=8)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(figDir + monkey + '_passive_r2.pdf',dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw = 3\n",
    "lag_axis = np.arange(-300, 320, 20)\n",
    "fig, ax = plt.subplots(figsize=(5.5, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# CD only\n",
    "r2_cd = data['x_r2_cd_only']  # shape [n_lags, n_splits]\n",
    "mean_cd = np.nanmean(r2_cd, axis=1)\n",
    "std_cd = np.nanstd(r2_cd, axis=1)\n",
    "plt.plot(lag_axis, mean_cd, linewidth=lw, color='green', label='cd')\n",
    "plt.fill_between(lag_axis, mean_cd - std_cd, mean_cd + std_cd, color='green', alpha=0.3)\n",
    "\n",
    "print(np.nanmax(mean_cd))\n",
    "print(lag_axis[np.nanargmax(mean_cd)])\n",
    "\n",
    "# FB only\n",
    "r2_fb = data['x_r2_fb_only']\n",
    "mean_fb = np.nanmean(r2_fb, axis=1)\n",
    "std_fb = np.nanstd(r2_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_fb, linewidth=lw, color='magenta', label='fb')\n",
    "plt.fill_between(lag_axis, mean_fb - std_fb, mean_fb + std_fb, color='magenta', alpha=0.3)\n",
    "\n",
    "print(np.nanmax(mean_fb))\n",
    "print(lag_axis[np.nanargmax(mean_fb)])\n",
    "\n",
    "# CD + FB\n",
    "r2_cd_fb = data['x_r2_cd_fb']\n",
    "mean_cd_fb = np.nanmean(r2_cd_fb, axis=1)\n",
    "std_cd_fb = np.nanstd(r2_cd_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_cd_fb, linewidth=lw, color='brown', label='cd+fb')\n",
    "plt.fill_between(lag_axis, mean_cd_fb - std_cd_fb, mean_cd_fb + std_cd_fb, color='brown', alpha=0.3)\n",
    "\n",
    "print(np.nanmax(mean_cd_fb))\n",
    "print(lag_axis[np.nanargmax(mean_cd_fb)])\n",
    "\n",
    "\n",
    "# All nrns\n",
    "r2_nrn = data['x_r2_nrn']\n",
    "mean_nrn = np.nanmean(r2_nrn, axis=1)\n",
    "std_nrn = np.nanstd(r2_nrn, axis=1)\n",
    "plt.plot(lag_axis, mean_nrn, linewidth=lw, linestyle = '--',color='grey', label='neurons', alpha=0.3)\n",
    "plt.fill_between(lag_axis, mean_nrn - std_nrn, mean_nrn + std_nrn, color='grey', alpha=0.3)\n",
    "print(np.nanmax(mean_nrn))\n",
    "print(lag_axis[np.nanargmax(mean_nrn)])\n",
    "\n",
    "# 20 PC\n",
    "r2_pc = data['x_r2_pc']\n",
    "mean_pc = np.nanmean(r2_pc, axis=1)\n",
    "std_pc = np.nanstd(r2_pc, axis=1)\n",
    "plt.plot(lag_axis, mean_pc, linewidth=lw, linestyle = '--',color='lightgrey', label='PCs', alpha=0.5)\n",
    "plt.fill_between(lag_axis, mean_pc - std_pc, mean_pc + std_pc, color='lightgrey', alpha=0.5)\n",
    "print(np.nanmax(mean_pc))\n",
    "print(lag_axis[np.nanargmax(mean_pc)])\n",
    "plt.axvline(0, color = 'k', linestyle='--')\n",
    "plt.ylim([-0.1, 0.85])\n",
    "\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('X R²')\n",
    "# plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(figDir + monkey + '_passive_x_r2.pdf',dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw = 3\n",
    "lag_axis = np.arange(-300, 320, 20)\n",
    "fig, ax = plt.subplots(figsize=(5.5, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# CD only\n",
    "r2_cd = data['y_r2_cd_only']  # shape [n_lags, n_splits]\n",
    "mean_cd = np.nanmean(r2_cd, axis=1)\n",
    "std_cd = np.nanstd(r2_cd, axis=1)\n",
    "plt.plot(lag_axis, mean_cd, linewidth=lw, color='green', label='cd')\n",
    "plt.fill_between(lag_axis, mean_cd - std_cd, mean_cd + std_cd, color='green', alpha=0.3)\n",
    "\n",
    "print(np.nanmax(mean_cd))\n",
    "print(lag_axis[np.nanargmax(mean_cd)])\n",
    "\n",
    "# FB only\n",
    "r2_fb = data['y_r2_fb_only']\n",
    "mean_fb = np.nanmean(r2_fb, axis=1)\n",
    "std_fb = np.nanstd(r2_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_fb, linewidth=lw, color='magenta', label='fb')\n",
    "plt.fill_between(lag_axis, mean_fb - std_fb, mean_fb + std_fb, color='magenta', alpha=0.3)\n",
    "\n",
    "print(np.nanmax(mean_fb))\n",
    "print(lag_axis[np.nanargmax(mean_fb)])\n",
    "\n",
    "# CD + FB\n",
    "r2_cd_fb = data['y_r2_cd_fb']\n",
    "mean_cd_fb = np.nanmean(r2_cd_fb, axis=1)\n",
    "std_cd_fb = np.nanstd(r2_cd_fb, axis=1)\n",
    "plt.plot(lag_axis, mean_cd_fb, linewidth=lw, color='brown', label='cd+fb')\n",
    "plt.fill_between(lag_axis, mean_cd_fb - std_cd_fb, mean_cd_fb + std_cd_fb, color='brown', alpha=0.3)\n",
    "\n",
    "print(np.nanmax(mean_cd_fb))\n",
    "print(lag_axis[np.nanargmax(mean_cd_fb)])\n",
    "\n",
    "# All nrns\n",
    "r2_nrn = data['y_r2_nrn']\n",
    "mean_nrn = np.nanmean(r2_nrn, axis=1)\n",
    "std_nrn = np.nanstd(r2_nrn, axis=1)\n",
    "plt.plot(lag_axis, mean_nrn, linewidth=lw, linestyle = '--',color='grey', label='neurons', alpha=0.3)\n",
    "plt.fill_between(lag_axis, mean_nrn - std_nrn, mean_nrn + std_nrn, color='grey', alpha=0.3)\n",
    "print(np.nanmax(mean_nrn))\n",
    "print(lag_axis[np.nanargmax(mean_nrn)])\n",
    "\n",
    "# 20 PC\n",
    "r2_pc = data['y_r2_pc']\n",
    "mean_pc = np.nanmean(r2_pc, axis=1)\n",
    "std_pc = np.nanstd(r2_pc, axis=1)\n",
    "plt.plot(lag_axis, mean_pc, linewidth=lw, linestyle = '--',color='lightgrey', label='PCs', alpha=0.5)\n",
    "plt.fill_between(lag_axis, mean_pc - std_pc, mean_pc + std_pc, color='lightgrey', alpha=0.5)\n",
    "print(np.nanmax(mean_pc))\n",
    "print(lag_axis[np.nanargmax(mean_pc)])\n",
    "plt.axvline(0, color = 'k', linestyle='--')\n",
    "plt.ylim([-0.1, 0.85])\n",
    "\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('Y R²')\n",
    "# plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_passive_y_r2.pdf',dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_array = nans([len(lag_axis)]); r2_feature_array = nans([len(lag_axis),dataset.data[y_field].shape[1]])\n",
    "r_array = nans([len(lag_axis)])\n",
    "coef_array = nans([len(lag_axis),dataset.data[y_field].shape[1],dim])\n",
    "for i in range(len(lag_axis)):\n",
    "    lag = lag_axis[i]\n",
    "    r2, coef,_,vel_df,r2_arr = fit_and_predict_MC(dataset, trial_mask, 'move_onset_time',pred_range, lag, x_field, y_field,cond_dict)\n",
    "    r2_array[i] = r2; r2_feature_array[i,:] = r2_arr\n",
    "    r = scipy.stats.pearsonr(vel_df[y_field].to_numpy().reshape(-1), vel_df['pred_vel'].to_numpy().reshape(-1))[0]\n",
    "    r_array[i] = r\n",
    "    coef_array[i,:,:] = coef\n",
    "time_max = lag_axis[np.argmax(r2_array)]\n",
    "print(np.max(r2_array))\n",
    "print(time_max)\n",
    "# _, best_coef,best_intercept, best_vel_df, r2_arr = fit_and_predict(dataset, trial_mask, 'move_onset_time',pred_range, time_max, x_field, y_field, cond_dict = cond_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_decoders = coef_array.transpose(0, 2, 1)\n",
    "big_decoders = big_decoders.reshape(dim, -1)\n",
    "big_decoders.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, R = np.linalg.qr(big_decoders) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = dataset_10ms.data[x_field].to_numpy()\n",
    "print(all_data.shape)\n",
    "proj_out = calc_proj(all_data,coef_array[np.argmax(r2_array)].T).T\n",
    "proj_out_data = all_data - proj_out\n",
    "print(proj_out_data.shape)\n",
    "# dataset_10ms.add_continuous_data(proj_out_data,'CD_vel_residual')\n",
    "# dataset_10ms.add_continuous_data(proj_out_data,'FB_vel_residual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_data[:,0])\n",
    "plt.plot(proj_out_data[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_var = np.sum(np.var(all_data,axis=0))\n",
    "proj_var = np.sum(np.var(proj_out,axis=0)) \n",
    "explained_fraction = proj_var / total_var\n",
    "print(f\"Velocity-related signals explain {explained_fraction * 100:.10f}% of CD variance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(monkey+'_v6_residual_cdfb_data', \\\n",
    "        FB_residual = dataset_10ms.data['FB_vel_residual'].to_numpy(), \\\n",
    "        CD_residual = dataset_10ms.data['CD_vel_residual'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'CD_vel_residual'\n",
    "y_field ='FB_vel_residual'\n",
    "lag_axis = np.arange(-300,320,20)\n",
    "pred_range = (-100, 1000)\n",
    "trial_mask = active_mask\n",
    "cond_dict = active_cond_dict\n",
    "# pred_range = (200, 500)\n",
    "# trial_mask = passive_mask\n",
    "# cond_dict = passive_cond_dict\n",
    "dataset=dataset_10ms\n",
    "dim = dataset.data[x_field].shape[1]\n",
    "print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_array = nans([len(lag_axis)]); r2_feature_array = nans([len(lag_axis),dataset.data[y_field].shape[1]])\n",
    "r_array = nans([len(lag_axis)])\n",
    "coef_array = nans([len(lag_axis),dataset.data[y_field].shape[1],dim])\n",
    "for i in range(len(lag_axis)):\n",
    "    lag = lag_axis[i]\n",
    "    r2, coef,_,vel_df,r2_arr = fit_and_predict(dataset, trial_mask, 'move_onset_time',pred_range, lag, x_field, y_field,cond_dict)\n",
    "    r2_array[i] = r2; r2_feature_array[i,:] = r2_arr\n",
    "    r = scipy.stats.pearsonr(vel_df[y_field].to_numpy().reshape(-1), vel_df['pred_vel'].to_numpy().reshape(-1))[0]\n",
    "    r_array[i] = r\n",
    "    coef_array[i,:,:] = coef\n",
    "time_max = lag_axis[np.argmax(r2_array)]\n",
    "print(np.max(r2_array))\n",
    "print(time_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw = 3\n",
    "lag_axis = np.arange(-300,320,20)\n",
    "fig, ax = plt.subplots(figsize=(5.5,4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(lag_axis, r2_array,linewidth=lw)\n",
    "print(np.max(r2_array))\n",
    "plt.title('CD -> FB')\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_r2_cd_only = r2_feature_array[:,0]\n",
    "# y_r2_cd_only = r2_feature_array[:,1]\n",
    "# r2_cd_only = r2_array\n",
    "# tmax_cd_only = time_max\n",
    "\n",
    "# x_r2_fb_only = r2_feature_array[:,0]\n",
    "# y_r2_fb_only = r2_feature_array[:,1]\n",
    "# r2_fb_only = r2_array\n",
    "# tmax_fb_only = time_max\n",
    "\n",
    "# x_r2_fbq_only = r2_feature_array[:,0]\n",
    "# y_r2_fbq_only = r2_feature_array[:,1]\n",
    "# r2_fbq_only = r2_array\n",
    "# tmax_fbq_only = time_max\n",
    "\n",
    "# x_r2_cd_fb = r2_feature_array[:,0]\n",
    "# y_r2_cd_fb = r2_feature_array[:,1]\n",
    "# r2_cd_fb = r2_array\n",
    "# tmax_cd_fb = time_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(monkey+'_200500_smooth40_spikes_'+y_field +'_pas_r2s', \\\n",
    "         x_r2_cd_only = x_r2_cd_only, y_r2_cd_only = y_r2_cd_only, r2_cd_only = r2_cd_only, \\\n",
    "         x_r2_fb_only = x_r2_fb_only, y_r2_fb_only = y_r2_fb_only, r2_fb_only = r2_fb_only,\\\n",
    "         x_r2_cd_fb = x_r2_cd_fb, y_r2_cd_fb = y_r2_cd_fb, r2_cd_fb = r2_cd_fb)\n",
    "        #  x_r2_fbq_only = x_r2_fbq_only, y_r2_fbq_only = y_r2_fbq_only, r2_fbq_only = r2_fbq_only,\\\n",
    "        \n",
    "# monkey = \"Han_20171207\"\n",
    "data = np.load(monkey+'_200500_smooth40_spikes_hand_vel_pas_r2s.npz')\n",
    "data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez(monkey+'_all_nrn_'+y_field +'_act_r2s', \\\n",
    "#         r2_all_nrn_array = r2_array,\\\n",
    "#         r2_feature_all_nrn_array = r2_feature_array)\n",
    "\n",
    "data_all_nrn = np.load(monkey+'_all_nrn_hand_vel_act_r2s.npz')\n",
    "data_all_nrn.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw = 3\n",
    "lag_axis = np.arange(-300,320,20)\n",
    "fig, ax = plt.subplots(figsize=(5.5,4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "# plt.axvline(0, color = 'k', linestyle='--')\n",
    "\n",
    "plt.plot(lag_axis, data['r2_cd_only'],linewidth=lw,color = 'green',label='cd')\n",
    "print(np.max(data['r2_cd_only']))\n",
    "print(lag_axis[np.argmax(data['r2_cd_only'])])\n",
    "plt.plot(lag_axis, data['r2_fb_only'],linewidth=lw,color = 'magenta',label='fb')\n",
    "print(np.max(data['r2_fb_only']))\n",
    "print(lag_axis[np.argmax(data['r2_fb_only'])])\n",
    "# plt.plot(lag_axis, data['r2_fbq_only'],linewidth=lw,linestyle='--',color = 'magenta',label='fb?')\n",
    "# print(np.max(data['r2_fbq_only']))\n",
    "# print(lag_axis[np.argmax(data['r2_fbq_only'])])\n",
    "plt.plot(lag_axis, data['r2_cd_fb'],linewidth=lw,color = 'brown',label='cd+fb')\n",
    "print(np.max(data['r2_cd_fb']))\n",
    "print(lag_axis[np.argmax(data['r2_cd_fb'])])\n",
    "\n",
    "# plt.plot(lag_axis, data_all_nrn['r2_all_nrn_array'],linewidth=lw,color = 'k',label='all nrn')\n",
    "# print(np.max(data_all_nrn['r2_all_nrn_array']))\n",
    "# print(lag_axis[np.argmax(data_all_nrn['r2_all_nrn_array'])])\n",
    "# plt.ylim([0,0.8])\n",
    "\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R2')\n",
    "# plt.title(\"alt method\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# plt.savefig(figDir + monkey + '_active_r2.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_axis = np.arange(-300,320,20)\n",
    "fig, ax = plt.subplots(figsize=(5.5,4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(lag_axis, data['x_r2_cd_only'],linewidth=lw,color = 'green',label='cd')\n",
    "plt.plot(lag_axis, data['x_r2_fb_only'],linewidth=lw,color = 'magenta',label='fb')\n",
    "plt.plot(lag_axis, data['x_r2_cd_fb'],linewidth=lw,color = 'brown',label='cd+fb')\n",
    "# plt.plot(lag_axis, data_all_nrn['r2_feature_all_nrn_array'][:,0],linewidth=lw,color = 'k',label='all nrn')\n",
    "plt.ylim([0,0.8])\n",
    "\n",
    "# plt.axvline(0, color = 'k', linestyle='--')\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('X-R2')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_active_x_r2.pdf',dpi = 'figure')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5.5,4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(lag_axis, data['y_r2_cd_only'],linewidth=lw,color = 'green',label='cd')\n",
    "plt.plot(lag_axis, data['y_r2_fb_only'],linewidth=lw,color = 'magenta',label='fb')\n",
    "plt.plot(lag_axis, data['y_r2_cd_fb'],linewidth=lw,color = 'brown',label='cd+fb')\n",
    "# plt.plot(lag_axis, data_all_nrn['r2_feature_all_nrn_array'][:,1],linewidth=lw,color = 'k',label='all nrn')\n",
    "plt.ylim([0,0.8])\n",
    "\n",
    "# plt.axvline(0, color = 'k', linestyle='--')\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('Y-R2')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_active_y_r2.pdf',dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lag_axis, r2_array)\n",
    "plt.plot(lag_axis, r2_feature_array[:,0],label='x')\n",
    "plt.plot(lag_axis, r2_feature_array[:,1],label='y')\n",
    "print(lag_axis[np.argmax(r2_feature_array[:,0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_time_max = lag_axis[np.argmax(r2_feature_array[:,0])]\n",
    "y_time_max = lag_axis[np.argmax(r2_feature_array[:,1])]\n",
    "print('x',x_time_max)\n",
    "print('y',y_time_max)\n",
    "if x_time_max != y_time_max:\n",
    "    _, x_best_coef,x_best_intercept, _, _ = fit_and_predict(dataset, trial_mask, 'move_onset_time',pred_range, x_time_max, x_field, y_field, cond_dict = cond_dict)\n",
    "    _, y_best_coef,y_best_intercept, _, _ = fit_and_predict(dataset, trial_mask, 'move_onset_time',pred_range, y_time_max, x_field, y_field, cond_dict = cond_dict)\n",
    "# r2_feature_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_cd_dims = 4\n",
    "# if x_time_max == y_time_max:\n",
    "#     x_cd_weight = np.sum(abs(best_coef[0,:n_cd_dims]))\n",
    "#     x_fb_weight = np.sum(abs(best_coef[0,n_cd_dims:]))\n",
    "#     y_cd_weight = np.sum(abs(best_coef[1,:n_cd_dims]))\n",
    "#     y_fb_weight = np.sum(abs(best_coef[1,n_cd_dims:]))\n",
    "# else:\n",
    "#     x_cd_weight = np.sum(abs(x_best_coef[0,:n_cd_dims]))\n",
    "#     x_fb_weight = np.sum(abs(x_best_coef[0,n_cd_dims:]))\n",
    "#     y_cd_weight = np.sum(abs(y_best_coef[1,:n_cd_dims]))\n",
    "#     y_fb_weight = np.sum(abs(y_best_coef[1,n_cd_dims:]))\n",
    "# print(x_cd_weight)\n",
    "# print(x_fb_weight)\n",
    "# print(y_cd_weight)\n",
    "# print(y_fb_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2_cross_array = nans([len(lag_axis)]); x_r2_cross_array = nans([len(lag_axis)])\n",
    "# r_cross_array = nans([len(lag_axis)])\n",
    "# for i in range(len(lag_axis)):\n",
    "#     lag = lag_axis[i]\n",
    "#     r2, _, x_r2,_,vel_df = pred_with_new_weights(dataset, trial_mask, 'move_onset_time',pred_range, lag,x_field,\n",
    "#                                                 y_field, x_best_coef, x_best_intercept, 'move_onset_time',pred_range, (pred_range[0]+x_time_max, pred_range[1]+x_time_max), trial_mask)\n",
    "#     r2_cross_array[i] = r2; x_r2_cross_array[i] = x_r2\n",
    "#     r = scipy.stats.pearsonr(vel_df[y_field].to_numpy().reshape(-1), vel_df['pred_vel'].to_numpy().reshape(-1))[0]\n",
    "#     r_cross_array[i] = r\n",
    "# plt.plot(lag_axis,r_cross_array)\n",
    "# plt.xlabel('Time lag (ms)')\n",
    "# plt.ylabel('r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_max_pos = np.argwhere(r2_array == np.max(r2_array[np.argwhere(lag_axis==0)[0,0]:]))[0,0]\n",
    "# idx_max_neg = np.argwhere(r2_array == np.max(r2_array[:np.argwhere(lag_axis==0)[0,0]]))[0,0]\n",
    "# print(lag_axis[idx_max_pos])\n",
    "# print(lag_axis[idx_max_neg])\n",
    "\n",
    "# #For velocity, override max identification\n",
    "# # idx_max_pos = np.argwhere(lag_axis==80)[0,0]\n",
    "# # idx_max_neg = np.argwhere(lag_axis==-40)[0,0]\n",
    "\n",
    "# ang_to_max_x = nans([len(lag_axis)])\n",
    "# ang_to_max_y = nans([len(lag_axis)])\n",
    "# # ang_to_max_z = nans([len(lag_axis)])\n",
    "# for i in range(0, len(coef_array)):\n",
    "#     ang_to_max_x[i] = math.degrees(angle_between(coef_array[i,0,:],coef_array[idx_max_neg,0,:]))\n",
    "#     ang_to_max_y[i] = math.degrees(angle_between(coef_array[i,1,:],coef_array[idx_max_neg,1,:]))\n",
    "#     # ang_to_max_z[i] = math.degrees(angle_between(coef_array[i,2,:],coef_array[idx_max_neg,2,:]))\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# # plt.ylim([-5, 130])\n",
    "# plt.xlim([-310, 310])\n",
    "# plt.scatter(lag_axis, ang_to_max_x,label = 'x',color = 'green')\n",
    "# plt.scatter(lag_axis, ang_to_max_y,label = 'y',color = 'blue')\n",
    "# # plt.scatter(lag_axis, ang_to_max_z,label = 'wrist_abduction',color = 'orange')\n",
    "\n",
    "# plt.legend()\n",
    "# plt.xlabel('Time lag (ms)')\n",
    "# plt.ylabel('Angle (degrees)')\n",
    "# mean = np.mean([ang_to_max_x[idx_max_neg], ang_to_max_y[idx_max_neg]])\n",
    "# print(mean)\n",
    "# # plt.vlines(lag_axis[idx_max_pos],-5, mean, color = 'k',linestyle=\"dashed\")\n",
    "# # plt.hlines(mean, -310, lag_axis[idx_max_pos], color = 'k',linestyle=\"dashed\")\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(figDir + monkey + label + str(0) +'_angle.pdf', dpi = 'figure')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_range_arr = [(-100, 0),(0, 200),(200, 400),(400, 600),(600, 800),(800, 1000)]\n",
    "# # pred_range_arr = [(-100, 0),(0, 100),(100, 200),(200, 300),(300, 400),(400, 500)]\n",
    "\n",
    "# r2_array_tw = nans([len(pred_range_arr)]); r2_feature_array_tw = nans([len(pred_range_arr),dataset.data[y_field].shape[1]])\n",
    "# for tw in range(len(pred_range_arr)):\n",
    "#     tw_range = pred_range_arr[tw]\n",
    "#     if x_time_max == y_time_max:\n",
    "#         r2, _, x_r2,y_r2,_ = pred_with_new_weights(dataset, trial_mask, 'move_onset_time',tw_range, time_max,x_field,\n",
    "#                                                 y_field, best_coef, best_intercept, 'move_onset_time',pred_range, (pred_range[0]+time_max, pred_range[1]+time_max), trial_mask)\n",
    "#         r2_array_tw[tw] = r2; r2_feature_array_tw[tw,:] = np.array([x_r2,y_r2])\n",
    "#     else:\n",
    "#         r2, _, _,_,_ = pred_with_new_weights(dataset, trial_mask, 'move_onset_time',tw_range, time_max,x_field,\n",
    "#                                                 y_field, best_coef, best_intercept, 'move_onset_time',pred_range, (pred_range[0]+time_max, pred_range[1]+time_max), trial_mask)\n",
    "#         _, _, x_r2,_,_ = pred_with_new_weights(dataset, trial_mask, 'move_onset_time',tw_range, x_time_max,x_field,\n",
    "#                                                 y_field, x_best_coef, x_best_intercept, 'move_onset_time',pred_range, (pred_range[0]+x_time_max, pred_range[1]+x_time_max), trial_mask)\n",
    "#         _, _, _,y_r2,_ = pred_with_new_weights(dataset, trial_mask, 'move_onset_time',tw_range, y_time_max,x_field,\n",
    "#                                         y_field, y_best_coef, y_best_intercept, 'move_onset_time',pred_range, (pred_range[0]+y_time_max, pred_range[1]+y_time_max), trial_mask)\n",
    "#         r2_array_tw[tw] = r2; r2_feature_array_tw[tw,:] = np.array([x_r2,y_r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for plotting\n",
    "plot_dir = [0.0, 90.0, 180.0, 270.0] # limit plot directions to reduce cluttering\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "plot_dim = 'x' # plot x velocity \n",
    "\n",
    "x_axis = np.arange(-100,1000,dataset.bin_width)\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir %360 == trial_dir].trial_id\n",
    "    for _, trial in vel_df[np.isin(vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        plt.plot(x_axis, trial[y_field][plot_dim], color=color, linewidth=0.5)\n",
    "        # plt.plot(x_axis, trial[y_field].to_numpy()[:,0], color=color, linewidth=0.5)\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "# plt.ylabel('Hand velocity (cm/s)')\n",
    "\n",
    "# plt.xlim([-100,500])\n",
    "# plt.ylim([-0.65,0.65])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + label + 'true.pdf', dpi = 'figure')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir %360 == trial_dir].trial_id\n",
    "    for _, trial in sub_vel_df[np.isin(sub_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "         plt.plot(x_axis, trial['pred_vel'][plot_dim], color=color, linewidth=0.5)\n",
    "        # plt.plot(x_axis, trial.pred_vel.to_numpy()[:,0], color=color, linewidth=0.5)\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "# plt.ylabel('Hand velocity (cm/s)')\n",
    "# plt.xlim([-100,500])\n",
    "# plt.ylim([-0.65,0.65])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + label + str(0) +'_pred.pdf', dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for plotting\n",
    "plot_dir = [0.0, 90.0, 180.0, 270.0] # limit plot directions to reduce cluttering\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "plot_dim = 'y' # plot x velocity \n",
    "\n",
    "x_axis = np.arange(-100,1000,dataset.bin_width)\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir %360 == trial_dir].trial_id\n",
    "    for _, trial in vel_df[np.isin(vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        plt.plot(x_axis, trial[y_field][plot_dim], color=color, linewidth=0.5)\n",
    "        # plt.plot(x_axis, trial[y_field].to_numpy()[:,0], color=color, linewidth=0.5)\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "# plt.ylabel('Hand velocity (cm/s)')\n",
    "\n",
    "# plt.xlim([-100,500])\n",
    "# plt.ylim([-0.65,0.65])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + label + 'true.pdf', dpi = 'figure')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir %360 == trial_dir].trial_id\n",
    "    for _, trial in sub_vel_df[np.isin(sub_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "         plt.plot(x_axis, trial['pred_vel'][plot_dim], color=color, linewidth=0.5)\n",
    "        # plt.plot(x_axis, trial.pred_vel.to_numpy()[:,0], color=color, linewidth=0.5)\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "# plt.ylabel('Hand velocity (cm/s)')\n",
    "# plt.xlim([-100,500])\n",
    "# plt.ylim([-0.65,0.65])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + label + str(0) +'_pred.pdf', dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot some periods of predictions\n",
    "plot_dim='y'\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(vel_df['hand_vel']['y'][1000:5000],color = 'k')\n",
    "plt.plot(vel_df['pred_vel']['y'][1000:5000],color = 'brown')\n",
    "plt.tight_layout()\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "plt.ylabel('Hand velocity (cm/s)')\n",
    "# plt.savefig(figDir + monkey+'_cd_pred_.pdf',dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim= np.min([10, dataset.data[x_field].shape[1]])\n",
    "plt.plot(lag_axis,r2_feature_array)\n",
    "plt.legend(fontsize=8)\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('Variable R2')\n",
    "plt.title(x_field+' -> '+y_field)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(lag_axis, r2_array)\n",
    "plt.title(x_field)\n",
    "\n",
    "# for i in range(dim):\n",
    "#     plt.plot(lag_axis,np.sum(abs(coef_array[:,:,order[i]]),axis=1),label = str(i+1))\n",
    "# plt.legend(fontsize=8)\n",
    "# plt.xlabel('Time lag (ms)')\n",
    "# plt.ylabel('Latent weight')\n",
    "# plt.title(x_field+' sum')\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(dim):\n",
    "#     plt.plot(lag_axis,(coef_array[:,0,order[i]]),label = str(i+1))\n",
    "# plt.legend(fontsize=8)\n",
    "# plt.xlabel('Time lag (ms)')\n",
    "# plt.ylabel('Latent weight')\n",
    "# plt.title(x_field+' x-dir')\n",
    "# plt.show()\n",
    "# for i in range(dim):\n",
    "#     plt.plot(lag_axis,(coef_array[:,1,order[i]]),label = str(i+1))\n",
    "# plt.legend(fontsize=8)\n",
    "# plt.xlabel('Time lag (ms)')\n",
    "# plt.ylabel('Latent weight')\n",
    "# plt.title(x_field+' y-dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = '8020_CD_FB_proj_spikes_smth_40'\n",
    "y_field ='hand_vel'\n",
    "lag_axis = np.arange(-300,300,20)\n",
    "pred_range = (-100, 1000)\n",
    "trial_mask = active_mask\n",
    "cond_dict = active_cond_dict\n",
    "# pred_range = (-100, 500)\n",
    "# trial_mask = passive_mask\n",
    "# cond_dict = passive_cond_dict\n",
    "\n",
    "dim = dataset.data[x_field].shape[1]\n",
    "print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_range_arr = [(-100, 0),(0, 200),(200, 400),(400, 600),(600, 800),(800, 1000)]\n",
    "# pred_range_arr = [(-100, 0),(0, 100),(100, 200),(200, 300),(300, 400),(400, 500)]\n",
    "\n",
    "r2_array_dynam = nans([len(pred_range_arr),len(lag_axis)]); r2_feature_array_dynam = nans([len(pred_range_arr),len(lag_axis),dataset.data[y_field].shape[1]])\n",
    "coef_array_dynam = nans([len(pred_range_arr),len(lag_axis),dataset.data[y_field].shape[1],dim])\n",
    "time_max_array_dynam = nans([len(pred_range_arr)])\n",
    "vel_df_array_dynam = []\n",
    "for tw in range(len(pred_range_arr)):\n",
    "    tw_range = pred_range_arr[tw]\n",
    "    for i in range(len(lag_axis)):\n",
    "        lag = lag_axis[i]\n",
    "        r2, coef,_,vel_df,r2_arr = fit_and_predict(dataset, trial_mask, 'move_onset_time',tw_range, lag, x_field, y_field,cond_dict)\n",
    "        r2_array_dynam[tw,i] = r2; r2_feature_array_dynam[tw,i,:] = r2_arr\n",
    "        coef_array_dynam[tw,i,:,:] = coef\n",
    "    t_max = lag_axis[np.argmax(r2_array_dynam[tw,:])]\n",
    "    print(np.max(r2_array_dynam[tw,:]))\n",
    "    time_max_array_dynam[tw] = t_max\n",
    "    _, _,_, vel_df, _ = fit_and_predict(dataset, trial_mask, 'move_onset_time',tw_range, t_max, x_field, y_field, cond_dict = cond_dict)\n",
    "    vel_df_array_dynam.append(np.array(vel_df['pred_vel']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(range(len(pred_range_arr)), r2_array_tw,'k',label='static')\n",
    "plt.plot(range(len(pred_range_arr)), r2_feature_array_tw[:,0],'blue')\n",
    "plt.plot(range(len(pred_range_arr)), r2_feature_array_tw[:,1],'orange')\n",
    "plt.plot(range(len(pred_range_arr)), np.max(r2_array_dynam,axis=1),'k--',label='dynamic')\n",
    "plt.plot(range(len(pred_range_arr)), np.max(r2_feature_array_dynam[:,:,0],axis=1),'blue',ls='--')\n",
    "plt.plot(range(len(pred_range_arr)), np.max(r2_feature_array_dynam[:,:,1],axis=1),'orange',ls='--')\n",
    "plt.xticks(range(len(pred_range_arr)),['-100','0','200','400','600','800'])\n",
    "plt.ylim([-0.1,1])\n",
    "plt.xlabel('Time window start (ms)')\n",
    "plt.ylabel('R2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(time_max_array_dynam,range(len(pred_range_arr)),'k--')\n",
    "plt.axvline(x=time_max,color='k')\n",
    "plt.yticks(range(len(pred_range_arr)),['-100','0','200','400','600','800'])\n",
    "plt.xlabel('Best time lag (ms)')\n",
    "plt.ylabel('Time window start (ms)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_array = []\n",
    "for i in range(len(vel_df_array_dynam)):\n",
    "    stack_array.append(vel_df_array_dynam[i].reshape(active_n_trials,-1,2))\n",
    "concat_pred = np.hstack(stack_array)\n",
    "print(concat_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dir = np.array([0.0, 45.0,90.0, 135.0, 180.0, 225.0, 270.0, 315.0])\n",
    "plot_dir = [0.0, 90.0, 180.0, 270.0] # limit plot directions to reduce cluttering\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "for i in range(len(plot_dir)):\n",
    "    idx = np.argwhere(all_dir==plot_dir[i])[0,0]\n",
    "    plt.plot(np.arange(-100, 1000, 10),concat_pred[active_cond_dict==idx,:,0].T,color=colors[i],alpha=0.5,linewidth=1)\n",
    "plt.xlabel('Time after move onset (ms)')\n",
    "plt.ylabel('Hand velocity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dir = np.array([0.0, 45.0,90.0, 135.0, 180.0, 225.0, 270.0, 315.0])\n",
    "plot_dir = [0.0, 90.0, 180.0, 270.0] # limit plot directions to reduce cluttering\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "for i in range(len(plot_dir)):\n",
    "    idx = np.argwhere(all_dir==plot_dir[i])[0,0]\n",
    "    plt.plot(np.arange(-100, 1000, 10),concat_pred[active_cond_dict==idx,:,1].T,color=colors[i],alpha=0.5,linewidth=1)\n",
    "plt.xlabel('Time after move onset (ms)')\n",
    "plt.ylabel('Hand velocity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for i in range(len(pred_range_arr)):\n",
    "    plt.plot(lag_axis, r2_array_dynam[i,:].T,label=pred_range_arr[i])\n",
    "plt.legend(fontsize=8)\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for i in range(len(pred_range_arr)):\n",
    "    plt.plot(lag_axis, r2_feature_array_dynam[i,:,0].T,label=pred_range_arr[i])\n",
    "plt.legend(fontsize=8)\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R2')\n",
    "plt.title('x-dir')\n",
    "plt.show()\n",
    "\n",
    "x_time_max_array = lag_axis[np.argmax(r2_feature_array_dynam[:,:,0],axis=1)]\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(x_time_max_array,range(len(pred_range_arr)),'k--')\n",
    "plt.axvline(x=x_time_max,color='k')\n",
    "plt.yticks(range(len(pred_range_arr)),['-100','0','200','400','600','800'])\n",
    "plt.xlabel('Best time lag (ms)')\n",
    "plt.ylabel('Time window start (ms)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for i in range(len(pred_range_arr)):\n",
    "    plt.plot(lag_axis, r2_feature_array_dynam[i,:,1].T,label=pred_range_arr[i])\n",
    "plt.legend(fontsize=8)\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R2')\n",
    "plt.title('y-dir')\n",
    "\n",
    "y_time_max_array = lag_axis[np.argmax(r2_feature_array_dynam[:,:,1],axis=1)]\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(y_time_max_array,range(len(pred_range_arr)),'k--')\n",
    "plt.axvline(x=y_time_max,color='k')\n",
    "plt.yticks(range(len(pred_range_arr)),['-100','0','200','400','600','800'])\n",
    "plt.xlabel('Best time lag (ms)')\n",
    "plt.ylabel('Time window start (ms)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cd_dims = 4\n",
    "x_max_idx_arr = np.argmax(r2_feature_array_dynam[:,:,0],axis=1)\n",
    "x_cd_dynam_weight=[]\n",
    "x_fb_dynam_weight = []\n",
    "for i in range(len(pred_range_arr)):\n",
    "    x_cd_dynam_weight.append(coef_array_dynam[i,x_max_idx_arr[i],0,:n_cd_dims])\n",
    "    x_fb_dynam_weight.append(coef_array_dynam[i,x_max_idx_arr[i],0,n_cd_dims:])\n",
    "x_cd_dynam_weight = np.sum(abs(np.array((x_cd_dynam_weight))),axis=1)\n",
    "x_fb_dynam_weight = np.sum(abs(np.array((x_fb_dynam_weight))),axis=1)\n",
    "\n",
    "y_max_idx_arr = np.argmax(r2_feature_array_dynam[:,:,1],axis=1)\n",
    "y_cd_dynam_weight=[]\n",
    "y_fb_dynam_weight = []\n",
    "for i in range(len(pred_range_arr)):\n",
    "    y_cd_dynam_weight.append(coef_array_dynam[i,y_max_idx_arr[i],1,:n_cd_dims])\n",
    "    y_fb_dynam_weight.append(coef_array_dynam[i,y_max_idx_arr[i],1,n_cd_dims:])\n",
    "y_cd_dynam_weight = np.sum(abs(np.array((y_cd_dynam_weight))),axis=1)\n",
    "y_fb_dynam_weight = np.sum(abs(np.array((y_fb_dynam_weight))),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if x_time_max == y_time_max:\n",
    "    x_cd_weight = np.sum(abs(best_coef[0,:n_cd_dims]))\n",
    "    x_fb_weight = np.sum(abs(best_coef[0,n_cd_dims:]))\n",
    "    y_cd_weight = np.sum(abs(best_coef[1,:n_cd_dims]))\n",
    "    y_fb_weight = np.sum(abs(best_coef[1,n_cd_dims:]))\n",
    "else:\n",
    "    x_cd_weight = np.sum(abs(x_best_coef[0,:n_cd_dims]))\n",
    "    x_fb_weight = np.sum(abs(x_best_coef[0,n_cd_dims:]))\n",
    "    y_cd_weight = np.sum(abs(y_best_coef[1,:n_cd_dims]))\n",
    "    y_fb_weight = np.sum(abs(y_best_coef[1,n_cd_dims:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.array(range(len(pred_range_arr)))\n",
    "plt.bar(x_axis,x_cd_dynam_weight,width=0.2,color='green',label='CD')\n",
    "plt.bar(x_axis+0.2,x_fb_dynam_weight,width=0.2,color='magenta',label='FB')\n",
    "plt.axhline(x_cd_weight,color='green')\n",
    "plt.axhline(x_fb_weight,color='magenta')\n",
    "\n",
    "plt.xticks(range(len(pred_range_arr)),['-100','0','200','400','600','800'])\n",
    "plt.ylabel('Sum signal dim weight in decoder')\n",
    "plt.xlabel('Time window start (ms)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.array(range(len(pred_range_arr)))\n",
    "plt.bar(x_axis+0,y_cd_dynam_weight,width=0.2,color='green',label='CD')\n",
    "plt.bar(x_axis+0.2,y_fb_dynam_weight,width=0.2,color='magenta',label='FB')\n",
    "plt.axhline(y_cd_weight,color='green')\n",
    "plt.axhline(y_fb_weight,color='magenta')\n",
    "plt.xticks(range(len(pred_range_arr)),['-100','0','200','400','600','800'])\n",
    "plt.ylabel('Sum signal weight in decoder')\n",
    "plt.xlabel('Time window start (ms)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey = \"Han_20171207\"\n",
    "# monkey = 'Duncan_20190710'\n",
    "data = np.load(monkey+'_smooth40_spikes_hand_vel_act_r2s.npz')\n",
    "print(data.files)\n",
    "data_all_nrn = np.load(monkey+'_all_nrn_hand_vel_act_r2s.npz')\n",
    "data_all_nrn.files\n",
    "\n",
    "# data = np.load(monkey+'_MC_smooth40_spikes_hand_vel_act_r2s.npz')\n",
    "# data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_axis = np.arange(-300,320,20)\n",
    "print(lag_axis[np.argmax(data['x_r2_cd_only'])])\n",
    "print(lag_axis[np.argmax(data['x_r2_fb_only'])])\n",
    "print(lag_axis[np.argmax(data['x_r2_cd_fb'])])\n",
    "print(lag_axis[np.argmax(data_all_nrn['r2_feature_all_nrn_array'][:,0])])\n",
    "print()\n",
    "print(lag_axis[np.argmax(data['y_r2_cd_only'])])\n",
    "print(lag_axis[np.argmax(data['y_r2_fb_only'])])\n",
    "print(lag_axis[np.argmax(data['y_r2_cd_fb'])])\n",
    "print(lag_axis[np.argmax(data_all_nrn['r2_feature_all_nrn_array'][:,1])])\n",
    "print()\n",
    "print(lag_axis[np.argmax(data['r2_cd_only'])])\n",
    "print(lag_axis[np.argmax(data['r2_fb_only'])])\n",
    "print(lag_axis[np.argmax(data['r2_cd_fb'])])\n",
    "print(lag_axis[np.argmax(data_all_nrn['r2_all_nrn_array'])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lag_axis = np.arange(-300,320,20)\n",
    "# print(lag_axis[np.argmax(np.mean(data['x_r2_cd_only'],axis=1))])\n",
    "# print(lag_axis[np.argmax(np.mean(data['x_r2_fb_only'],axis=1))])\n",
    "# print(lag_axis[np.argmax(np.mean(data['x_r2_cd_fb'],axis=1))])\n",
    "# print(lag_axis[np.argmax(np.mean(data['x_r2_nrn'],axis=1))])\n",
    "# print()\n",
    "# print(lag_axis[np.argmax(np.mean(data['y_r2_cd_only'],axis=1))])\n",
    "# print(lag_axis[np.argmax(np.mean(data['y_r2_fb_only'],axis=1))])\n",
    "# print(lag_axis[np.argmax(np.mean(data['y_r2_cd_fb'],axis=1))])\n",
    "# print(lag_axis[np.argmax(np.mean(data['y_r2_nrn'],axis=1))])\n",
    "# print()\n",
    "# print(lag_axis[np.argmax(np.mean(data['r2_cd_only'],axis=1))])\n",
    "# print(lag_axis[np.argmax(np.mean(data['r2_fb_only'],axis=1))])\n",
    "# print(lag_axis[np.argmax(np.mean(data['r2_cd_fb'],axis=1))])\n",
    "# print(lag_axis[np.argmax(np.mean(data['r2_nrn'],axis=1))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['r2_cd_only'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'CD_proj'\n",
    "y_field = 'hand_vel'\n",
    "train_mask = active_mask\n",
    "cond_dict = active_cond_dict\n",
    "train_range = (-100, 1000)\n",
    "x_lag = lag_axis[np.argmax(data['x_r2_cd_only'])]\n",
    "y_lag = lag_axis[np.argmax(data['y_r2_cd_only'])]\n",
    "\n",
    "_, x_coef, x_intercept, _,_ = fit_and_predict(dataset, train_mask, 'move_onset_time',train_range, x_lag, x_field, y_field, cond_dict = cond_dict)\n",
    "_, y_coef, y_intercept, _,_ = fit_and_predict(dataset, train_mask, 'move_onset_time',train_range, y_lag, x_field, y_field, cond_dict = cond_dict)\n",
    "pas_r2_array = nans([len(lag_axis)]); pas_x_r2_array = nans([len(lag_axis)]); pas_y_r2_array = nans([len(lag_axis)])\n",
    "for i in range(len(lag_axis)):\n",
    "    lag = lag_axis[i]\n",
    "    _, _, x_r2,_,x_vel_df = pred_with_new_weights(dataset, passive_mask, 'move_onset_time',(-100,120), lag, x_field,\n",
    "                                                    y_field, x_coef, x_intercept, 'move_onset_time',train_range, (train_range[0]+x_lag, train_range[1]+x_lag), active_mask)\n",
    "    \n",
    "    _, _, _,y_r2,y_vel_df = pred_with_new_weights(dataset, passive_mask, 'move_onset_time',(-100,120), lag, x_field,\n",
    "                                                    y_field, y_coef, y_intercept, 'move_onset_time',train_range, (train_range[0]+y_lag, train_range[1]+y_lag), active_mask)\n",
    "    pas_x_r2_array[i] = x_r2; pas_y_r2_array[i] = y_r2\n",
    "    true_vel = np.array(x_vel_df['hand_vel'])\n",
    "    pred_vel = np.vstack([np.array(x_vel_df['pred_vel']['x']),np.array(y_vel_df['pred_vel']['y'])]).T\n",
    "    sses =get_sses_pred(true_vel,pred_vel)\n",
    "    sses_mean=get_sses_mean(true_vel)\n",
    "    r2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "    pas_r2_array[i] = r2\n",
    "\n",
    "pas_cd_r2_array = pas_r2_array\n",
    "pas_cd_x_r2_array = pas_x_r2_array\n",
    "pas_cd_y_r2_array = pas_y_r2_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'FB_proj'\n",
    "y_field = 'hand_vel'\n",
    "train_mask = active_mask\n",
    "cond_dict = active_cond_dict\n",
    "train_range = (-100, 1000)\n",
    "x_lag = lag_axis[np.argmax(data['x_r2_fb_only'])]\n",
    "y_lag = lag_axis[np.argmax(data['y_r2_fb_only'])]\n",
    "\n",
    "_, x_coef, x_intercept, _,_ = fit_and_predict(dataset, train_mask, 'move_onset_time',train_range, x_lag, x_field, y_field, cond_dict = cond_dict)\n",
    "_, y_coef, y_intercept, _,_ = fit_and_predict(dataset, train_mask, 'move_onset_time',train_range, y_lag, x_field, y_field, cond_dict = cond_dict)\n",
    "pas_r2_array = nans([len(lag_axis)]); pas_x_r2_array = nans([len(lag_axis)]); pas_y_r2_array = nans([len(lag_axis)])\n",
    "for i in range(len(lag_axis)):\n",
    "    lag = lag_axis[i]\n",
    "    _, _, x_r2,_,x_vel_df = pred_with_new_weights(dataset, passive_mask, 'move_onset_time',(-100,120), lag, x_field,\n",
    "                                                    y_field, x_coef, x_intercept, 'move_onset_time',train_range, (train_range[0]+x_lag, train_range[1]+x_lag), active_mask)\n",
    "    \n",
    "    _, _, _,y_r2,y_vel_df = pred_with_new_weights(dataset, passive_mask, 'move_onset_time',(-100,120), lag, x_field,\n",
    "                                                    y_field, y_coef, y_intercept, 'move_onset_time',train_range, (train_range[0]+y_lag, train_range[1]+y_lag), active_mask)\n",
    "    pas_x_r2_array[i] = x_r2; pas_y_r2_array[i] = y_r2\n",
    "    true_vel = np.array(x_vel_df['hand_vel'])\n",
    "    pred_vel = np.vstack([np.array(x_vel_df['pred_vel']['x']),np.array(y_vel_df['pred_vel']['y'])]).T\n",
    "    sses =get_sses_pred(true_vel,pred_vel)\n",
    "    sses_mean=get_sses_mean(true_vel)\n",
    "    r2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "    pas_r2_array[i] = r2\n",
    "\n",
    "pas_fb_r2_array = pas_r2_array\n",
    "pas_fb_x_r2_array = pas_x_r2_array\n",
    "pas_fb_y_r2_array = pas_y_r2_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'CD_FB_proj'\n",
    "y_field = 'hand_vel'\n",
    "train_mask = active_mask\n",
    "cond_dict = active_cond_dict\n",
    "train_range = (-100, 1000)\n",
    "x_lag = lag_axis[np.argmax(data_all_nrn['r2_feature_all_nrn_array'][:,0])]\n",
    "y_lag = lag_axis[np.argmax(data_all_nrn['r2_feature_all_nrn_array'][:,1])]\n",
    "\n",
    "_, x_coef, x_intercept, _,_ = fit_and_predict(dataset, train_mask, 'move_onset_time',train_range, x_lag, x_field, y_field, cond_dict = cond_dict)\n",
    "_, y_coef, y_intercept, _,_ = fit_and_predict(dataset, train_mask, 'move_onset_time',train_range, y_lag, x_field, y_field, cond_dict = cond_dict)\n",
    "pas_r2_array = nans([len(lag_axis)]); pas_x_r2_array = nans([len(lag_axis)]); pas_y_r2_array = nans([len(lag_axis)])\n",
    "for i in range(len(lag_axis)):\n",
    "    lag = lag_axis[i]\n",
    "    _, _, x_r2,_,x_vel_df = pred_with_new_weights(dataset, passive_mask, 'move_onset_time',(-100,120), lag, x_field,\n",
    "                                                    y_field, x_coef, x_intercept, 'move_onset_time',train_range, (train_range[0]+x_lag, train_range[1]+x_lag), active_mask)\n",
    "    \n",
    "    _, _, _,y_r2,y_vel_df = pred_with_new_weights(dataset, passive_mask, 'move_onset_time',(-100,120), lag, x_field,\n",
    "                                                    y_field, y_coef, y_intercept, 'move_onset_time',train_range, (train_range[0]+y_lag, train_range[1]+y_lag), active_mask)\n",
    "    pas_x_r2_array[i] = x_r2; pas_y_r2_array[i] = y_r2\n",
    "    true_vel = np.array(x_vel_df['hand_vel'])\n",
    "    pred_vel = np.vstack([np.array(x_vel_df['pred_vel']['x']),np.array(y_vel_df['pred_vel']['y'])]).T\n",
    "    sses =get_sses_pred(true_vel,pred_vel)\n",
    "    sses_mean=get_sses_mean(true_vel)\n",
    "    r2 =1-np.sum(sses)/np.sum(sses_mean)   \n",
    "    pas_r2_array[i] = r2\n",
    "\n",
    "pas_cdfb_r2_array = pas_r2_array\n",
    "pas_cdfb_x_r2_array = pas_x_r2_array\n",
    "pas_cdfb_y_r2_array = pas_y_r2_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez(monkey+'_hand_vel_pas120_r2s_cross', \\\n",
    "#          pas_cd_r2_array = pas_cd_r2_array, pas_cd_x_r2_array = pas_cd_x_r2_array, pas_cd_y_r2_array = pas_cd_y_r2_array, \\\n",
    "#          pas_fb_r2_array = pas_fb_r2_array, pas_fb_x_r2_array = pas_fb_x_r2_array, pas_fb_y_r2_array = pas_fb_y_r2_array, \\\n",
    "#          pas_cdfb_r2_array = pas_cdfb_r2_array, pas_cdfb_x_r2_array = pas_cdfb_x_r2_array, pas_cdfb_y_r2_array = pas_cdfb_y_r2_array)\n",
    "\n",
    "monkey = \"Han_20171207\"\n",
    "# monkey = 'Duncan_20190710'\n",
    "data = np.load(monkey+'_hand_vel_pas120_r2s_cross.npz')\n",
    "data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw = 3\n",
    "lag_axis = np.arange(-300,320,20)\n",
    "ylim = [-0.7, 0.7]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5.5,4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(lag_axis, data['pas_cd_r2_array'],linewidth=lw,color = 'green',label='cd')\n",
    "plt.plot(lag_axis, data['pas_fb_r2_array'],linewidth=lw,color = 'magenta',label='fb')\n",
    "plt.plot(lag_axis, data['pas_cdfb_r2_array'],linewidth=lw,color = 'brown',label='cd+fb')\n",
    "plt.axvline(0, color = 'k', linestyle='--')\n",
    "# print(time_max)\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R²')\n",
    "# plt.title(x_field)\n",
    "plt.ylim(ylim)\n",
    "# plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_passive_r2_dot.pdf',dpi = 'figure')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5.5,4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(lag_axis, data['pas_cd_x_r2_array'],linewidth=lw,color = 'green',label='cd')\n",
    "plt.plot(lag_axis, data['pas_fb_x_r2_array'],linewidth=lw,color = 'magenta',label='fb')\n",
    "plt.plot(lag_axis, data['pas_cdfb_x_r2_array'],linewidth=lw,color = 'brown',label='cd+fb')\n",
    "plt.axvline(0, color = 'k', linestyle='--')\n",
    "# print(time_max)\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('X R²')\n",
    "# plt.title(x_field)\n",
    "plt.ylim(ylim)\n",
    "# plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_passive_x_r2_dot.pdf',dpi = 'figure')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5.5,4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(lag_axis, data['pas_cd_y_r2_array'],linewidth=lw,color = 'green',label='cd')\n",
    "plt.plot(lag_axis, data['pas_fb_y_r2_array'],linewidth=lw,color = 'magenta',label='fb')\n",
    "plt.plot(lag_axis, data['pas_cdfb_y_r2_array'],linewidth=lw,color = 'brown',label='cd+fb')\n",
    "plt.axvline(0, color = 'k', linestyle='--')\n",
    "# print(time_max)\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('Y R²')\n",
    "# plt.title(x_field)\n",
    "plt.ylim(ylim)\n",
    "# plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_passive_y_r2_dot.pdf',dpi = 'figure')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data.keys().unique(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'spikes_smth_40'\n",
    "data = np.load(monkey+'_X_cdfb_data_proj_out.npz')\n",
    "data.files\n",
    "dataset.add_continuous_data(data['CD_FB_proj'],'CD_FB_proj')\n",
    "dataset.add_continuous_data(data['CD_proj'],'CD_proj')\n",
    "dataset.add_continuous_data(data['FB_proj'],'FB_proj')\n",
    "dataset.data.keys().unique(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "negative_lag = -180\n",
    "positive_lag = 60\n",
    "both_lag = 0\n",
    "cond_dict = active_cond_dict\n",
    "n_trials = active_n_trials\n",
    "\n",
    "y_field ='hand_vel'\n",
    "train_range = (-100,1000)\n",
    "train_pos_lag_range = (train_range[0]+positive_lag, train_range[1]+positive_lag)\n",
    "train_neg_lag_range = (train_range[0]+negative_lag, train_range[1]+negative_lag)\n",
    "train_both_lag_range = (train_range[0]+both_lag, train_range[1]+both_lag)\n",
    "n_timepoints = int((train_range[1] - train_range[0])/dataset.bin_width)\n",
    "\n",
    "train_mask = active_mask\n",
    "# _, eff_weights, eff_offset, _,_ = fit_and_predict(dataset, train_mask, 'move_onset_time',train_range, negative_lag, 'CD_proj', y_field, cond_dict = cond_dict)\n",
    "# _, _, _,_,act_eff_vel_df = pred_with_new_weights(dataset, train_mask, 'move_onset_time',train_range, 0,'CD_proj',\n",
    "#                                                  y_field, eff_weights, eff_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "\n",
    "# _, aff_weights, aff_offset, _,_ = fit_and_predict(dataset, train_mask, 'move_onset_time',train_range, positive_lag, 'FB_proj', y_field, cond_dict = cond_dict)\n",
    "# _, _, _,_,act_aff_vel_df = pred_with_new_weights(dataset, train_mask, 'move_onset_time',train_range, 0,'FB_proj',\n",
    "#                                                  y_field, aff_weights, aff_offset, 'move_onset_time',train_range, train_pos_lag_range, train_mask)\n",
    "\n",
    "_, all_weights, all_offset, _,_ = fit_and_predict(dataset, train_mask, 'move_onset_time',train_range, both_lag, 'spikes_smth_40', y_field, cond_dict = cond_dict)\n",
    "_, _, _,_,act_all_vel_df = pred_with_new_weights(dataset, train_mask, 'move_onset_time',train_range, 0,'spikes_smth_40',\n",
    "                                                 y_field, all_weights, all_offset, 'move_onset_time',train_range, train_both_lag_range, train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=train_range, ignored_trials=~active_mask, allow_overlap=True)\n",
    "latents = np.array(df.CD_FB_proj).reshape(n_trials, n_timepoints, -1)\n",
    "latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(act_all_vel_df.pred_vel).reshape(active_n_trials, n_timepoints, -1)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Neural_Decoding.decoders import DenseNNDecoder\n",
    "Y = predictions\n",
    "data_field = 'CD_FB_proj'\n",
    "lag_axis = np.arange(-100,200,50)\n",
    "regressed_r2_array = nans([len(lag_axis)])\n",
    "for i in range(len(lag_axis)):\n",
    "    lag = lag_axis[i]\n",
    "    print(lag)\n",
    "    lag_train_range = (train_range[0]+lag, train_range[1]+lag)\n",
    "    df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=lag_train_range, ignored_trials=~active_mask, allow_overlap=True)\n",
    "    latents = np.array(df[data_field]).reshape(n_trials, n_timepoints, -1)\n",
    "    X = X = latents \n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True,random_state = 42)   \n",
    "    true_concat = nans([n_trials*n_timepoints,predictions.shape[-1]])\n",
    "    pred_concat = nans([n_trials*n_timepoints,predictions.shape[-1]])\n",
    "    trial_save_idx = 0\n",
    "    for training_set, test_set in skf.split(range(0,n_trials),cond_dict):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test, y_train, y_test = process_train_test(X,Y,training_set,test_set)\n",
    "        # lr = LinearRegression().fit(X_train, y_train)\n",
    "        # y_test_predicted = lr.predict(X_test)\n",
    "        dnn = DenseNNDecoder(units=400,dropout=0.25,num_epochs=10)\n",
    "        dnn.fit(X_train, y_train)\n",
    "        y_test_predicted = dnn.predict(X_test)\n",
    "\n",
    "        n = y_test_predicted.shape[0]\n",
    "        true_concat[trial_save_idx:trial_save_idx+n,:] = y_test\n",
    "        pred_concat[trial_save_idx:trial_save_idx+n,:] = y_test_predicted\n",
    "        trial_save_idx += n\n",
    "        \n",
    "    sses =get_sses_pred(true_concat,pred_concat)\n",
    "    sses_mean=get_sses_mean(true_concat)\n",
    "    R2 =1-np.sum(sses)/np.sum(sses_mean)  \n",
    "    regressed_r2_array[i] = R2\n",
    "    print(R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lag_axis,regressed_r2_array)\n",
    "plt.ylabel('R2')\n",
    "plt.xlabel('Latent lag relative to prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "best_lag = lag_axis[np.argmax(regressed_r2_array)]\n",
    "lag_train_range = (train_range[0]+best_lag, train_range[1]+best_lag)\n",
    "df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=lag_train_range, ignored_trials=~active_mask, allow_overlap=True)\n",
    "X = np.array(df[data_field])\n",
    "Y = np.array(act_all_vel_df.pred_vel)\n",
    "lr = LinearRegression().fit(X, Y)\n",
    "pred = lr.predict(X)\n",
    "act_all_vel_df = pd.concat([act_all_vel_df, pd.DataFrame(pred, columns=dataset._make_midx('regr_vel_dnn_CDFB', ['x', 'y'], 2))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = act_all_vel_df\n",
    "# Prepare for plotting\n",
    "plot_dir = [0.0, 90.0, 180.0, 270.0] # limit plot directions to reduce cluttering\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "plot_dim = 'x' # plot x velocity \n",
    "\n",
    "x_axis = np.arange(-100,1000,dataset.bin_width)\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir %360 == trial_dir].trial_id\n",
    "    for _, trial in df[np.isin(df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        plt.plot(x_axis, trial[y_field][plot_dim], color=color, linewidth=0.5)\n",
    "        # plt.plot(x_axis, trial[y_field].to_numpy()[:,0], color=color, linewidth=0.5)\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "# plt.ylabel('Hand velocity (cm/s)')\n",
    "\n",
    "# plt.xlim([-100,500])\n",
    "# plt.ylim([-0.65,0.65])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + label + 'true.pdf', dpi = 'figure')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir %360 == trial_dir].trial_id\n",
    "    for _, trial in df[np.isin(df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "         plt.plot(x_axis, trial['pred_vel'][plot_dim], color=color, linewidth=0.5)\n",
    "        # plt.plot(x_axis, trial.pred_vel.to_numpy()[:,0], color=color, linewidth=0.5)\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "# plt.ylabel('Hand velocity (cm/s)')\n",
    "# plt.xlim([-100,500])\n",
    "# plt.ylim([-0.65,0.65])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + label + str(0) +'_pred.pdf', dpi = 'figure')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir %360 == trial_dir].trial_id\n",
    "    for _, trial in df[np.isin(df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "         plt.plot(x_axis, trial['regr_vel_dnn_CDFB'][plot_dim], color=color, linewidth=0.5)\n",
    "        # plt.plot(x_axis, trial.pred_vel.to_numpy()[:,0], color=color, linewidth=0.5)\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "# plt.ylabel('Hand velocity (cm/s)')\n",
    "# plt.xlim([-100,500])\n",
    "# plt.ylim([-0.65,0.65])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + label + str(0) +'_pred.pdf', dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_field ='hand_vel'\n",
    "lag_axis = np.arange(-300,300,20)\n",
    "pred_range = (0, 120)\n",
    "trial_mask = active_mask\n",
    "cond_dict = active_cond_dict\n",
    "\n",
    "# Note it differs for x- and y-dir\n",
    "cd_lag = -60\n",
    "fb_lag = 40\n",
    "cdfb_lag = 60\n",
    "\n",
    "_, active_cd_coef,active_cd_intercept, cd_vel_df, _ = fit_and_predict(dataset, trial_mask, 'move_onset_time',pred_range, cd_lag, 'CD_proj', y_field, cond_dict = cond_dict)\n",
    "_, active_fb_coef,active_fb_intercept, fb_vel_df, _ = fit_and_predict(dataset, trial_mask, 'move_onset_time',pred_range, fb_lag, 'FB_proj', y_field, cond_dict = cond_dict)\n",
    "_, active_cdfb_coef,active_cdfb_intercept, cdfb_vel_df, _ = fit_and_predict(dataset, trial_mask, 'move_onset_time',pred_range, cdfb_lag, 'CD_FB_proj', y_field, cond_dict = cond_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-decoding\n",
    "\n",
    "dataset = dataset_10ms\n",
    "# all_mask = dataset.trial_info.split != 'none'\n",
    "\n",
    "\n",
    "negative_lag = -100\n",
    "positive_lag = 60\n",
    "both_lag = 0\n",
    "\n",
    "y_field ='hand_vel'\n",
    "train_range = (-100,1000)\n",
    "train_pos_lag_range = (train_range[0]+positive_lag, train_range[1]+positive_lag)\n",
    "train_neg_lag_range = (train_range[0]+negative_lag, train_range[1]+negative_lag)\n",
    "train_mask = active_mask\n",
    "# train_mask = passive_mask\n",
    "_, aff_weights, aff_offset, act_aff_vel_df,_ = fit_and_predict(dataset, train_mask, 'move_onset_time', train_range, positive_lag, 'ac150_FB_proj_spikes_smth_150_oneside', y_field)\n",
    "_, eff_weights, eff_offset, act_eff_vel_df,_ = fit_and_predict(dataset, train_mask, 'move_onset_time', train_range, negative_lag,'ac150_CD_proj_spikes_smth_150_oneside',y_field)\n",
    "_, both_weights, both_offset, act_both_vel_df,_ = fit_and_predict(dataset, train_mask, 'move_onset_time', train_range, both_lag,'ac150_CD_FB_proj_spikes_smth_150_oneside',y_field)\n",
    "\n",
    "#pred active\n",
    "pred_range = (-100,1000)\n",
    "active_x_axis = np.arange(pred_range[0], pred_range[1], dataset.bin_width)\n",
    "_, _, _,_,act_aff_vel_df = pred_with_new_weights(dataset, active_mask, 'move_onset_time',pred_range, positive_lag,'ac150_FB_proj_spikes_smth_150_oneside',\n",
    "                                                 y_field, aff_weights, aff_offset, 'move_onset_time',train_range, train_pos_lag_range, train_mask)\n",
    "_, _, _,_,act_eff_vel_df = pred_with_new_weights(dataset, active_mask, 'move_onset_time',pred_range, negative_lag,'ac150_CD_proj_spikes_smth_150_oneside',\n",
    "                                                 y_field, eff_weights, eff_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "_, _, _,_,act_both_vel_df = pred_with_new_weights(dataset, active_mask, 'move_onset_time',pred_range, both_lag,'ac150_CD_FB_proj_spikes_smth_150_oneside',\n",
    "                                                 y_field, both_weights, both_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "#pred passive\n",
    "pred_range = (-100, 500)\n",
    "passive_x_axis = np.arange(pred_range[0], pred_range[1], dataset.bin_width)\n",
    "_, _, _,_,pas_aff_vel_df = pred_with_new_weights(dataset, passive_mask, 'move_onset_time',pred_range, positive_lag,'ac150_FB_proj_spikes_smth_150_oneside',\n",
    "                                                 y_field, aff_weights, aff_offset, 'move_onset_time',train_range, train_pos_lag_range, train_mask)\n",
    "_, _, _,_,pas_eff_vel_df = pred_with_new_weights(dataset, passive_mask, 'move_onset_time',pred_range, negative_lag,'ac150_CD_proj_spikes_smth_150_oneside',\n",
    "                                                 y_field, eff_weights, eff_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "_, _, _,_,pas_both_vel_df = pred_with_new_weights(dataset, passive_mask, 'move_onset_time',pred_range, both_lag,'ac150_CD_FB_proj_spikes_smth_150_oneside',\n",
    "                                                 y_field, both_weights, both_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "\n",
    "# #pred nan\n",
    "# pred_range = (-100, 1000)\n",
    "# passive_x_axis = np.arange(pred_range[0], pred_range[1], dataset.bin_width)\n",
    "# _, _, _,_,pas_aff_vel_df = pred_with_new_weights(dataset, nan_mask, 'move_onset_time',pred_range, positive_lag,'FB_proj',\n",
    "#                                                  y_field, aff_weights, aff_offset, 'move_onset_time',train_range, train_pos_lag_range, train_mask)\n",
    "# _, _, _,_,pas_eff_vel_df = pred_with_new_weights(dataset, nan_mask, 'move_onset_time',pred_range, negative_lag,'CD_proj',\n",
    "#                                                  y_field, eff_weights, eff_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "# _, _, _,_,pas_both_vel_df = pred_with_new_weights(dataset, nan_mask, 'move_onset_time',pred_range, both_lag,'CD_FB_proj',\n",
    "#                                                  y_field, both_weights, both_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "\n",
    "plot_dir = [0.0, 180.0] # limit plot directions to reduce cluttering\n",
    "colors = ['gray','gray']\n",
    "plot_dim = 'x'\n",
    "fig, axs = plt.subplots(6, 4, sharex=False, sharey=True, figsize=(18, 18))\n",
    "# plt.ylim(-50,50)\n",
    "i = 0\n",
    "alpha = 0.5\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir%360 == trial_dir].trial_id\n",
    "    for _, trial in act_eff_vel_df[np.isin(act_eff_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        axs[0][i].plot(active_x_axis, trial[y_field][plot_dim], color=color, alpha = alpha, linewidth=0.5)\n",
    "        axs[0][i].plot(active_x_axis, trial.pred_vel[plot_dim], color='green', alpha = alpha, linewidth=0.5)\n",
    "        axs[0][i].axvline(x=300, ls='--')\n",
    "    for _, trial in act_aff_vel_df[np.isin(act_aff_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        axs[1][i].plot(active_x_axis, trial.pred_vel[plot_dim], color='magenta', alpha = alpha, linewidth=0.5)\n",
    "        axs[1][i].plot(active_x_axis, trial[y_field][plot_dim], color=color, alpha = alpha, linewidth=0.5)\n",
    "        axs[1][i].axvline(x=300, ls='--')\n",
    "    for _, trial in act_both_vel_df[np.isin(act_both_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        axs[2][i].plot(active_x_axis, trial[y_field][plot_dim], color=color, alpha = alpha, linewidth=0.5)\n",
    "        axs[2][i].plot(active_x_axis, trial.pred_vel[plot_dim], color='brown', alpha = alpha, linewidth=0.5)    \n",
    "        axs[2][i].spines[['right', 'top']].set_visible(False) \n",
    "        axs[2][i].axvline(x=300, ls='--')\n",
    "    # cond_ids = dataset.trial_info[dataset.trial_info.bump_dir%360 == trial_dir].trial_id\n",
    "    for _, trial in pas_eff_vel_df[np.isin(pas_eff_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        axs[3][i].plot(passive_x_axis, trial.pred_vel[plot_dim], color='green', alpha = alpha, linewidth=0.5)\n",
    "        axs[3][i].plot(passive_x_axis, trial[y_field][plot_dim], color=color, alpha = alpha, linewidth=0.5)\n",
    "        axs[3][i].axvline(x=120, ls='--')\n",
    "    for _, trial in pas_aff_vel_df[np.isin(pas_aff_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        axs[4][i].plot(passive_x_axis, trial.pred_vel[plot_dim], color='magenta', alpha = alpha, linewidth=0.5)\n",
    "        axs[4][i].plot(passive_x_axis, trial[y_field][plot_dim], color=color, alpha = alpha, linewidth=0.5)\n",
    "        axs[4][i].axvline(x=120, ls='--')\n",
    "    for _, trial in pas_both_vel_df[np.isin(pas_both_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        axs[5][i].plot(passive_x_axis, trial.pred_vel[plot_dim], color='brown', alpha = alpha, linewidth=0.5)\n",
    "        axs[5][i].plot(passive_x_axis, trial[y_field][plot_dim], color=color, alpha = alpha, linewidth=0.5)\n",
    "        axs[5][i].spines[['right', 'top']].set_visible(False)\n",
    "        axs[5][i].axvline(x=120, ls='--')\n",
    "    i+=2\n",
    "\n",
    "\n",
    "negative_lag = -100\n",
    "positive_lag = 120\n",
    "both_lag = 0\n",
    "\n",
    "y_field ='hand_vel'\n",
    "train_range = (-100,1000)\n",
    "train_pos_lag_range = (train_range[0]+positive_lag, train_range[1]+positive_lag)\n",
    "train_neg_lag_range = (train_range[0]+negative_lag, train_range[1]+negative_lag)\n",
    "train_mask = active_mask\n",
    "# train_mask = passive_mask\n",
    "\n",
    "_, aff_weights, aff_offset, act_aff_vel_df,_ = fit_and_predict(dataset, train_mask, 'move_onset_time', train_range, positive_lag, 'ac150_FB_proj_spikes_smth_150_oneside', y_field)\n",
    "_, eff_weights, eff_offset, act_eff_vel_df,_ = fit_and_predict(dataset, train_mask, 'move_onset_time', train_range, negative_lag,'ac150_CD_proj_spikes_smth_150_oneside',y_field)\n",
    "_, both_weights, both_offset, act_both_vel_df,_ = fit_and_predict(dataset, train_mask, 'move_onset_time', train_range, both_lag,'ac150_CD_FB_proj_spikes_smth_150_oneside',y_field)\n",
    "\n",
    "#pred active\n",
    "pred_range = (-100, 1000)\n",
    "active_x_axis = np.arange(pred_range[0], pred_range[1], dataset.bin_width)\n",
    "_, _, _,_,act_aff_vel_df = pred_with_new_weights(dataset, active_mask, 'move_onset_time',pred_range, positive_lag,'ac150_FB_proj_spikes_smth_150_oneside',\n",
    "                                                 y_field, aff_weights, aff_offset, 'move_onset_time',train_range, train_pos_lag_range, train_mask)\n",
    "_, _, _,_,act_eff_vel_df = pred_with_new_weights(dataset, active_mask, 'move_onset_time',pred_range, negative_lag,'ac150_CD_proj_spikes_smth_150_oneside',\n",
    "                                                 y_field, eff_weights, eff_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "_, _, _,_,act_both_vel_df = pred_with_new_weights(dataset, active_mask, 'move_onset_time',pred_range, both_lag,'ac150_CD_FB_proj_spikes_smth_150_oneside',\n",
    "                                                 y_field, both_weights, both_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "#pred passive\n",
    "pred_range = (-100, 500)\n",
    "passive_x_axis = np.arange(pred_range[0], pred_range[1], dataset.bin_width)\n",
    "_, _, _,_,pas_aff_vel_df = pred_with_new_weights(dataset, passive_mask, 'move_onset_time',pred_range, positive_lag,'ac150_FB_proj_spikes_smth_150_oneside',\n",
    "                                                 y_field, aff_weights, aff_offset, 'move_onset_time',train_range, train_pos_lag_range, train_mask)\n",
    "_, _, _,_,pas_eff_vel_df = pred_with_new_weights(dataset, passive_mask, 'move_onset_time',pred_range, negative_lag,'ac150_CD_proj_spikes_smth_150_oneside',\n",
    "                                                 y_field, eff_weights, eff_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "_, _, _,_,pas_both_vel_df = pred_with_new_weights(dataset, passive_mask, 'move_onset_time',pred_range, both_lag,'ac150_CD_FB_proj_spikes_smth_150_oneside',\n",
    "                                                 y_field, both_weights, both_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "\n",
    "#pred nan\n",
    "# pred_range = (-100, 1000)\n",
    "# passive_x_axis = np.arange(pred_range[0], pred_range[1], dataset.bin_width)\n",
    "# _, _, _,_,pas_aff_vel_df = pred_with_new_weights(dataset, nan_mask, 'move_onset_time',pred_range, positive_lag,'FB_proj',\n",
    "#                                                  y_field, aff_weights, aff_offset, 'move_onset_time',train_range, train_pos_lag_range, train_mask)\n",
    "# _, _, _,_,pas_eff_vel_df = pred_with_new_weights(dataset, nan_mask, 'move_onset_time',pred_range, negative_lag,'CD_proj',\n",
    "#                                                  y_field, eff_weights, eff_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "# _, _, _,_,pas_both_vel_df = pred_with_new_weights(dataset, nan_mask, 'move_onset_time',pred_range,both_lag,'CD_FB_proj',\n",
    "#                                                  y_field, both_weights, both_offset, 'move_onset_time',train_range, train_neg_lag_range, train_mask)\n",
    "\n",
    "\n",
    "plot_dir = [90.0, 270.0] # limit plot directions to reduce cluttering\n",
    "colors = ['gray', 'gray']\n",
    "plot_dim = 'y'\n",
    "i = 1\n",
    "alpha = 0.5\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir%360 == trial_dir].trial_id\n",
    "    for _, trial in act_eff_vel_df[np.isin(act_eff_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        axs[0][i].plot(active_x_axis, trial[y_field][plot_dim], color=color, alpha = alpha, linewidth=0.5)\n",
    "        axs[0][i].plot(active_x_axis, trial.pred_vel[plot_dim], color='green', alpha = alpha, linewidth=0.5)\n",
    "        axs[0][i].axvline(x=300, ls='--')\n",
    "    for _, trial in act_aff_vel_df[np.isin(act_aff_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        axs[1][i].plot(active_x_axis, trial.pred_vel[plot_dim], color='magenta', alpha = alpha, linewidth=0.5)\n",
    "        axs[1][i].plot(active_x_axis, trial[y_field][plot_dim], color=color, alpha = alpha, linewidth=0.5)\n",
    "        axs[1][i].axvline(x=300, ls='--')\n",
    "    for _, trial in act_both_vel_df[np.isin(act_both_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        axs[2][i].plot(active_x_axis, trial[y_field][plot_dim], color=color, alpha = alpha, linewidth=0.5)\n",
    "        axs[2][i].plot(active_x_axis, trial.pred_vel[plot_dim], color='brown', alpha = alpha, linewidth=0.5)    \n",
    "        axs[2][i].spines[['right', 'top']].set_visible(False) \n",
    "        axs[2][i].axvline(x=300, ls='--')\n",
    "    # cond_ids = dataset.trial_info[dataset.trial_info.bump_dir%360 == trial_dir].trial_id\n",
    "    for _, trial in pas_eff_vel_df[np.isin(pas_eff_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        axs[3][i].plot(passive_x_axis, trial.pred_vel[plot_dim], color='green', alpha = alpha, linewidth=0.5)\n",
    "        axs[3][i].plot(passive_x_axis, trial[y_field][plot_dim], color=color, alpha = alpha, linewidth=0.5)\n",
    "        axs[3][i].axvline(x=120, ls='--')\n",
    "    for _, trial in pas_aff_vel_df[np.isin(pas_aff_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        axs[4][i].plot(passive_x_axis, trial.pred_vel[plot_dim], color='magenta', alpha = alpha, linewidth=0.5)\n",
    "        axs[4][i].plot(passive_x_axis, trial[y_field][plot_dim], color=color, alpha = alpha, linewidth=0.5)\n",
    "        axs[4][i].axvline(x=120, ls='--')\n",
    "    for _, trial in pas_both_vel_df[np.isin(pas_both_vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        axs[5][i].plot(passive_x_axis, trial.pred_vel[plot_dim], color='brown', alpha = alpha, linewidth=0.5)\n",
    "        axs[5][i].plot(passive_x_axis, trial[y_field][plot_dim], color=color, alpha = alpha, linewidth=0.5)\n",
    "        axs[5][i].spines[['right', 'top']].set_visible(False)\n",
    "        axs[5][i].axvline(x=120, ls='--')\n",
    "    i+=2\n",
    "\n",
    "fig.supxlabel('Time after movement onset (ms)')\n",
    "# axs[0][0].set_ylabel('Prediction',fontsize=14)\n",
    "# axs[1][0].set_ylabel('Prediction',fontsize=14)\n",
    "# axs[0][0].set_ylabel('Hand acceleration \\n (cm/s^2)',fontsize=14)\n",
    "# axs[1][0].set_ylabel('Hand acceleration \\n (cm/s^2)',fontsize=14)\n",
    "axs[0][0].set_ylabel('Hand velocity \\n (cm/s)',fontsize=14)\n",
    "axs[1][0].set_ylabel('Hand velocity \\n (cm/s)',fontsize=14)\n",
    "\n",
    "\n",
    "axs[0][0].set_title('0 deg')\n",
    "axs[0][1].set_title('90 deg')\n",
    "axs[0][2].set_title('180 deg')\n",
    "axs[0][3].set_title('270 deg')\n",
    "\n",
    "\n",
    "# legend_elements = [Patch(facecolor='magenta', label='Afferent prediction'),\n",
    "#                     Patch(facecolor='k', label='Efferent prediction')]\n",
    "# legend_elements = [Patch(facecolor='magenta', label='Afferent prediction')]\n",
    "# plt.legend(handles=legend_elements)\n",
    "plt.tight_layout()\n",
    "# figDir = '/Users/sherryan/area2_population_analysis/'\n",
    "# plt.savefig(figDir + monkey + '_cross_vel_both_early_aligned.pdf', dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PC proj_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(monkey+'_CDFB_weights_'+'PCA_40'+'.npz')\n",
    "X = data['CD_axes']\n",
    "eff_weights = pca.inverse_transform(X)\n",
    "print(eff_weights.shape)\n",
    "X = data['FB_axes']\n",
    "aff_weights = pca.inverse_transform(X)\n",
    "print(aff_weights.shape)\n",
    "np.savez(monkey+'_CDFB_weights_pc_proj_back', CD_axes = eff_weights, FB_axes = aff_weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_weights_mean = np.mean(abs(aff_weights),axis=0)\n",
    "print(aff_weights_mean.shape)\n",
    "plt.hist(aff_weights_mean)\n",
    "plt.show()\n",
    "eff_weights_mean = np.mean(abs(eff_weights),axis=0)\n",
    "print(eff_weights_mean.shape)\n",
    "plt.hist(eff_weights_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neuron weights plot\n",
    "\n",
    "def adjacent_values(vals, q1, q3):\n",
    "    upper_adjacent_value = q3 + (q3 - q1) * 1.5\n",
    "    upper_adjacent_value = np.clip(upper_adjacent_value, q3, vals[-1])\n",
    "    lower_adjacent_value = q1 - (q3 - q1) * 1.5\n",
    "    lower_adjacent_value = np.clip(lower_adjacent_value, vals[0], q1)\n",
    "    return lower_adjacent_value, upper_adjacent_value\n",
    "    \n",
    "plt.hist(aff_weights_mean)\n",
    "plt.show()\n",
    "plt.hist(eff_weights_mean)\n",
    "plt.show()\n",
    "\n",
    "Ki_x = []\n",
    "for i in range(n_neurons):\n",
    "    Wa = aff_weights_mean[i]\n",
    "    We = eff_weights_mean[i]\n",
    "    if abs(Wa) > 0.02 or abs(We) > 0.02:\n",
    "        Ki_x.append((abs(Wa) - abs(We)) / (abs(Wa)+abs(We)))\n",
    "    else:\n",
    "        Ki_x.append(np.nan)\n",
    "print(len(Ki_x))\n",
    "\n",
    "Ki_x = np.array(Ki_x)\n",
    "Ki_x_plot = Ki_x[~np.isnan(Ki_x)]\n",
    "print(len(Ki_x_plot))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "# fig.suptitle('Relative contribution to acc decoder between Afference and Efference')\n",
    "import seaborn as sns\n",
    "parts = ax.violinplot(Ki_x_plot,showmeans = False, showextrema=False)\n",
    "for pc in parts['bodies']:\n",
    "    pc.set_facecolor('grey')\n",
    "    pc.set_edgecolor('black')\n",
    "    pc.set_alpha(.5)\n",
    "\n",
    "\n",
    "quartile1, medians, quartile3 = np.percentile(Ki_x_plot, [25, 50, 75])\n",
    "whiskers = adjacent_values(sorted(Ki_x_plot), quartile1, quartile3)\n",
    "whiskers_min, whiskers_max = whiskers[0], whiskers[1]\n",
    "ax.scatter(1, medians, marker='o', color='white', s=20, zorder=3)\n",
    "ax.vlines(1, quartile1, quartile3, color='k', linestyle='-', lw=5)\n",
    "ax.vlines(1, whiskers_min, whiskers_max, color='k', linestyle='-', lw=1)\n",
    "\n",
    "ax.set_ylabel('Relative contribution')\n",
    "ax.set_xticks([])\n",
    "# ax.set_xlabel('All neurons')\n",
    "ax.set_ylim([-1,1])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_contrib_violin_vel.pdf', dpi = 'figure')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.hist(sorted(Ki_x_plot),10,rwidth=0.8,color = 'grey')\n",
    "plt.xlabel('Relative contribution')\n",
    "plt.ylabel('Neuron count')\n",
    "plt.xlim([-1.01,1.01])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + '_contrib_bar_vel.pdf', dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argwhere(np.isnan(np.array(Ki_x))).squeeze()\n",
    "# np.argsort(np.array(Ki_x))\n",
    "valid_sort = np.array([x for x in np.argsort(np.array(Ki_x)) if x not in np.argwhere(np.isnan(np.array(Ki_x))).squeeze()])\n",
    "valid_sort\n",
    "\n",
    "#To pick exammple single neuron \n",
    "# np.array(Ki_x)[47]\n",
    "\n",
    "np.savez(monkey+'_cdfb_contrib'+'_pc_proj', relative_contrib = np.array(Ki_x), valid_sort=valid_sort)\n",
    "data = np.load(monkey+'_cdfb_contrib_pc_proj.npz')\n",
    "data.files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Null distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey = 'Duncan_20190710'\n",
    "# monkey = 'Han_20171207'\n",
    "data = np.load(monkey+'_v6_alt_zscore_unsmoothed100_cdfb_weights_spikes.npz')\n",
    "eff_weights = data['CD_axes']\n",
    "print(eff_weights.shape)\n",
    "aff_weights = data['FB_axes']\n",
    "print(aff_weights.shape)\n",
    "true_p_angles = principal_angles(eff_weights.T, aff_weights.T)\n",
    "print(\"Principal angles\", np.degrees(true_p_angles)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(eff_weights.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(aff_weights.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_matrix = eff_weights\n",
    "# Compute the range (max - min) and standard deviation of the entire matrix\n",
    "E_matrix_range = np.max(E_matrix) - np.min(E_matrix)\n",
    "E_matrix_mean = np.mean(E_matrix)\n",
    "E_matrix_std = np.std(E_matrix)\n",
    "E_matrix_mean = np.mean(E_matrix)\n",
    "print(f\"Range of the matrix: {E_matrix_range}\")\n",
    "print(f\"Mean of the matrix: {E_matrix_mean}\")\n",
    "print(f\"Standard deviation of the matrix: {E_matrix_std}\")\n",
    "\n",
    "A_matrix = aff_weights\n",
    "# Compute the range (max - min) and standard deviation of the entire matrix\n",
    "A_matrix_range = np.max(A_matrix) - np.min(A_matrix)\n",
    "A_matrix_mean = np.mean(A_matrix)\n",
    "A_matrix_std = np.std(A_matrix)\n",
    "print(f\"Range of the matrix: {A_matrix_range}\")\n",
    "print(f\"Mean of the matrix: {A_matrix_mean}\")\n",
    "print(f\"Standard deviation of the matrix: {A_matrix_std}\")\n",
    "\n",
    "N = 1000\n",
    "p_angles = nans([N])\n",
    "for i in range(N):\n",
    "    # Draw samples from a Gaussian distribution with the computed standard deviation\n",
    "    rand_E_matrix = np.random.normal(loc=E_matrix_mean, scale=E_matrix_std, size=E_matrix.shape)\n",
    "    rand_A_matrix = np.random.normal(loc=A_matrix_mean, scale=A_matrix_std, size=A_matrix.shape)\n",
    "    X = rand_E_matrix.T\n",
    "    Y = rand_A_matrix.T\n",
    "    p_angles[i] = np.degrees(principal_angles(X, Y))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.hist(p_angles,color='blue',alpha=0.5,edgecolor='black',bins=20)\n",
    "plt.axvline(x=np.degrees(true_p_angles)[0],color='k',linestyle='--',linewidth=2)\n",
    "plt.xlabel('First principal angle (deg)')\n",
    "plt.ylabel('Count')\n",
    "plt.text(np.degrees(true_p_angles)[0] + 1, plt.gca().get_ylim()[1] * 0.9,  # Adjust position as needed\n",
    "         \"True principal angle \" + str(np.degrees(true_p_angles)[0]), color='black', fontsize=10)\n",
    "plt.text(70, 60,np.sum(p_angles < np.degrees(true_p_angles)[0])/1000)\n",
    "plt.ylim([0,140])\n",
    "plt.savefig(figDir + monkey + '_zscore_null_dist.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relative contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkey = 'Duncan_20190710'\n",
    "# monkey = 'Han_20171207'\n",
    "data = np.load(monkey+'_v6_unsmoothed100_cdfb_weights_spikes.npz')\n",
    "# data = np.load(monkey+'_v6_alt_zscore_unsmoothed100_cdfb_weights_spikes.npz')\n",
    "\n",
    "eff_weights = data['CD_axes']\n",
    "print(eff_weights.shape)\n",
    "aff_weights = data['FB_axes']\n",
    "print(aff_weights.shape)\n",
    "\n",
    "n_neurons = aff_weights.shape[1]\n",
    "print(n_neurons,'neurons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_weights_mean = np.mean(abs(aff_weights),axis=0)\n",
    "print(aff_weights_mean.shape)\n",
    "plt.hist(aff_weights_mean)\n",
    "plt.show()\n",
    "eff_weights_mean = np.mean(abs(eff_weights),axis=0)\n",
    "print(eff_weights_mean.shape)\n",
    "plt.hist(eff_weights_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neuron weights plot\n",
    "\n",
    "def adjacent_values(vals, q1, q3):\n",
    "    upper_adjacent_value = q3 + (q3 - q1) * 1.5\n",
    "    upper_adjacent_value = np.clip(upper_adjacent_value, q3, vals[-1])\n",
    "    lower_adjacent_value = q1 - (q3 - q1) * 1.5\n",
    "    lower_adjacent_value = np.clip(lower_adjacent_value, vals[0], q1)\n",
    "    return lower_adjacent_value, upper_adjacent_value\n",
    "    \n",
    "plt.hist(aff_weights_mean)\n",
    "plt.show()\n",
    "plt.hist(eff_weights_mean)\n",
    "plt.show()\n",
    "\n",
    "Ki_x = []\n",
    "for i in range(n_neurons):\n",
    "    Wa = aff_weights_mean[i]\n",
    "    We = eff_weights_mean[i]\n",
    "    if abs(Wa) > 0.05 or abs(We) > 0.05:\n",
    "        Ki_x.append((abs(Wa) - abs(We)) / (abs(Wa)+abs(We)))\n",
    "    else:\n",
    "        Ki_x.append(np.nan)\n",
    "print(len(Ki_x))\n",
    "\n",
    "Ki_x = np.array(Ki_x)\n",
    "Ki_x_plot = Ki_x[~np.isnan(Ki_x)]\n",
    "print(len(Ki_x_plot))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "# fig.suptitle('Relative contribution to acc decoder between Afference and Efference')\n",
    "import seaborn as sns\n",
    "parts = ax.violinplot(Ki_x_plot,showmeans = False, showextrema=False)\n",
    "for pc in parts['bodies']:\n",
    "    pc.set_facecolor('grey')\n",
    "    pc.set_edgecolor('black')\n",
    "    pc.set_alpha(.5)\n",
    "\n",
    "\n",
    "quartile1, medians, quartile3 = np.percentile(Ki_x_plot, [25, 50, 75])\n",
    "whiskers = adjacent_values(sorted(Ki_x_plot), quartile1, quartile3)\n",
    "whiskers_min, whiskers_max = whiskers[0], whiskers[1]\n",
    "ax.scatter(1, medians, marker='o', color='white', s=20, zorder=3)\n",
    "ax.vlines(1, quartile1, quartile3, color='k', linestyle='-', lw=5)\n",
    "ax.vlines(1, whiskers_min, whiskers_max, color='k', linestyle='-', lw=1)\n",
    "\n",
    "ax.set_ylabel('Relative contribution')\n",
    "ax.set_xticks([])\n",
    "# ax.set_xlabel('All neurons')\n",
    "ax.set_ylim([-1,1])\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_contrib_violin_vel.pdf', dpi = 'figure')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.hist(sorted(Ki_x_plot),10,rwidth=0.8,color = 'grey')\n",
    "plt.xlabel('Relative contribution')\n",
    "plt.ylabel('Neuron count')\n",
    "plt.xlim([-1.01,1.01])\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + '_contrib_bar_vel.pdf', dpi = 'figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argwhere(np.isnan(np.array(Ki_x))).squeeze()\n",
    "# np.argsort(np.array(Ki_x))\n",
    "valid_sort = np.array([x for x in np.argsort(np.array(Ki_x)) if x not in np.argwhere(np.isnan(np.array(Ki_x))).squeeze()])\n",
    "print(valid_sort)\n",
    "print(len(valid_sort),'neurons')\n",
    "\n",
    "#To pick exammple single neuron \n",
    "print(np.array(Ki_x)[71])\n",
    "print(np.array(Ki_x)[138])\n",
    "print(np.array(Ki_x)[101])\n",
    "print(np.array(Ki_x)[75])\n",
    "print(np.array(Ki_x)[97])\n",
    "print(np.array(Ki_x)[3])\n",
    "\n",
    "np.savez(monkey+'_cdfb_contrib'+'_neuron', relative_contrib = np.array(Ki_x), valid_sort=valid_sort)\n",
    "data = np.load(monkey+'_cdfb_contrib_neuron.npz')\n",
    "data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import subspace_angles\n",
    "\n",
    "# Parameters\n",
    "ambient_dim = 50  # Ambient space dimension\n",
    "dim_A = 4          # Dimension of first subspace\n",
    "dim_B = 2          # Dimension of second subspace\n",
    "num_trials = 1000  # Number of random subspace pairs\n",
    "\n",
    "# Function to generate a random orthonormal basis of a subspace\n",
    "def random_subspace(dim, ambient_dim):\n",
    "    Q, _ = np.linalg.qr(np.random.randn(ambient_dim, dim))\n",
    "    return Q\n",
    "\n",
    "# Store smallest principal angles (in degrees)\n",
    "min_angles = []\n",
    "\n",
    "# Simulation loop\n",
    "for _ in range(num_trials):\n",
    "    A = random_subspace(dim_A, ambient_dim)\n",
    "    B = random_subspace(dim_B, ambient_dim)\n",
    "    angles_rad = subspace_angles(A, B)        # Principal angles in radians\n",
    "    smallest_angle_deg = np.degrees(min(angles_rad))  # Convert to degrees\n",
    "    min_angles.append(smallest_angle_deg)\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(min_angles, bins=50, density=True, alpha=0.75, edgecolor='black')\n",
    "plt.axvline(71.3, color='red', linestyle='--', label='Angle (71.3°)')\n",
    "plt.title('Distribution of Smallest Principal Angle\\nBetween Random 4D and 2D Subspaces in ℝ¹⁰⁰')\n",
    "plt.xlabel('Smallest Principal Angle (degrees)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import subspace_angles\n",
    "\n",
    "# Parameters\n",
    "ambient_dim = 100  # Ambient space dimension\n",
    "dim_A = 4          # Dimension of first subspace\n",
    "dim_B = 2          # Dimension of second subspace\n",
    "num_trials = 1000  # Number of random subspace pairs\n",
    "\n",
    "# Function to generate a random orthonormal basis of a subspace\n",
    "def random_subspace(dim, ambient_dim):\n",
    "    Q, _ = np.linalg.qr(np.random.randn(ambient_dim, dim))\n",
    "    return Q\n",
    "\n",
    "# Store smallest principal angles (in degrees)\n",
    "min_angles = []\n",
    "\n",
    "# Simulation loop\n",
    "for _ in range(num_trials):\n",
    "    A = random_subspace(dim_A, ambient_dim)\n",
    "    B = random_subspace(dim_B, ambient_dim)\n",
    "    angles_rad = subspace_angles(A, B)        # Principal angles in radians\n",
    "    smallest_angle_deg = np.degrees(min(angles_rad))  # Convert to degrees\n",
    "    min_angles.append(smallest_angle_deg)\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(min_angles, bins=50, density=True, alpha=0.75, edgecolor='black')\n",
    "plt.axvline(71, color='red', linestyle='--', label='Angle (71.0°)')\n",
    "plt.title('Distribution of Smallest Principal Angle\\nBetween Random 4D and 2D Subspaces in ℝ100')\n",
    "plt.xlabel('Smallest Principal Angle (degrees)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdt_env",
   "language": "python",
   "name": "sdt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
