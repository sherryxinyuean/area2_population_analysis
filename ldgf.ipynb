{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlb_tools.nwb_interface import NWBDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "from Neural_Decoding.preprocessing_funcs import get_spikes_with_history\n",
    "from Area2_analysis.lr_funcs import angle_between, process_train_test, gaussian_filter1d_oneside, comp_cc,xcorr\n",
    "from Area2_analysis.lr_funcs import get_sses_pred, get_sses_mean, nans, fit_and_predict_LDGF,pred_with_new_LDGF\n",
    "matplotlib.rc('font', size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import Area2_analysis.lr_funcs\n",
    "importlib.reload(Area2_analysis.lr_funcs)\n",
    "from Area2_analysis.lr_funcs import fit_and_predict_LDGF, retrieve_LDGF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch basics - Linear Regression from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/aakashns/pytorch-basics-linear-regression-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# Create tensors.\n",
    "x = torch.tensor(3.)\n",
    "w = torch.tensor(4., requires_grad=True)\n",
    "b = torch.tensor(5., requires_grad=True)\n",
    "print(x)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = w * x + b\n",
    "print(y)\n",
    "# What makes PyTorch special, is that we can automatically compute the derivative of y \n",
    "# w.r.t. the tensors that have requires_grad set to True i.e. w and b.\n",
    "y.backward()\n",
    "print('dy/dw:', w.grad)\n",
    "print('dy/db:', b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')\n",
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs)\n",
    "print(targets)\n",
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and biases\n",
    "w = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def model(x):\n",
    "    return x @ w.t() + b\n",
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with targets\n",
    "print(targets)\n",
    "# Because we've started with random weights and biases, the model does not a very good job of predicting the target varaibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can compare the predictions with the actual targets, using the following method:\n",
    "\n",
    "# Calculate the difference between the two matrices (preds and targets).\n",
    "# Square all elements of the difference matrix to remove negative values.\n",
    "# Calculate the average of the elements in the resulting matrix.\n",
    "# The result is a single number, known as the mean squared error (MSE).\n",
    "# MSE loss\n",
    "def mse(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff) / diff.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Gradients\n",
    "# With PyTorch, we can automatically compute the gradient or derivative of the loss w.r.t. to the weights and biases, because they have requires_grad set to True.\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gradients are stored in the .grad property of the respective tensors.\n",
    "# Gradients for weights\n",
    "print(w)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradients for bias\n",
    "print(b)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A key insight from calculus is that the gradient indicates the rate of change of the loss, or the slope of the loss function w.r.t. the weights and biases.\n",
    "\n",
    "# If a gradient element is postive,\n",
    "# increasing the element's value slightly will increase the loss.\n",
    "# decreasing the element's value slightly will decrease the loss.\n",
    "\n",
    "# If a gradient element is negative,\n",
    "# increasing the element's value slightly will decrease the loss.\n",
    "# decreasing the element's value slightly will increase the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we'll reset the gradients to zero before moving forward, because PyTorch accumulates gradients.\n",
    "\n",
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust weights and biases using gradient descent¶\n",
    "# We'll reduce the loss and improve our model using the gradient descent algorithm, which has the following steps:\n",
    "\n",
    "# 1. Generate predictions\n",
    "# 2. Calculate the loss\n",
    "# 3. Compute gradients w.r.t the weights and biases\n",
    "# 4. Adjust the weights by subtracting a small quantity proportional to the gradient\n",
    "# 5. Reset the gradients to zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust weights & reset gradients\n",
    "\n",
    "# Context-manager that disables gradient calculation.\n",
    "# Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). \n",
    "# It will reduce memory consumption for computations that would otherwise have requires_grad=True.\n",
    "\n",
    "with torch.no_grad():     \n",
    "    w -= w.grad * 1e-5\n",
    "    b -= b.grad * 1e-5\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the new weights and biases, the model should have a lower loss.\n",
    "# Calculate loss\n",
    "preds = model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 100 epochs\n",
    "for i in range(100):\n",
    "    preds = model(inputs)\n",
    "    loss = mse(preds, targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * 1e-5\n",
    "        b -= b.grad * 1e-5\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate loss\n",
    "preds = model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print predictions\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Model using PyTorch built-ins\n",
    "# Let's re-implement the same model using some built-in functions and classes from PyTorch.\n",
    "\n",
    "import torch.nn as nn\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader\n",
    "# We'll create a TensorDataset, which allows access to rows from inputs and targets as tuples. \n",
    "# We'll also create a DataLoader, to split the data into batches while training. It also provides other utilities like shuffling and sampling\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# Define dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loader\n",
    "batch_size = 5\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Linear\n",
    "# Instead of initializing the weights & biases manually, we can define the model using nn.Linear.\n",
    "\n",
    "# Define model\n",
    "model = nn.Linear(3, 2)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "# Instead of manually manipulating the weights & biases using gradients, we can use the optimizer optim.SGD.\n",
    "\n",
    "# Define optimizer\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "# Instead of defining a loss function manually, we can use the built-in loss function mse_loss.\n",
    "\n",
    "# Import nn.functional\n",
    "import torch.nn.functional as F\n",
    "# Define loss function\n",
    "loss_fn = F.mse_loss\n",
    "\n",
    "loss = loss_fn(model(inputs), targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# We are ready to train the model now. We can define a utility function fit which trains the model for a given number of epochs.\n",
    "\n",
    "# Define a utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt):\n",
    "    for epoch in range(num_epochs):\n",
    "        for xb,yb in train_dl:\n",
    "            # Generate predictions\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            # Perform gradient descent\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "    print('Training loss: ', loss_fn(model(inputs), targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for 100 epochs\n",
    "fit(100, model, loss_fn, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with targets\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Feedfoward Neural Network¶\n",
    "# Conceptually, you think of feedforward neural networks as two or more linear regression models stacked on top of one another with a non-linear activation function applied between them.\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    # Initialize the layers\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(3, 3)\n",
    "        self.act1 = nn.ReLU() # Activation function\n",
    "        self.linear2 = nn.Linear(3, 2)\n",
    "    \n",
    "    # Perform the computation\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNet()\n",
    "opt = torch.optim.SGD(model.parameters(), 1e-5)\n",
    "loss_fn = F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(100, model, loss_fn, opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# Define dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.parametrize as P\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')\n",
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = torch.from_numpy(inputs)\n",
    "# targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = TensorDataset(inputs, targets)\n",
    "# # Define data loader\n",
    "# batch_size = 5\n",
    "# train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "# next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a utility function to train the model\n",
    "# def fit(num_epochs, model, loss_fn, opt):\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for xb,yb in train_dl:\n",
    "#             # Generate predictions\n",
    "#             pred = model(xb)\n",
    "#             loss = loss_fn(pred, yb)\n",
    "#             # Perform gradient descent\n",
    "#             loss.backward()\n",
    "#             opt.step()\n",
    "#             opt.zero_grad()\n",
    "#     print('Training loss: ', loss_fn(model(inputs), targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LinearModel(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super().__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.output_dim = output_dim\n",
    "#         self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         output = self.linear(x)\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LDGF(object):\n",
    "#     def __init__(self, n_epochs=500, learning_rate = 0.01, init = None):\n",
    "#         self.n_epochs = n_epochs\n",
    "#         self.init = init\n",
    "#         self.learning_rate = learning_rate\n",
    "#     def fit_transform(self, X, Y):\n",
    "#         model = LinearModel(X.shape[1], Y.shape[1])\n",
    "            \n",
    "#         X = torch.tensor(X,dtype=torch.float)\n",
    "#         y = torch.tensor(Y,dtype=torch.float)\n",
    "\n",
    "#         model.eval()\n",
    "#         y_pred = model(X)\n",
    "#         loss_fn = F.mse_loss\n",
    "#         loss = loss_fn(y_pred, y)\n",
    "#         losses=np.zeros(self.n_epochs+1)\n",
    "#         losses[0]=loss.item()\n",
    "\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr = self.learning_rate)\n",
    "#         model.train()\n",
    "#         for epoch in tqdm(range(self.n_epochs), position=0, leave=True):\n",
    "#             optimizer.zero_grad()\n",
    "#             # Forward pass\n",
    "#             y_pred = model(X)\n",
    "#             # Compute Loss\n",
    "#             loss = loss_fn(y_pred, y)\n",
    "#             losses[epoch+1]=loss.item()\n",
    "#             # Backward pass\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "        \n",
    "#         # Include attributes as part of self\n",
    "#         self.model=model\n",
    "#         self.losses=losses\n",
    "#         self.params={}\n",
    "#         self.params['weight']=model.linear.state_dict()['weight'].detach().numpy()\n",
    "#         self.params['bias']=model.linear.state_dict()['bias'].detach().numpy()\n",
    "#         self.r2_score=r2_score(y,y_pred.detach().numpy())\n",
    "#         return y_pred.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, weight_init, b_init):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.linear = nn.Linear(input_dim, output_dim, bias = True)\n",
    "        self.linear.weight = torch.nn.Parameter(torch.tensor(weight_init, dtype=torch.float))\n",
    "        self.linear.bias = torch.nn.Parameter(torch.tensor(b_init, dtype=torch.float))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.linear(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "class LDGF(object):\n",
    "    def __init__(self, n_epochs=3000, learning_rate = None, init = None):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.init = init\n",
    "        self.learning_rate = learning_rate\n",
    "    def fit_transform(self, X, y):\n",
    "        if self.init is None:\n",
    "            weight_init = np.random.randn(X.shape[1], y.shape[1]).T\n",
    "            b_init = np.zeros(y.shape[1])\n",
    "        elif self.init == 'linear_regression':\n",
    "            reg = LinearRegression().fit(X, y)\n",
    "            weight_init = reg.coef_\n",
    "            b_init = reg.intercept_\n",
    "\n",
    "        if self.learning_rate == None:\n",
    "            if self.init == 'linear_regression':\n",
    "                self.learning_rate = 0.001\n",
    "            elif self.init is None:\n",
    "                self.learning_rate = 0.01\n",
    "            \n",
    "        model = LinearModel(X.shape[1], y.shape[1], weight_init, b_init)\n",
    "            \n",
    "        X = torch.tensor(X,dtype=torch.float)\n",
    "        y = torch.tensor(y,dtype=torch.float)\n",
    "\n",
    "        model.eval()\n",
    "        y_pred = model(X)\n",
    "        loss_fn = F.mse_loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        losses=np.zeros(self.n_epochs+1)\n",
    "        losses[0]=loss.item()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = self.learning_rate)\n",
    "        model.train()\n",
    "        for epoch in tqdm(range(self.n_epochs), position=0, leave=True):\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            y_pred = model(X)\n",
    "            # Compute Loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            losses[epoch+1]=loss.item()\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Include attributes as part of self\n",
    "        self.model=model\n",
    "        self.losses=losses\n",
    "        self.params={}\n",
    "        self.params['weight']=model.linear.weight.detach().numpy()\n",
    "        self.params['bias']=model.linear.bias.detach().numpy()\n",
    "        self.r2_score=r2_score(y,y_pred.detach().numpy())\n",
    "        return y_pred.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldgf=LDGF()\n",
    "y_pred=ldgf.fit_transform(X=inputs, y=targets)\n",
    "print(y_pred)\n",
    "y_true = targets\n",
    "print(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ldgf.losses)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "print('loss',ldgf.losses[-1])\n",
    "print('r2',ldgf.r2_score)\n",
    "print('weight',ldgf.params['weight'])\n",
    "print('bias',ldgf.params['bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_true[:,0],label='true')\n",
    "plt.plot(y_pred[:,0],label='predicted')\n",
    "plt.legend()\n",
    "plt.title('feature 1')\n",
    "plt.show()\n",
    "plt.plot(y_true[:,1],label='true')\n",
    "plt.plot(y_pred[:,1],label='predicted')\n",
    "plt.legend()\n",
    "plt.title('feature 2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T= 200 #Time\n",
    "N_neurons = 10\n",
    "N_features = 2\n",
    "X0 = np.random.randn(T,N_neurons)\n",
    "w0 = np.random.randn(N_neurons, N_features)\n",
    "print(X0.shape)\n",
    "print(w0.shape)\n",
    "b0 = np.random.randn(N_features)\n",
    "Y0 = X0 @ w0 + b0\n",
    "print(Y0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "N_plot_neurons = 7\n",
    "for i in range(N_plot_neurons):\n",
    "    \n",
    "    #Plot ground truth\n",
    "    plt.subplot(N_plot_neurons,2,2*i+1)\n",
    "    plt.plot((X0)[:,i]) \n",
    "    \n",
    "    # plt.yticks([])\n",
    "    if i<N_plot_neurons-1:\n",
    "        plt.xticks([])\n",
    "    else:\n",
    "        plt.xlabel('Time')\n",
    "for i in range(N_features):\n",
    "    plt.subplot(N_plot_neurons,2,2*i+2)\n",
    "    plt.plot((Y0)[:,i]) \n",
    "    # plt.yticks([])    \n",
    "    if i<N_features-1:\n",
    "        plt.xticks([])\n",
    "    else:\n",
    "        plt.xlabel('Time')\n",
    "\n",
    "plt.subplot(N_plot_neurons,2,1)\n",
    "plt.title('Simulated neurons')\n",
    "\n",
    "plt.subplot(N_plot_neurons,2,2)\n",
    "plt.title('Simulated features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ldgf=LDGF(init='linear_regression')\n",
    "ldgf=LDGF()\n",
    "\n",
    "y_pred=ldgf.fit_transform(X=X0, y=Y0)\n",
    "print(y_pred)\n",
    "y_true = Y0\n",
    "print(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_true[:,0],label='true')\n",
    "plt.plot(y_pred[:,0],label='predicted')\n",
    "plt.legend()\n",
    "plt.title('feature 1')\n",
    "plt.show()\n",
    "plt.plot(y_true[:,1],label='true')\n",
    "plt.plot(y_pred[:,1],label='predicted')\n",
    "plt.legend()\n",
    "plt.title('feature 2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('true weight',w0)\n",
    "print('true bias',b0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ldgf.losses)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "print('loss',ldgf.losses[-1])\n",
    "print('r2',ldgf.r2_score)\n",
    "print('pred weight',ldgf.params['weight'].T)\n",
    "print('pred bias',ldgf.params['bias'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate data + add filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_filter(x, sigma):\n",
    "    return np.exp(-0.5*((x)/sigma)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_length = 41\n",
    "x_range = np.arange(-filter_length//2+1, filter_length//2+1)\n",
    "sigmas = [2,5]\n",
    "gaussian_filter_list = [gaussian_filter(x_range,sigmas[j]) for j in range(N_features)]\n",
    "\n",
    "Y0_with_filter = np.stack([np.convolve(Y0[:,j], filter, mode='same') for j, filter in enumerate(gaussian_filter_list)]).T\n",
    "print(Y0_with_filter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = gaussian_filter(x_range, sigma=2)\n",
    "X0_with_filter = np.stack([np.convolve(X0[:,j], filter,mode='same') for j in range(X0.shape[1])]).T\n",
    "print(X0_with_filter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "N_plot_neurons = 7\n",
    "for i in range(N_plot_neurons):\n",
    "    \n",
    "    #Plot ground truth\n",
    "    plt.subplot(N_plot_neurons,3,3*i+1)\n",
    "    plt.plot((X0)[:,i]) \n",
    "    \n",
    "    # plt.yticks([])\n",
    "    if i<N_plot_neurons-1:\n",
    "        plt.xticks([])\n",
    "    else:\n",
    "        plt.xlabel('Time')\n",
    "\n",
    "for i in range(N_plot_neurons):\n",
    "    \n",
    "    #Plot ground truth\n",
    "    plt.subplot(N_plot_neurons,3,3*i+2)\n",
    "    plt.plot((X0_with_filter)[:,i]) \n",
    "    \n",
    "    # plt.yticks([])\n",
    "    if i<N_plot_neurons-1:\n",
    "        plt.xticks([])\n",
    "    else:\n",
    "        plt.xlabel('Time')\n",
    "\n",
    "# for i in range(N_features):\n",
    "    # plt.subplot(N_plot_neurons,3,3*i+2)\n",
    "    # plt.plot((Y0)[:,i]) \n",
    "    # # plt.yticks([])    \n",
    "    # if i<N_features-1:\n",
    "    #     plt.xticks([])\n",
    "    # else:\n",
    "    #     plt.xlabel('Time')\n",
    "\n",
    "for i in range(N_features):\n",
    "    plt.subplot(N_plot_neurons,3,3*i+3)\n",
    "    plt.plot((Y0_with_filter)[:,i]) \n",
    "    # plt.yticks([])    \n",
    "    if i<N_features-1:\n",
    "        plt.xticks([])\n",
    "    else:\n",
    "        plt.xlabel('Time')\n",
    "\n",
    "plt.subplot(N_plot_neurons,3,1)\n",
    "plt.title('Simulated neurons')\n",
    "\n",
    "\n",
    "plt.subplot(N_plot_neurons,3,2)\n",
    "plt.title('Simulated neurons, filtered')\n",
    "\n",
    "# plt.subplot(N_plot_neurons,3,2)\n",
    "# plt.title('Simulated features')\n",
    "\n",
    "plt.subplot(N_plot_neurons,3,3)\n",
    "plt.title('Simulated features, filtered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldgf=LDGF(init='linear_regression')\n",
    "# ldgf=LDGF()\n",
    "\n",
    "# y_pred=ldgf.fit_transform(X=X0, y=Y0)\n",
    "# y_pred=ldgf.fit_transform(X=X0, y=Y0_with_filter)\n",
    "y_pred=ldgf.fit_transform(X=X0_with_filter, y=Y0_with_filter)\n",
    "\n",
    "\n",
    "print(y_pred)\n",
    "# y_true = Y0\n",
    "y_true = Y0_with_filter\n",
    "print(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_true[:,0],label='true')\n",
    "plt.plot(y_pred[:,0],label='predicted')\n",
    "plt.legend()\n",
    "plt.title('feature 1')\n",
    "plt.show()\n",
    "plt.plot(y_true[:,1],label='true')\n",
    "plt.plot(y_pred[:,1],label='predicted')\n",
    "plt.legend()\n",
    "plt.title('feature 2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('true weight',w0)\n",
    "print('true bias',b0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ldgf.losses)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "print('loss',ldgf.losses[-1])\n",
    "print('r2',ldgf.r2_score)\n",
    "print('pred weight',ldgf.params['weight'].T)\n",
    "print('pred bias',ldgf.params['bias'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# Define dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.parametrize as P\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from ldgf.model import LDGF   \n",
    "\n",
    "import importlib\n",
    "import ldgf.model\n",
    "importlib.reload(ldgf.model)\n",
    "from ldgf.model import LDGF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import dateutil\n",
    "import pytz\n",
    "\n",
    "print(numpy.__version__)\n",
    "print(dateutil.__version__)\n",
    "print(pytz.__version__)\n",
    "import hdmf\n",
    "print(hdmf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynwb\n",
    "print(pynwb.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_filter(x, sigma,filter_length):\n",
    "    return np.exp(-0.5*((x)/sigma)**2)\n",
    "def causal_filter(x, sigma,filter_length):\n",
    "    phi_x = np.exp(-0.5*((x)/sigma)**2)\n",
    "    phi_x[:filter_length//2] = 0\n",
    "    return phi_x\n",
    "def anticausal_filter(x, sigma,filter_length):\n",
    "    phi_x = np.exp(-0.5*((x)/sigma)**2)\n",
    "    phi_x[-filter_length//2+1:] = 0\n",
    "    return phi_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T= 200 #Time\n",
    "N_neurons = 10\n",
    "N_features = 2\n",
    "X0 = np.ones([T,N_neurons])\n",
    "X0[:50,:]=0\n",
    "X0[-50:,:]=0\n",
    "w0 = np.random.randn(N_neurons, N_features)\n",
    "print(w0.shape)\n",
    "b0 = np.random.randn(N_features)\n",
    "Y0 = X0 @ w0 + b0\n",
    "print(X0.shape)\n",
    "print(Y0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_length = 41\n",
    "x_range = np.arange(-filter_length//2+1, filter_length//2+1)\n",
    "sigmas = [2,5]\n",
    "# filter_list = [gaussian_filter(x_range,sigmas[j],filter_length) for j in range(N_features)]\n",
    "filter_list = [causal_filter(x_range,sigmas[j],filter_length) for j in range(N_features)]\n",
    "# filter_list = [anticausal_filter(x_range,sigmas[j],filter_length) for j in range(N_features)]\n",
    "\n",
    "Y0_with_filter_valid = np.stack([np.convolve(Y0[:,j], filter, mode='valid') for j, filter in enumerate(filter_list)]).T\n",
    "Y0_with_filter_valid = Y0_with_filter_valid.reshape(1,Y0_with_filter_valid.shape[0], Y0_with_filter_valid.shape[1])\n",
    "\n",
    "\n",
    "Y0_with_filter_full = np.stack([np.convolve(Y0[:,j], filter, mode='full') for j, filter in enumerate(filter_list)]).T\n",
    "Y0_with_filter_full = Y0_with_filter_full.reshape(1,Y0_with_filter_full.shape[0], Y0_with_filter_full.shape[1])\n",
    "\n",
    "\n",
    "filter_list = [causal_filter(x_range,sigmas[j],filter_length) for j in range(N_features)]\n",
    "Y0_with_filter = np.stack([np.convolve(Y0[:,j], filter, mode='same') for j, filter in enumerate(filter_list)]).T\n",
    "Y0_with_filter = Y0_with_filter.reshape(1,Y0_with_filter.shape[0], Y0_with_filter.shape[1])\n",
    "print(Y0_with_filter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filter_length = 41\n",
    "sigma = 5.0\n",
    "x = np.arange(-filter_length // 2 + 1, filter_length // 2 + 1)\n",
    "\n",
    "causal = causal_filter(x, sigma, filter_length)\n",
    "anticausal = anticausal_filter(x, sigma, filter_length)\n",
    "\n",
    "plt.plot(x, causal, label='Causal Filter')\n",
    "plt.plot(x, anticausal, label='Anti-Causal Filter')\n",
    "plt.axvline(0, color='k', linestyle='--', label='Current Time')\n",
    "plt.legend()\n",
    "plt.title(\"Causal vs. Anti-Causal Filters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(N_features):\n",
    "    plt.subplot(N_features, 1,i+1)\n",
    "    plt.plot(x_range,filter_list[i],label='true filter')\n",
    "print(filter_list[0][np.argwhere(x_range==0)[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter = gaussian_filter(x_range, sigma=2)\n",
    "# X0_with_filter = np.stack([np.convolve(X0[:,j], filter,mode='same') for j in range(X0.shape[1])]).T\n",
    "# X0_with_filter = X0_with_filter.reshape(1,X0_with_filter.shape[0], X0_with_filter.shape[1])\n",
    "# print(X0_with_filter.shape)\n",
    "X0 = X0.reshape(1,X0.shape[0], X0.shape[1])\n",
    "Y0 = Y0.reshape(1,Y0.shape[0], Y0.shape[1])\n",
    "print(X0.shape)\n",
    "print(Y0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "N_plot_neurons = 7\n",
    "for i in range(N_plot_neurons):\n",
    "    \n",
    "    #Plot ground truth\n",
    "    plt.subplot(N_plot_neurons,3,3*i+1)\n",
    "    plt.plot((X0)[0,:,i]) \n",
    "    \n",
    "    # plt.yticks([])\n",
    "    if i<N_plot_neurons-1:\n",
    "        plt.xticks([])\n",
    "    else:\n",
    "        plt.xlabel('Time')\n",
    "\n",
    "\n",
    "for i in range(N_features):\n",
    "    plt.subplot(N_plot_neurons,3,3*i+2)\n",
    "    plt.plot((Y0)[0,:,i]) \n",
    "    # plt.yticks([])    \n",
    "    if i<N_features-1:\n",
    "        plt.xticks([])\n",
    "    else:\n",
    "        plt.xlabel('Time')\n",
    "\n",
    "for i in range(N_features):\n",
    "    plt.subplot(N_plot_neurons,3,3*i+3)\n",
    "    plt.plot((Y0_with_filter)[0,:,i]) \n",
    "    # plt.yticks([])    \n",
    "    if i<N_features-1:\n",
    "        plt.xticks([])\n",
    "    else:\n",
    "        plt.xlabel('Time')\n",
    "\n",
    "\n",
    "plt.subplot(N_plot_neurons,3,1)\n",
    "plt.title('Simulated neurons')\n",
    "\n",
    "\n",
    "plt.subplot(N_plot_neurons,3,2)\n",
    "plt.title('Simulated features')\n",
    "\n",
    "plt.subplot(N_plot_neurons,3,3)\n",
    "plt.title('Simulated features, causal filtered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ldgf=LDGF(init='linear_regression',n_epochs=6000)\n",
    "ldgf=LDGF(filter_type = 'causal',n_epochs=8000)\n",
    "# ldgf=LDGF(add_filter = False)\n",
    "\n",
    "\n",
    "# y_pred=ldgf.fit_transform(X=X0, y=Y0)\n",
    "y_pred=ldgf.fit_transform(X=X0, y=Y0_with_filter)\n",
    "# y_pred=ldgf.fit_transform(X=X0_with_\n",
    "# filter, y=Y0_with_filter)\n",
    "\n",
    "print(y_pred)\n",
    "# y_true = Y0\n",
    "y_true = Y0_with_filter\n",
    "print(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_true[0,:,0],label='true')\n",
    "plt.plot(y_pred[0,:,0],label='predicted')\n",
    "plt.legend()\n",
    "plt.title('feature 1')\n",
    "plt.show()\n",
    "plt.plot(y_true[0,:,1],label='true')\n",
    "plt.plot(y_pred[0,:,1],label='predicted')\n",
    "plt.legend()\n",
    "plt.title('feature 2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('true weight',w0)\n",
    "print('true bias',b0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ldgf.losses)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "print('loss',ldgf.losses[-1])\n",
    "print('r2',ldgf.r2_score)\n",
    "print('pred weight',ldgf.params['weight'].T)\n",
    "print('pred bias',ldgf.params['bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = ldgf.model.plottable_filters()\n",
    "sigmas = ldgf.model.get_sigmas()\n",
    "print(sigmas)\n",
    "j=0\n",
    "for i in range(N_features):\n",
    "    plt.subplot(N_features, 1,i+1)\n",
    "    plt.plot(x,filter_list[i],label='true filter')\n",
    "    plt.plot(x,y[i][::-1],label='pred filter')\n",
    "    if i < N_features-1:\n",
    "        plt.xticks([])\n",
    "    j+=2\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldername = \"~/area2_population_analysis/s1-kinematics/actpas_NWB/\"\n",
    "# monkey = \"Han_20171207\"\n",
    "# filename = foldername + monkey + \"_COactpas_TD_offset2.nwb\"\n",
    "\n",
    "monkey = 'Duncan_20190710'\n",
    "filename = foldername + monkey + \"_COactpas_offset2.nwb\"\n",
    "\n",
    "dataset_10ms = NWBDataset(filename, split_heldout=False)\n",
    "dataset_10ms.resample(10) #in 10-ms bin, has to resample first to avoid NaNs\n",
    "bin_width = dataset_10ms.bin_width\n",
    "print(bin_width)\n",
    "xy_vel = dataset_10ms.data['hand_vel'].to_numpy() \n",
    "# xy_acc = np.diff(xy_vel, axis = 0, prepend=[xy_vel[0]])\n",
    "# dataset_10ms.add_continuous_data(xy_acc,'hand_acc',chan_names = ['x','y'])\n",
    "# dataset_10ms.smooth_spk(40, name='smth_40')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'spikes'\n",
    "data = np.load(monkey+'_unsmoothed50_cdfb_data_'+x_field+'.npz')\n",
    "data.files\n",
    "dataset_10ms.add_continuous_data(data['CD_FB_proj'],'CD_FB_proj')\n",
    "dataset_10ms.add_continuous_data(data['CD_proj'],'CD_proj')\n",
    "dataset_10ms.add_continuous_data(data['FB_proj'],'FB_proj')\n",
    "dataset_10ms.data.keys().unique(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = dataset_10ms.trial_info.shape[0]\n",
    "print(n_trials,'total trials')\n",
    "n_neurons = dataset_10ms.data.spikes.shape[1]\n",
    "print(n_neurons,'neurons')\n",
    "\n",
    "n_dims = 20 # for PCA\n",
    "\n",
    "all_data = np.array(dataset_10ms.data.spikes)\n",
    "print(all_data.shape)\n",
    "if not np.isnan(all_data).any():\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(all_data)\n",
    "    pca = PCA(n_components=n_dims,random_state = 42)\n",
    "    PCA_data = pca.fit_transform(X)\n",
    "print(PCA_data.shape)\n",
    "dataset_10ms.add_continuous_data(PCA_data,'PCA')\n",
    "print('PCA total var explained:',sum(pca.explained_variance_ratio_))\n",
    "\n",
    "n_trials = dataset_10ms.trial_info.shape[0]\n",
    "print(n_trials,'total trials')\n",
    "n_neurons = dataset_10ms.data.spikes.shape[1]\n",
    "print(n_neurons,'neurons')\n",
    "\n",
    "# all_data = np.array(dataset_10ms.data.spikes_smth_40)\n",
    "# print(all_data.shape)\n",
    "# if not np.isnan(all_data).any():\n",
    "#     scaler = StandardScaler()\n",
    "#     X = scaler.fit_transform(all_data)\n",
    "#     pca = PCA(n_components=n_dims,random_state = 42)\n",
    "#     PCA_data = pca.fit_transform(X)\n",
    "# print(PCA_data.shape)\n",
    "# dataset_10ms.add_continuous_data(PCA_data,'PCA_40')\n",
    "# print('PCA total var explained:',sum(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dictionary for trial condition (reaching directions) for Stratified CV\n",
    "dataset = dataset_10ms\n",
    "active_mask = (dataset.trial_info.ctr_hold_bump==0) & (dataset.trial_info['split'] != 'none')\n",
    "passive_mask = (dataset.trial_info.ctr_hold_bump==1) & (dataset.trial_info['split'] != 'none')\n",
    "nan_mask = (np.isnan(dataset.trial_info.ctr_hold_bump)) & (dataset.trial_info['split'] != 'none')\n",
    "all_mask = (dataset.trial_info['split'] != 'none')\n",
    "\n",
    "trial_mask = all_mask\n",
    "valid_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(valid_n_trials,'valid trials')\n",
    "\n",
    "trial_mask = active_mask\n",
    "active_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "active_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(active_n_trials,'active trials')\n",
    "\n",
    "trial_mask = passive_mask\n",
    "passive_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "passive_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(passive_n_trials,'passive trials')\n",
    "\n",
    "trial_mask = nan_mask\n",
    "nan_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "nan_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(nan_n_trials,'reach bump trials')\n",
    "\n",
    "active_cond_dir_idx = []\n",
    "passive_cond_dir_idx = []\n",
    "nan_cond_dir_idx = []\n",
    "nan_bump_cond_dir_idx = []\n",
    "for direction in [0,45,90,135,180,225,270,315]:\n",
    "# for direction in [0,90,180,270]:\n",
    "    active_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & (dataset.trial_info['ctr_hold_bump'] == 0) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "    passive_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & (dataset.trial_info['ctr_hold_bump'] == 1) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "    nan_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & (np.isnan(dataset.trial_info.ctr_hold_bump)) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "    nan_bump_cond_dir_idx.append(np.where((dataset.trial_info['bump_dir']%360 == direction) & (np.isnan(dataset.trial_info.ctr_hold_bump)) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "\n",
    "active_cond_dict = nans([active_n_trials])\n",
    "i = 0\n",
    "for idx in active_trials_idx:\n",
    "    for cond in range(0,len(active_cond_dir_idx)):\n",
    "        if idx in active_cond_dir_idx[cond]:\n",
    "            active_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(active_cond_dict)\n",
    "print(len(active_cond_dict))\n",
    "\n",
    "passive_cond_dict = nans([passive_n_trials])\n",
    "i = 0\n",
    "for idx in passive_trials_idx:\n",
    "    for cond in range(0,len(passive_cond_dir_idx)):\n",
    "        if idx in passive_cond_dir_idx[cond]:\n",
    "            passive_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(passive_cond_dict)\n",
    "print(len(passive_cond_dict))\n",
    "\n",
    "nan_cond_dict = nans([nan_n_trials])\n",
    "i = 0\n",
    "for idx in nan_trials_idx:\n",
    "    for cond in range(0,len(nan_cond_dir_idx)):\n",
    "        if idx in nan_cond_dir_idx[cond]:\n",
    "            nan_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(nan_cond_dict)\n",
    "print(len(nan_cond_dict))\n",
    "\n",
    "nan_bump_cond_dict = nans([nan_n_trials])\n",
    "i = 0\n",
    "for idx in nan_trials_idx:\n",
    "    for cond in range(0,len(nan_bump_cond_dir_idx)):\n",
    "        if idx in nan_bump_cond_dir_idx[cond]:\n",
    "            nan_bump_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(nan_bump_cond_dict)\n",
    "print(len(nan_bump_cond_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passive_df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range = (-100,100), ignored_trials = ~passive_mask)\n",
    "set(passive_trials_idx) - set(passive_df['trial_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range = (-100,100), ignored_trials = ~active_mask)\n",
    "del_indices = list(set(active_trials_idx) - set(active_df['trial_id'].unique()))\n",
    "print('was',active_n_trials,'active trials')\n",
    "active_n_trials = active_n_trials - len(list(set(active_trials_idx) - set(active_df['trial_id'].unique())))\n",
    "active_cond_dict = np.delete(active_cond_dict,np.argwhere(active_trials_idx==del_indices)[0])\n",
    "print('now',active_n_trials,'active trials')\n",
    "print(len(active_cond_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sca.models import SCA\n",
    "# align_range = (-100, 500)\n",
    "# active_trial_data = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=align_range, ignored_trials=~active_mask)\n",
    "# active_trial_spsm = np.array(active_trial_data.spikes_smth_40)\n",
    "# target_n_trials = active_trial_data['trial_id'].nunique()\n",
    "# n_timepoints = int((align_range[1]-align_range[0])/bin_width)\n",
    "# active_sample_weights= np.ones((target_n_trials, n_timepoints))\n",
    "# # active_sample_weights[:,:int(100/dataset_10ms.bin_width)] = 10\n",
    "# active_sample_weights = active_sample_weights.flatten()\n",
    "# print(active_sample_weights.shape)\n",
    "\n",
    "# align_range = (-100, 500)\n",
    "# passive_trial_data = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=align_range, ignored_trials=~passive_mask)\n",
    "# passive_trial_spsm = np.array(passive_trial_data.spikes_smth_40)\n",
    "# target_n_trials = passive_trial_data['trial_id'].nunique()\n",
    "# n_timepoints = int((align_range[1]-align_range[0])/bin_width)\n",
    "# passive_sample_weights= np.ones((target_n_trials, n_timepoints))\n",
    "# passive_sample_weights = passive_sample_weights.flatten()\n",
    "# print(passive_sample_weights.shape)\n",
    "\n",
    "# sample_weights = np.hstack((active_sample_weights, passive_sample_weights))\n",
    "# print(sample_weights.shape)\n",
    "# all_trial_spsm = np.concatenate((active_trial_spsm, passive_trial_spsm),axis=0)\n",
    "# print(all_trial_spsm.shape)\n",
    "\n",
    "# all_data = np.array(dataset_10ms.data.spikes_smth_40)\n",
    "# print(all_data.shape)\n",
    "# if not np.isnan(all_trial_spsm).any():\n",
    "#     scaler = StandardScaler()\n",
    "#     X_trial = scaler.fit_transform(all_trial_spsm,sample_weight=sample_weights)\n",
    "#     sca = SCA(n_components=n_dims)\n",
    "#     sca.fit(X_trial) # scaler and sca fit to trial data\n",
    "#     X_all = scaler.transform(all_data) #scaler and sca transform all data\n",
    "#     SCA_data = sca.transform(X_all)\n",
    "# print(SCA_data.shape)\n",
    "# dataset_10ms.add_continuous_data(SCA_data,'SCA_40')\n",
    "# print('SCA_40 var explained:',sca.r2_score)\n",
    "# ssa_order_smth40=np.argsort(-np.array(sca.explained_squared_activity))\n",
    "# print('SCA_40 activity explained:',sca.explained_squared_activity[ssa_order_smth40])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = np.array(dataset_10ms.data.spikes)\n",
    "# X_all = scaler.transform(all_data) #scaler and sca transform all data\n",
    "# SCA_data = sca.transform(X_all)\n",
    "# print(SCA_data.shape)\n",
    "# dataset_10ms.add_continuous_data(SCA_data,'SCA')\n",
    "# print('SCA var explained:',sca.r2_score)\n",
    "# ssa_order_smth40=np.argsort(-np.array(sca.explained_squared_activity))\n",
    "# print('SCA_40 activity explained:',sca.explained_squared_activity[ssa_order_smth40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot PCA projections over trial, for different reaching directions\n",
    "# plot_field = 'SCA_40'\n",
    "# order = ssa_order_smth40\n",
    "\n",
    "# pred_range = (-100, 1000)\n",
    "# trial_mask = active_mask\n",
    "# cond_dict = active_cond_dict\n",
    "# n_timepoints = int((pred_range[1] - pred_range[0])/dataset_10ms.bin_width)\n",
    "# # n_trials = dataset_10ms.trial_info.loc[trial_mask].shape[0]\n",
    "# data = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range=pred_range, ignored_trials=~trial_mask)\n",
    "# n_trials = data['trial_id'].nunique()\n",
    "# trials_pca = nans([n_trials,n_timepoints,n_dims])\n",
    "# i = 0\n",
    "# for idx, trial in data.groupby('trial_id'):\n",
    "#     trials_pca[i,:,:]=trial[plot_field].to_numpy()\n",
    "#     i+=1\n",
    "# print(trials_pca.shape)\n",
    "\n",
    "# plot_dir = np.array([0,45,90,135,180,225,270,315]) \n",
    "# directions = np.array([0,45,90,135,180,225,270,315])\n",
    "\n",
    "# # plot_dir = np.array([0,90,180,270]) \n",
    "# # directions = np.array([0,90,180,270])\n",
    "\n",
    "# x_axis = np.arange(pred_range[0], pred_range[1], dataset_10ms.bin_width)\n",
    "\n",
    "# # define some useful time points\n",
    "# move_idx=0\n",
    "# ret_idx = 120\n",
    "\n",
    "# import matplotlib as mpl\n",
    "# cmap = plt.get_cmap('coolwarm',len(plot_dir))\n",
    "# custom_palette = [mpl.colors.rgb2hex(cmap(i)) for i in range(len(plot_dir))]\n",
    "\n",
    "# plot_dims = 10\n",
    "\n",
    "# fig,ax=plt.subplots(plot_dims,1,figsize=(10,12))\n",
    "# for i in range(plot_dims):\n",
    "#     for j in range(len(plot_dir)):\n",
    "#         color = custom_palette[j]\n",
    "#         dir_idx = np.argwhere(directions == plot_dir[j])[0]\n",
    "#         cond_mean_proj = np.mean(trials_pca[np.argwhere(cond_dict==dir_idx).flatten(),:,:], axis = 0)[:,order[i]] \n",
    "#         pca_mean = np.mean(data[plot_field].to_numpy(),axis = 0)[order[i]] \n",
    "#         ax[i].plot(x_axis,cond_mean_proj - pca_mean,linewidth=2.25,color = color,label = plot_dir[j])\n",
    "        \n",
    "#         ax[i].axvline(move_idx, color='k',linewidth = .5)\n",
    "#         ax[i].axvline(ret_idx, color='k',linewidth = .5)\n",
    "#         ax[i].set_xlim([-100,1000])\n",
    "#         # ax[i].set_ylim([-15, 15])\n",
    "#         ax[i].axhline(0,color ='k',ls = '--')\n",
    "#         if i<plot_dims-1:\n",
    "#             ax[i].set_xticks([])\n",
    "#         else:\n",
    "#             ax[i].set_xlabel('Time after movement onset (ms)')\n",
    "            \n",
    "#         ax[i].set_yticks([])\n",
    "#         ax[i].set_ylabel('Dim. '+str(i+1))\n",
    "\n",
    "#     ax[0].set_title('Active trials')\n",
    "    \n",
    "# plt.legend(bbox_to_anchor = (1, 1), loc = 'upper left')\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(figDir + monkey + '_PCA_active.pdf',dpi = 'figure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_10ms\n",
    "x_field = 'CD_proj'\n",
    "y_field ='hand_vel'\n",
    "n_features = 2\n",
    "lag_axis = np.arange(-300,320,20)\n",
    "trial_mask = active_mask\n",
    "cond_dict = active_cond_dict\n",
    "# trial_mask = passive_mask\n",
    "# cond_dict = passive_cond_dict\n",
    "\n",
    "figDir = '/Users/sherryan/area2_population_analysis/figures_plus/PCA/'\n",
    "dim = dataset_10ms.data[x_field].shape[1]\n",
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "filter = True\n",
    "filter_type = 'causal'\n",
    "init = None\n",
    "pred_range = (-500, 1400)\n",
    "r2_array = nans([len(lag_axis)]); r2_wo_filter_array = nans([len(lag_axis)]); \n",
    "x_r2_array = nans([len(lag_axis)]); y_r2_array = nans([len(lag_axis)])\n",
    "r_array = nans([len(lag_axis)])\n",
    "coef_array = nans([len(lag_axis),2,dim])\n",
    "sigmas_list = []\n",
    "if filter:\n",
    "    for i in range(len(lag_axis)):\n",
    "        lag = lag_axis[i]\n",
    "        print(lag)\n",
    "        r2, r2_wo, ldgf, vel_df, r2_arr, sigmas = fit_and_predict_LDGF(dataset, trial_mask, 'move_onset_time',pred_range, lag, x_field, y_field,cond_dict,filter=filter,filter_type=filter_type,init=init)\n",
    "        coef,bias = ldgf.params['weight'],ldgf.params['bias']\n",
    "        r2_array[i] = r2; r2_wo_filter_array[i] = r2_wo\n",
    "        x_r2_array[i] = r2_arr[0]; y_r2_array[i] = r2_arr[1]\n",
    "        r = scipy.stats.pearsonr(vel_df[y_field].to_numpy().reshape(-1), vel_df['pred_vel'].to_numpy().reshape(-1))[0]\n",
    "        r_array[i] = r\n",
    "        coef_array[i,:,:] = coef\n",
    "        sigmas_list.append(sigmas)\n",
    "    time_max = lag_axis[np.argmax(r2_array)]\n",
    "    print(np.max(r2_array))\n",
    "    ldgf_best, vel_df, best_sigmas = retrieve_LDGF(dataset, trial_mask, 'move_onset_time',pred_range, time_max, x_field, y_field, cond_dict=cond_dict,filter=filter,filter_type=filter_type,init=init)\n",
    "    print(best_sigmas)\n",
    "    best_coef,best_intercept = ldgf_best.params['weight'], ldgf_best.params['bias']\n",
    "    np.savez(monkey+'_'+x_field+'_'+y_field+'_'+filter_type+'_81filter', r2_array = r2_array, r2_wo_filter_array = r2_wo_filter_array, x_r2_array = x_r2_array, y_r2_array = y_r2_array, r_array = r_array,\\\n",
    "            coef_array = coef_array, time_max = time_max, best_coef = best_coef, best_intercept = best_intercept, best_sigmas = best_sigmas) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "filter = True\n",
    "filter_type = 'gaussian'\n",
    "init = None\n",
    "pred_range = (-500, 1400)\n",
    "r2_array = nans([len(lag_axis)]); r2_wo_filter_array = nans([len(lag_axis)]); \n",
    "x_r2_array = nans([len(lag_axis)]); y_r2_array = nans([len(lag_axis)])\n",
    "r_array = nans([len(lag_axis)])\n",
    "coef_array = nans([len(lag_axis),2,dim])\n",
    "sigmas_list = []\n",
    "if filter:\n",
    "    for i in range(len(lag_axis)):\n",
    "        lag = lag_axis[i]\n",
    "        print(lag)\n",
    "        r2, r2_wo, ldgf, vel_df, r2_arr, sigmas = fit_and_predict_LDGF(dataset, trial_mask, 'move_onset_time',pred_range, lag, x_field, y_field,cond_dict,filter=filter,filter_type=filter_type,init=init)\n",
    "        coef,bias = ldgf.params['weight'],ldgf.params['bias']\n",
    "        r2_array[i] = r2; r2_wo_filter_array[i] = r2_wo\n",
    "        x_r2_array[i] = r2_arr[0]; y_r2_array[i] = r2_arr[1]\n",
    "        r = scipy.stats.pearsonr(vel_df[y_field].to_numpy().reshape(-1), vel_df['pred_vel'].to_numpy().reshape(-1))[0]\n",
    "        r_array[i] = r\n",
    "        coef_array[i,:,:] = coef\n",
    "        sigmas_list.append(sigmas)\n",
    "    time_max = lag_axis[np.argmax(r2_array)]\n",
    "    print(np.max(r2_array))\n",
    "    ldgf_best, vel_df, best_sigmas = retrieve_LDGF(dataset, trial_mask, 'move_onset_time',pred_range, time_max, x_field, y_field, cond_dict=cond_dict,filter=filter,filter_type=filter_type,init=init)\n",
    "    print(best_sigmas)\n",
    "    best_coef,best_intercept = ldgf_best.params['weight'], ldgf_best.params['bias']\n",
    "    np.savez(monkey+'_'+x_field+'_'+y_field+'_'+filter_type+'_81filter', r2_array = r2_array, r2_wo_filter_array = r2_wo_filter_array, x_r2_array = x_r2_array, y_r2_array = y_r2_array, r_array = r_array,\\\n",
    "            coef_array = coef_array, time_max = time_max, best_coef = best_coef, best_intercept = best_intercept, best_sigmas = best_sigmas) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "filter = True\n",
    "filter_type = 'anti-causal'\n",
    "init = None\n",
    "pred_range = (-500, 1400)\n",
    "r2_array = nans([len(lag_axis)]); r2_wo_filter_array = nans([len(lag_axis)]); \n",
    "x_r2_array = nans([len(lag_axis)]); y_r2_array = nans([len(lag_axis)])\n",
    "r_array = nans([len(lag_axis)])\n",
    "coef_array = nans([len(lag_axis),2,dim])\n",
    "sigmas_list = []\n",
    "if filter:\n",
    "    for i in range(len(lag_axis)):\n",
    "        lag = lag_axis[i]\n",
    "        print(lag)\n",
    "        r2, r2_wo, ldgf, vel_df, r2_arr, sigmas = fit_and_predict_LDGF(dataset, trial_mask, 'move_onset_time',pred_range, lag, x_field, y_field,cond_dict,filter=filter,filter_type=filter_type,init=init)\n",
    "        coef,bias = ldgf.params['weight'],ldgf.params['bias']\n",
    "        r2_array[i] = r2; r2_wo_filter_array[i] = r2_wo\n",
    "        x_r2_array[i] = r2_arr[0]; y_r2_array[i] = r2_arr[1]\n",
    "        r = scipy.stats.pearsonr(vel_df[y_field].to_numpy().reshape(-1), vel_df['pred_vel'].to_numpy().reshape(-1))[0]\n",
    "        r_array[i] = r\n",
    "        coef_array[i,:,:] = coef\n",
    "        sigmas_list.append(sigmas)\n",
    "    time_max = lag_axis[np.argmax(r2_array)]\n",
    "    print(np.max(r2_array))\n",
    "    ldgf_best, vel_df, best_sigmas = retrieve_LDGF(dataset, trial_mask, 'move_onset_time',pred_range, time_max, x_field, y_field, cond_dict=cond_dict,filter=filter,filter_type=filter_type,init=init)\n",
    "    print(best_sigmas)\n",
    "    best_coef,best_intercept = ldgf_best.params['weight'], ldgf_best.params['bias']\n",
    "    np.savez(monkey+'_'+x_field+'_'+y_field+'_'+filter_type+'_81filter', r2_array = r2_array, r2_wo_filter_array = r2_wo_filter_array, x_r2_array = x_r2_array, y_r2_array = y_r2_array, r_array = r_array,\\\n",
    "            coef_array = coef_array, time_max = time_max, best_coef = best_coef, best_intercept = best_intercept, best_sigmas = best_sigmas) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_10ms\n",
    "x_field = 'FB_proj'\n",
    "y_field ='hand_vel'\n",
    "n_features = 2\n",
    "lag_axis = np.arange(-300,320,20)\n",
    "trial_mask = active_mask\n",
    "cond_dict = active_cond_dict\n",
    "# trial_mask = passive_mask\n",
    "# cond_dict = passive_cond_dict\n",
    "\n",
    "figDir = '/Users/sherryan/area2_population_analysis/figures_plus/PCA/'\n",
    "dim = dataset_10ms.data[x_field].shape[1]\n",
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "filter = True\n",
    "filter_type = 'causal'\n",
    "init = None\n",
    "pred_range = (-500, 1400)\n",
    "r2_array = nans([len(lag_axis)]); r2_wo_filter_array = nans([len(lag_axis)]); \n",
    "x_r2_array = nans([len(lag_axis)]); y_r2_array = nans([len(lag_axis)])\n",
    "r_array = nans([len(lag_axis)])\n",
    "coef_array = nans([len(lag_axis),2,dim])\n",
    "sigmas_list = []\n",
    "if filter:\n",
    "    for i in range(len(lag_axis)):\n",
    "        lag = lag_axis[i]\n",
    "        print(lag)\n",
    "        r2, r2_wo, ldgf, vel_df, r2_arr, sigmas = fit_and_predict_LDGF(dataset, trial_mask, 'move_onset_time',pred_range, lag, x_field, y_field,cond_dict,filter=filter,filter_type=filter_type,init=init)\n",
    "        coef,bias = ldgf.params['weight'],ldgf.params['bias']\n",
    "        r2_array[i] = r2; r2_wo_filter_array[i] = r2_wo\n",
    "        x_r2_array[i] = r2_arr[0]; y_r2_array[i] = r2_arr[1]\n",
    "        r = scipy.stats.pearsonr(vel_df[y_field].to_numpy().reshape(-1), vel_df['pred_vel'].to_numpy().reshape(-1))[0]\n",
    "        r_array[i] = r\n",
    "        coef_array[i,:,:] = coef\n",
    "        sigmas_list.append(sigmas)\n",
    "    time_max = lag_axis[np.argmax(r2_array)]\n",
    "    print(np.max(r2_array))\n",
    "    ldgf_best, vel_df, best_sigmas = retrieve_LDGF(dataset, trial_mask, 'move_onset_time',pred_range, time_max, x_field, y_field, cond_dict=cond_dict,filter=filter,filter_type=filter_type,init=init)\n",
    "    print(best_sigmas)\n",
    "    best_coef,best_intercept = ldgf_best.params['weight'], ldgf_best.params['bias']\n",
    "    np.savez(monkey+'_'+x_field+'_'+y_field+'_'+filter_type+'_81filter', r2_array = r2_array, r2_wo_filter_array = r2_wo_filter_array, x_r2_array = x_r2_array, y_r2_array = y_r2_array, r_array = r_array,\\\n",
    "            coef_array = coef_array, time_max = time_max, best_coef = best_coef, best_intercept = best_intercept, best_sigmas = best_sigmas) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "filter = True\n",
    "filter_type = 'gaussian'\n",
    "init = None\n",
    "pred_range = (-500, 1400)\n",
    "r2_array = nans([len(lag_axis)]); r2_wo_filter_array = nans([len(lag_axis)]); \n",
    "x_r2_array = nans([len(lag_axis)]); y_r2_array = nans([len(lag_axis)])\n",
    "r_array = nans([len(lag_axis)])\n",
    "coef_array = nans([len(lag_axis),2,dim])\n",
    "sigmas_list = []\n",
    "if filter:\n",
    "    for i in range(len(lag_axis)):\n",
    "        lag = lag_axis[i]\n",
    "        print(lag)\n",
    "        r2, r2_wo, ldgf, vel_df, r2_arr, sigmas = fit_and_predict_LDGF(dataset, trial_mask, 'move_onset_time',pred_range, lag, x_field, y_field,cond_dict,filter=filter,filter_type=filter_type,init=init)\n",
    "        coef,bias = ldgf.params['weight'],ldgf.params['bias']\n",
    "        r2_array[i] = r2; r2_wo_filter_array[i] = r2_wo\n",
    "        x_r2_array[i] = r2_arr[0]; y_r2_array[i] = r2_arr[1]\n",
    "        r = scipy.stats.pearsonr(vel_df[y_field].to_numpy().reshape(-1), vel_df['pred_vel'].to_numpy().reshape(-1))[0]\n",
    "        r_array[i] = r\n",
    "        coef_array[i,:,:] = coef\n",
    "        sigmas_list.append(sigmas)\n",
    "    time_max = lag_axis[np.argmax(r2_array)]\n",
    "    print(np.max(r2_array))\n",
    "    ldgf_best, vel_df, best_sigmas = retrieve_LDGF(dataset, trial_mask, 'move_onset_time',pred_range, time_max, x_field, y_field, cond_dict=cond_dict,filter=filter,filter_type=filter_type,init=init)\n",
    "    print(best_sigmas)\n",
    "    best_coef,best_intercept = ldgf_best.params['weight'], ldgf_best.params['bias']\n",
    "    np.savez(monkey+'_'+x_field+'_'+y_field+'_'+filter_type+'_81filter', r2_array = r2_array, r2_wo_filter_array = r2_wo_filter_array, x_r2_array = x_r2_array, y_r2_array = y_r2_array, r_array = r_array,\\\n",
    "            coef_array = coef_array, time_max = time_max, best_coef = best_coef, best_intercept = best_intercept, best_sigmas = best_sigmas) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "filter = True\n",
    "filter_type = 'anti-causal'\n",
    "init = None\n",
    "pred_range = (-500, 1400)\n",
    "r2_array = nans([len(lag_axis)]); r2_wo_filter_array = nans([len(lag_axis)]); \n",
    "x_r2_array = nans([len(lag_axis)]); y_r2_array = nans([len(lag_axis)])\n",
    "r_array = nans([len(lag_axis)])\n",
    "coef_array = nans([len(lag_axis),2,dim])\n",
    "sigmas_list = []\n",
    "if filter:\n",
    "    for i in range(len(lag_axis)):\n",
    "        lag = lag_axis[i]\n",
    "        print(lag)\n",
    "        r2, r2_wo, ldgf, vel_df, r2_arr, sigmas = fit_and_predict_LDGF(dataset, trial_mask, 'move_onset_time',pred_range, lag, x_field, y_field,cond_dict,filter=filter,filter_type=filter_type,init=init)\n",
    "        coef,bias = ldgf.params['weight'],ldgf.params['bias']\n",
    "        r2_array[i] = r2; r2_wo_filter_array[i] = r2_wo\n",
    "        x_r2_array[i] = r2_arr[0]; y_r2_array[i] = r2_arr[1]\n",
    "        r = scipy.stats.pearsonr(vel_df[y_field].to_numpy().reshape(-1), vel_df['pred_vel'].to_numpy().reshape(-1))[0]\n",
    "        r_array[i] = r\n",
    "        coef_array[i,:,:] = coef\n",
    "        sigmas_list.append(sigmas)\n",
    "    time_max = lag_axis[np.argmax(r2_array)]\n",
    "    print(np.max(r2_array))\n",
    "    ldgf_best, vel_df, best_sigmas = retrieve_LDGF(dataset, trial_mask, 'move_onset_time',pred_range, time_max, x_field, y_field, cond_dict=cond_dict,filter=filter,filter_type=filter_type,init=init)\n",
    "    print(best_sigmas)\n",
    "    best_coef,best_intercept = ldgf_best.params['weight'], ldgf_best.params['bias']\n",
    "    np.savez(monkey+'_'+x_field+'_'+y_field+'_'+filter_type+'_81filter', r2_array = r2_array, r2_wo_filter_array = r2_wo_filter_array, x_r2_array = x_r2_array, y_r2_array = y_r2_array, r_array = r_array,\\\n",
    "            coef_array = coef_array, time_max = time_max, best_coef = best_coef, best_intercept = best_intercept, best_sigmas = best_sigmas) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monkey = \"Han_20171207\"\n",
    "monkey = 'Duncan_20190710'\n",
    "filter_type = 'causal'\n",
    "lag_axis = np.arange(-300,320,20)\n",
    "\n",
    "foldername = \"/Users/sherryan/area2_population_analysis/\"\n",
    "\n",
    "x_field = 'CD_FB_proj'\n",
    "y_field ='hand_vel'\n",
    "with np.load(foldername+monkey+'_'+x_field+'_'+y_field+'_'+filter_type+'_81filter_100_v6'+'.npz', allow_pickle=True) as data:\n",
    "    cd_fb_r2_array = data['r2_array']\n",
    "    cd_fb_x_r2_array = data['x_r2_array']\n",
    "    cd_fb_y_r2_array = data['y_r2_array']\n",
    "    cd_fb_r_array = data['r_array']\n",
    "    cd_fb_r2_wo_filter_array = data['r2_wo_filter_array']\n",
    "    cd_fb_coef_array = data['coef_array']\n",
    "    cd_fb_time_max = data['time_max']\n",
    "    cd_fb_best_coef = data['best_coef']\n",
    "    cd_fb_best_intercept = data['best_intercept']\n",
    "    cd_fb_best_sigmas = data['best_sigmas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'CD_proj'\n",
    "with np.load(monkey+'_'+x_field+'_'+y_field+'_'+filter_type+'_81filter_100_v6'+'.npz', allow_pickle=True) as data:\n",
    "    cd_r2_array = data['r2_array']\n",
    "    cd_x_r2_array = data['x_r2_array']\n",
    "    cd_y_r2_array = data['y_r2_array']\n",
    "    cd_r_array = data['r_array']\n",
    "    cd_r2_wo_filter_array = data['r2_wo_filter_array']\n",
    "    cd_coef_array = data['coef_array']\n",
    "    cd_time_max = data['time_max']\n",
    "    cd_best_coef = data['best_coef']\n",
    "    cd_best_intercept = data['best_intercept']\n",
    "    cd_best_sigmas = data['best_sigmas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'FB_proj'\n",
    "with np.load(monkey+'_'+x_field+'_'+y_field+'_'+filter_type+'_81filter_100_v6'+'.npz', allow_pickle=True) as data:\n",
    "    fb_r2_array = data['r2_array']\n",
    "    fb_x_r2_array = data['x_r2_array']\n",
    "    fb_y_r2_array = data['y_r2_array']\n",
    "    fb_r_array = data['r_array']\n",
    "    fb_r2_wo_filter_array = data['r2_wo_filter_array']\n",
    "    fb_coef_array = data['coef_array']\n",
    "    fb_time_max = data['time_max']\n",
    "    fb_best_coef = data['best_coef']\n",
    "    fb_best_intercept = data['best_intercept']\n",
    "    fb_best_sigmas = data['best_sigmas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cd_best_sigmas)\n",
    "print(fb_best_sigmas)\n",
    "print(cd_fb_best_sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw = 3\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(lag_axis,cd_r2_array, label='CD',linewidth=lw,color='green')\n",
    "print(np.max(cd_r2_array))\n",
    "print(lag_axis[np.argmax(cd_r2_array)])\n",
    "plt.plot(lag_axis,fb_r2_array, label='FB',linewidth=lw,color='magenta')\n",
    "print(np.max(fb_r2_array))\n",
    "print(lag_axis[np.argmax(fb_r2_array)])\n",
    "plt.plot(lag_axis,cd_fb_r2_array, linewidth=lw,label='CD+FB',color='brown')\n",
    "print(np.max(cd_fb_r2_array))\n",
    "print(lag_axis[np.argmax(cd_fb_r2_array)])\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R2')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(lag_axis,cd_x_r2_array, linewidth=lw,label='CD',color='green')\n",
    "print(np.max(cd_x_r2_array))\n",
    "print(lag_axis[np.argmax(cd_x_r2_array)])\n",
    "plt.plot(lag_axis,fb_x_r2_array, linewidth=lw,label='FB',color='magenta')\n",
    "print(np.max(fb_x_r2_array))\n",
    "print(lag_axis[np.argmax(fb_x_r2_array)])\n",
    "plt.plot(lag_axis,cd_fb_x_r2_array, linewidth=lw,label='CD+FB',color='brown')\n",
    "print(np.max(cd_fb_x_r2_array))\n",
    "print(lag_axis[np.argmax(cd_fb_x_r2_array)])\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('X-R2')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(lag_axis,cd_y_r2_array, linewidth=lw,label='CD',color='green')\n",
    "print(np.max(cd_y_r2_array))\n",
    "print(lag_axis[np.argmax(cd_y_r2_array)])\n",
    "plt.plot(lag_axis,fb_y_r2_array,linewidth=lw,label='FB',color='magenta')\n",
    "print(np.max(fb_y_r2_array))\n",
    "print(lag_axis[np.argmax(fb_y_r2_array)])\n",
    "plt.plot(lag_axis,cd_fb_y_r2_array, linewidth=lw,label='CD+FB',color='brown')\n",
    "print(np.max(cd_fb_y_r2_array))\n",
    "print(lag_axis[np.argmax(cd_fb_y_r2_array)])\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('Y-R2')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lag_axis,cd_r2_wo_filter_array, linewidth=lw,label='CD',color='green')\n",
    "print(np.max(cd_r2_wo_filter_array))\n",
    "print(lag_axis[np.argmax(cd_r2_wo_filter_array)])\n",
    "plt.plot(lag_axis,fb_r2_wo_filter_array,linewidth=lw,label='FB',color='magenta')\n",
    "print(np.max(fb_r2_wo_filter_array))\n",
    "print(lag_axis[np.argmax(fb_r2_wo_filter_array)])\n",
    "plt.plot(lag_axis,cd_fb_r2_wo_filter_array, linewidth=lw,label='CD+FB',color='brown')\n",
    "print(np.max(cd_fb_r2_wo_filter_array))\n",
    "print(lag_axis[np.argmax(cd_fb_r2_wo_filter_array)])\n",
    "plt.title('w/o filter')\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_10ms\n",
    "x_field = 'CD_FB_proj'\n",
    "y_field ='hand_vel'\n",
    "lag_axis = np.arange(-300,320,20)\n",
    "trial_mask = active_mask\n",
    "cond_dict = active_cond_dict\n",
    "# trial_mask = passive_mask\n",
    "# cond_dict = passive_cond_dict\n",
    "\n",
    "figDir = '/Users/sherryan/area2_population_analysis/figures_plus/PCA/'\n",
    "dim = dataset_10ms.data[x_field].shape[1]\n",
    "filter = True\n",
    "filter_type='anti-causal'\n",
    "init = None\n",
    "pred_range = (-500, 1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(monkey+'_'+x_field+'_'+y_field+'_'+filter_type+'_81filter'+'.npz', allow_pickle=True) as data:\n",
    "    cd_fb_r2_array = data['r2_array']\n",
    "    cd_fb_x_r2_array = data['x_r2_array']\n",
    "    cd_fb_y_r2_array = data['y_r2_array']\n",
    "    cd_fb_r_array = data['r_array']\n",
    "    cd_fb_r2_wo_filter_array = data['r2_wo_filter_array']\n",
    "    cd_fb_coef_array = data['coef_array']\n",
    "    cd_fb_time_max = data['time_max']\n",
    "    cd_fb_best_coef = data['best_coef']\n",
    "    cd_fb_best_intercept = data['best_intercept']\n",
    "    cd_fb_best_sigmas = data['best_sigmas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaussian_x_r2_array = x_r2_array\n",
    "# gaussian_y_r2_array = y_r2_array\n",
    "# gaussian_r2_array = r2_array\n",
    "\n",
    "# causal_x_r2_array = x_r2_array\n",
    "# causal_y_r2_array = y_r2_array\n",
    "# causal_r2_array = r2_array\n",
    "\n",
    "anticausal_x_r2_array = x_r2_array\n",
    "anticausal_y_r2_array = y_r2_array\n",
    "anticausal_r2_array = r2_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.plot(lag_axis,(np.sum(abs(coef_array[:,:,order[i]]),axis=1)),label = str(i+1))\n",
    "plt.legend(fontsize=8)\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('Latent weight')\n",
    "plt.title(x_field)\n",
    "plt.show()\n",
    "for i in range(10):\n",
    "    plt.plot(lag_axis,np.sum(coef_array[:,:,order[i]],axis=1),label = str(i+1))\n",
    "plt.legend(fontsize=8)\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('Latent weight')\n",
    "plt.title(x_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.plot(lag_axis,abs(coef_array[:,0,order[i]]),label = str(i+1))\n",
    "plt.legend(fontsize=8)\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('Latent weight')\n",
    "plt.title(x_field+' x-dir')\n",
    "plt.show()\n",
    "for i in range(10):\n",
    "    plt.plot(lag_axis,abs(coef_array[:,1,order[i]]),label = str(i+1))\n",
    "plt.legend(fontsize=8)\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('Latent weight')\n",
    "plt.title(x_field+' y-dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time_max)\n",
    "ldgf_best, vel_df, sigmas = retrieve_LDGF(dataset, trial_mask, 'move_onset_time',pred_range, time_max, x_field, y_field, cond_dict=cond_dict,filter=filter,filter_type=filter_type,init=init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for plotting\n",
    "plot_dir = [0.0, 90.0, 180.0, 270.0] # limit plot directions to reduce cluttering\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "plot_dim = 'x' # plot x velocity \n",
    "\n",
    "x_axis = np.arange(-500,1400,dataset.bin_width)\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir == trial_dir].trial_id\n",
    "    for _, trial in vel_df[np.isin(vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        plt.plot(x_axis, trial[y_field][plot_dim], color=color, linewidth=0.5)\n",
    "        # plt.plot(x_axis, trial[y_field].to_numpy()[:,0], color=color, linewidth=0.5)\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "# plt.ylabel('Shoulder angle')\n",
    "# plt.ylabel('Shoulder velocity')\n",
    "# plt.ylabel('Muscle pc (au)')\n",
    "# plt.ylabel('Hand position (cm)')\n",
    "# plt.ylabel('Hand acceleration (cm/s^2)')\n",
    "plt.ylabel('Hand velocity (cm/s)')\n",
    "\n",
    "# plt.xlim([-100,500])\n",
    "# plt.ylim([-0.65,0.65])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + label + 'true.pdf', dpi = 'figure')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir == trial_dir].trial_id\n",
    "    for _, trial in vel_df[np.isin(vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "         plt.plot(x_axis, trial['pred_vel'][plot_dim], color=color, linewidth=0.5)\n",
    "        # plt.plot(x_axis, trial.pred_vel.to_numpy()[:,0], color=color, linewidth=0.5)\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "# plt.ylabel('Shoulder angle')\n",
    "# plt.ylabel('Shoulder velocity')\n",
    "# plt.ylabel('Muscle pc (au)')\n",
    "# plt.ylabel('Hand position (cm)')\n",
    "# plt.ylabel('Hand acceleration (cm/s^2)')\n",
    "plt.ylabel('Hand velocity  (cm/s)')\n",
    "# plt.xlim([-100,500])\n",
    "# plt.ylim([-0.65,0.65])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + label + str(0) +'_pred.pdf', dpi = 'figure')\n",
    "plt.show()\n",
    "\n",
    "plot_dim = 'y' # plot y velocity \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir == trial_dir].trial_id\n",
    "    for _, trial in vel_df[np.isin(vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        plt.plot(x_axis, trial[y_field][plot_dim], color=color, linewidth=0.5)\n",
    "        # plt.plot(x_axis, trial[y_field].to_numpy()[:,0], color=color, linewidth=0.5)\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "# plt.ylabel('Shoulder angle')\n",
    "# plt.ylabel('Shoulder velocity')\n",
    "# plt.ylabel('Muscle pc (au)')\n",
    "# plt.ylabel('Hand position (cm)')\n",
    "# plt.ylabel('Hand acceleration (cm/s^2)')\n",
    "plt.ylabel('Hand velocity (cm/s)')\n",
    "\n",
    "# plt.xlim([-100,500])\n",
    "# plt.ylim([-0.65,0.65])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + label + 'true.pdf', dpi = 'figure')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset.trial_info[dataset.trial_info.cond_dir == trial_dir].trial_id\n",
    "    for _, trial in vel_df[np.isin(vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "         plt.plot(x_axis, trial['pred_vel'][plot_dim], color=color, linewidth=0.5)\n",
    "        # plt.plot(x_axis, trial.pred_vel.to_numpy()[:,0], color=color, linewidth=0.5)\n",
    "plt.xlabel('Time after movement onset (ms)')\n",
    "# plt.ylabel('Shoulder angle')\n",
    "# plt.ylabel('Shoulder velocity')\n",
    "# plt.ylabel('Muscle pc (au)')\n",
    "# plt.ylabel('Hand position (cm)')\n",
    "# plt.ylabel('Hand acceleration (cm/s^2)')\n",
    "plt.ylabel('Hand velocity  (cm/s)')\n",
    "# plt.xlim([-100,500])\n",
    "# plt.ylim([-0.65,0.65])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + label + str(0) +'_pred.pdf', dpi = 'figure')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(lag_axis, x_r2_array,color = 'k')\n",
    "plt.axvline(time_max, color = 'k', linestyle='--')\n",
    "print(r2_array)\n",
    "print(time_max)\n",
    "# plt.title('R2 score predicting ' + y_field + ' ' + str(pred_range))\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('X R2')\n",
    "# plt.ylim([0.00,0.5])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey +'_acc_' + str(0) +'.pdf', dpi = 'figure')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(lag_axis, y_r2_array,color = 'k')\n",
    "plt.axvline(time_max, color = 'k', linestyle='--')\n",
    "print(r2_array)\n",
    "print(time_max)\n",
    "# plt.title('R2 score predicting ' + y_field + ' ' + str(pred_range))\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('Y R2')\n",
    "# plt.ylim([0.00,0.5])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey +'_acc_' + str(0) +'.pdf', dpi = 'figure')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(lag_axis, r2_array,color = 'k')\n",
    "plt.axvline(time_max, color = 'k', linestyle='--')\n",
    "print(r2_array)\n",
    "print(time_max)\n",
    "# plt.title('R2 score predicting ' + y_field + ' ' + str(pred_range))\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R2')\n",
    "# plt.ylim([0.00,0.5])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey +'_acc_' + str(0) +'.pdf', dpi = 'figure')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(lag_axis, r2_wo_filter_array,color = 'k')\n",
    "plt.axvline(time_max, color = 'k', linestyle='--')\n",
    "print(r2_wo_filter_array)\n",
    "print(time_max)\n",
    "# plt.title('R2 score predicting ' + y_field + ' ' + str(pred_range))\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R2_neuro')\n",
    "# plt.ylim([0.00,0.5])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey +'_acc_' + str(0) +'.pdf', dpi = 'figure')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "plt.plot(lag_axis, r_array,color = 'k')\n",
    "plt.axvline(time_max, color = 'k', linestyle='--')\n",
    "print(time_max)\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('r')\n",
    "# plt.ylim([0.05,0.8])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey +'_vel_' + str(0) +'.pdf', dpi = 'figure')\n",
    "plt.show()\n",
    "\n",
    "idx_max_pos = np.argwhere(r2_array == np.max(r2_array[np.argwhere(lag_axis==0)[0,0]:]))[0,0]\n",
    "idx_max_neg = np.argwhere(r2_array == np.max(r2_array[:np.argwhere(lag_axis==0)[0,0]]))[0,0]\n",
    "print(lag_axis[idx_max_pos])\n",
    "print(lag_axis[idx_max_neg])\n",
    "\n",
    "#For velocity, override max identification\n",
    "# idx_max_pos = np.argwhere(lag_axis==80)[0,0]\n",
    "# idx_max_neg = np.argwhere(lag_axis==-40)[0,0]\n",
    "\n",
    "ang_to_max_x = nans([len(lag_axis)])\n",
    "ang_to_max_y = nans([len(lag_axis)])\n",
    "# ang_to_max_z = nans([len(lag_axis)])\n",
    "for i in range(0, len(coef_array)):\n",
    "    ang_to_max_x[i] = math.degrees(angle_between(coef_array[i,0,:],coef_array[idx_max_neg,0,:]))\n",
    "    ang_to_max_y[i] = math.degrees(angle_between(coef_array[i,1,:],coef_array[idx_max_neg,1,:]))\n",
    "    # ang_to_max_z[i] = math.degrees(angle_between(coef_array[i,2,:],coef_array[idx_max_neg,2,:]))\n",
    "fig, ax = plt.subplots()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "# plt.ylim([-5, 130])\n",
    "plt.xlim([-310, 310])\n",
    "plt.scatter(lag_axis, ang_to_max_x,label = 'x',color = 'green')\n",
    "plt.scatter(lag_axis, ang_to_max_y,label = 'y',color = 'blue')\n",
    "# plt.scatter(lag_axis, ang_to_max_z,label = 'wrist_abduction',color = 'orange')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('Angle (degrees)')\n",
    "mean = np.mean([ang_to_max_x[idx_max_pos], ang_to_max_y[idx_max_pos]])\n",
    "print(mean)\n",
    "# plt.vlines(lag_axis[idx_max_pos],-5, mean, color = 'k',linestyle=\"dashed\")\n",
    "# plt.hlines(mean, -310, lag_axis[idx_max_pos], color = 'k',linestyle=\"dashed\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figDir + monkey + label + str(0) +'_angle.pdf', dpi = 'figure')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lag_axis,gaussian_r2_array, label='gaussian')\n",
    "print(np.max(gaussian_r2_array))\n",
    "print(lag_axis[np.argmax(gaussian_r2_array)])\n",
    "plt.plot(lag_axis,causal_r2_array, label='causal')\n",
    "print(np.max(causal_r2_array))\n",
    "print(lag_axis[np.argmax(causal_r2_array)])\n",
    "plt.plot(lag_axis,anticausal_r2_array, label='anti-causal')\n",
    "print(np.max(anticausal_r2_array))\n",
    "print(lag_axis[np.argmax(anticausal_r2_array)])\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lag_axis,gaussian_x_r2_array, label='gaussian')\n",
    "print(np.max(gaussian_x_r2_array))\n",
    "print(lag_axis[np.argmax(gaussian_x_r2_array)])\n",
    "plt.plot(lag_axis,causal_x_r2_array, label='causal')\n",
    "print(np.max(causal_x_r2_array))\n",
    "print(lag_axis[np.argmax(causal_x_r2_array)])\n",
    "plt.plot(lag_axis,anticausal_x_r2_array, label='anti-causal')\n",
    "print(np.max(anticausal_x_r2_array))\n",
    "print(lag_axis[np.argmax(anticausal_x_r2_array)])\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('X R2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lag_axis,gaussian_y_r2_array, label='gaussian')\n",
    "print(np.max(gaussian_y_r2_array))\n",
    "print(lag_axis[np.argmax(gaussian_y_r2_array)])\n",
    "plt.plot(lag_axis,causal_y_r2_array, label='causal')\n",
    "print(np.max(causal_y_r2_array))\n",
    "print(lag_axis[np.argmax(causal_y_r2_array)])\n",
    "plt.plot(lag_axis,anticausal_y_r2_array, label='anti-causal')\n",
    "print(np.max(anticausal_y_r2_array))\n",
    "print(lag_axis[np.argmax(anticausal_y_r2_array)])\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('Y R2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldername = \"~/area2_population_analysis/s1-kinematics/actpas_NWB/\"\n",
    "# monkey = \"Han_20171207\"\n",
    "# filename = foldername + monkey + \"_COactpas_TD_offset4.nwb\"\n",
    "\n",
    "monkey = 'Duncan_20190710'\n",
    "filename = foldername + monkey + \"_COactpas_offset4.nwb\"\n",
    "\n",
    "dataset_10ms = NWBDataset(filename, split_heldout=False)\n",
    "dataset_10ms.resample(10) #in 10-ms bin, has to resample first to avoid NaNs\n",
    "bin_width = dataset_10ms.bin_width\n",
    "print(bin_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'spikes'\n",
    "data = np.load(monkey+'_v4_unsmoothed100_cdfb_data_'+x_field+'.npz')\n",
    "data.files\n",
    "dataset_10ms.add_continuous_data(data['CD_FB_proj'],'CD_FB_proj')\n",
    "dataset_10ms.add_continuous_data(data['CD_proj'],'CD_proj')\n",
    "dataset_10ms.add_continuous_data(data['FB_proj'],'FB_proj')\n",
    "dataset_10ms.data.keys().unique(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = dataset_10ms.trial_info.shape[0]\n",
    "print(n_trials,'total trials')\n",
    "n_neurons = dataset_10ms.data.spikes.shape[1]\n",
    "print(n_neurons,'neurons')\n",
    "\n",
    "#make dictionary for trial condition (reaching directions) for Stratified CV\n",
    "dataset = dataset_10ms\n",
    "active_mask = (dataset.trial_info.ctr_hold_bump==0) & (dataset.trial_info['split'] != 'none')\n",
    "passive_mask = (dataset.trial_info.ctr_hold_bump==1) & (dataset.trial_info['split'] != 'none')\n",
    "nan_mask = (np.isnan(dataset.trial_info.ctr_hold_bump)) & (dataset.trial_info['split'] != 'none')\n",
    "all_mask = (dataset.trial_info['split'] != 'none')\n",
    "\n",
    "trial_mask = all_mask\n",
    "valid_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(valid_n_trials,'valid trials')\n",
    "\n",
    "trial_mask = active_mask\n",
    "active_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "active_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(active_n_trials,'active trials')\n",
    "\n",
    "trial_mask = passive_mask\n",
    "passive_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "passive_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(passive_n_trials,'passive trials')\n",
    "\n",
    "trial_mask = nan_mask\n",
    "nan_trials_idx = np.array(dataset.trial_info.loc[trial_mask]['trial_id'])\n",
    "nan_n_trials = dataset.trial_info.loc[trial_mask].shape[0]\n",
    "print(nan_n_trials,'reach bump trials')\n",
    "\n",
    "active_cond_dir_idx = []\n",
    "passive_cond_dir_idx = []\n",
    "nan_cond_dir_idx = []\n",
    "nan_bump_cond_dir_idx = []\n",
    "for direction in [0,45,90,135,180,225,270,315]:\n",
    "# for direction in [0,90,180,270]:\n",
    "    active_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & (dataset.trial_info['ctr_hold_bump'] == 0) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "    passive_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & (dataset.trial_info['ctr_hold_bump'] == 1) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "    nan_cond_dir_idx.append(np.where((dataset.trial_info['cond_dir']%360 == direction) & (np.isnan(dataset.trial_info.ctr_hold_bump)) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "    nan_bump_cond_dir_idx.append(np.where((dataset.trial_info['bump_dir']%360 == direction) & (np.isnan(dataset.trial_info.ctr_hold_bump)) & \\\n",
    "           (dataset.trial_info['split'] != 'none'))[0])\n",
    "\n",
    "active_cond_dict = nans([active_n_trials])\n",
    "i = 0\n",
    "for idx in active_trials_idx:\n",
    "    for cond in range(0,len(active_cond_dir_idx)):\n",
    "        if idx in active_cond_dir_idx[cond]:\n",
    "            active_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(active_cond_dict)\n",
    "print(len(active_cond_dict))\n",
    "\n",
    "passive_cond_dict = nans([passive_n_trials])\n",
    "i = 0\n",
    "for idx in passive_trials_idx:\n",
    "    for cond in range(0,len(passive_cond_dir_idx)):\n",
    "        if idx in passive_cond_dir_idx[cond]:\n",
    "            passive_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(passive_cond_dict)\n",
    "print(len(passive_cond_dict))\n",
    "\n",
    "nan_cond_dict = nans([nan_n_trials])\n",
    "i = 0\n",
    "for idx in nan_trials_idx:\n",
    "    for cond in range(0,len(nan_cond_dir_idx)):\n",
    "        if idx in nan_cond_dir_idx[cond]:\n",
    "            nan_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(nan_cond_dict)\n",
    "print(len(nan_cond_dict))\n",
    "\n",
    "nan_bump_cond_dict = nans([nan_n_trials])\n",
    "i = 0\n",
    "for idx in nan_trials_idx:\n",
    "    for cond in range(0,len(nan_bump_cond_dir_idx)):\n",
    "        if idx in nan_bump_cond_dir_idx[cond]:\n",
    "            nan_bump_cond_dict[i] = cond\n",
    "            break\n",
    "    i+=1\n",
    "print(nan_bump_cond_dict)\n",
    "print(len(nan_bump_cond_dict))\n",
    "\n",
    "if monkey =='Duncan_20190710':\n",
    "    active_df = dataset_10ms.make_trial_data(align_field='move_onset_time', align_range = (-100,100), ignored_trials = ~active_mask)\n",
    "    del_indices = list(set(active_trials_idx) - set(active_df['trial_id'].unique()))\n",
    "    print('was',active_n_trials,'active trials')\n",
    "    active_n_trials = active_n_trials - len(list(set(active_trials_idx) - set(active_df['trial_id'].unique())))\n",
    "    active_cond_dict = np.delete(active_cond_dict,np.argwhere(active_trials_idx==del_indices)[0])\n",
    "    print('now',active_n_trials,'active trials')\n",
    "    print(len(active_cond_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_type = 'gaussian'\n",
    "lag_axis = np.arange(-300,320,20)\n",
    "\n",
    "foldername = \"/Users/sherryan/area2_population_analysis/\"\n",
    "\n",
    "x_field = 'CD_FB_proj'\n",
    "y_field ='hand_vel'\n",
    "with np.load(foldername+monkey+'_'+x_field+'_'+y_field+'_'+filter_type+'_81filter_100_v4'+'.npz', allow_pickle=True) as data:\n",
    "    cd_fb_best_coef = data['best_coef']\n",
    "    cd_fb_best_intercept = data['best_intercept']\n",
    "    cd_fb_best_sigmas = data['best_sigmas']\n",
    "    cd_fb_time_max = data['time_max']\n",
    "\n",
    "x_field = 'CD_proj'\n",
    "y_field ='hand_vel'\n",
    "with np.load(foldername+monkey+'_'+x_field+'_'+y_field+'_'+filter_type+'_81filter_100_v4'+'.npz', allow_pickle=True) as data:\n",
    "    cd_best_coef = data['best_coef']\n",
    "    cd_best_intercept = data['best_intercept']\n",
    "    cd_best_sigmas = data['best_sigmas']\n",
    "    cd_time_max = data['time_max']\n",
    "\n",
    "x_field = 'FB_proj'\n",
    "y_field ='hand_vel'\n",
    "with np.load(foldername+monkey+'_'+x_field+'_'+y_field+'_'+filter_type+'_81filter_100_v4'+'.npz', allow_pickle=True) as data:\n",
    "    fb_best_coef = data['best_coef']\n",
    "    fb_best_intercept = data['best_intercept']\n",
    "    fb_best_sigmas = data['best_sigmas']\n",
    "    fb_time_max = data['time_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.signal import convolve\n",
    "import scipy\n",
    "def pred_with_new_LDGF(dataset, filter_type, trial_mask, align_field, align_range, lag, x_field, y_field, weights, offset, sigmas, train_align_field, train_range, train_best_lag, train_mask,filter_length=81):\n",
    "    def gaussian_filter(x, sigma,filter_length):\n",
    "        return np.exp(-0.5*((x)/sigma)**2)\n",
    "    def causal_filter(x, sigma,filter_length):\n",
    "        phi_x = np.exp(-0.5*((x)/sigma)**2)\n",
    "        phi_x[:filter_length//2] = 0\n",
    "        return phi_x\n",
    "    def anticausal_filter(x, sigma,filter_length):\n",
    "        phi_x = np.exp(-0.5*((x)/sigma)**2)\n",
    "        phi_x[-filter_length//2+1:] = 0\n",
    "        return phi_x\n",
    "    x_range = np.arange(-filter_length//2+1, filter_length//2+1)\n",
    "    \"\"\" Returns R2, r, and predictions using given weights, basically a matrix multiplication \"\"\"\n",
    "    vel_df = dataset.make_trial_data(align_field=align_field, align_range=align_range, ignored_trials=~trial_mask)\n",
    "\n",
    "    lag_align_range = (align_range[0] + lag, align_range[1] + lag)\n",
    "    rates_df = dataset.make_trial_data(align_field=align_field, align_range=lag_align_range, ignored_trials=~trial_mask)\n",
    "    n_trials = rates_df['trial_id'].nunique()\n",
    "    n_timepoints = int((align_range[1] - align_range[0])/dataset.bin_width)\n",
    "\n",
    "    rates_array = rates_df[x_field].to_numpy()\n",
    "    vel_array = vel_df[y_field].to_numpy()\n",
    "\n",
    "    train_lag_range = (train_range[0] + train_best_lag, train_range[1] + train_best_lag)\n",
    "    train_rates_df = dataset.make_trial_data(align_field=train_align_field, align_range=train_lag_range, ignored_trials=~train_mask, allow_overlap=True)\n",
    "    train_rates_array = train_rates_df[x_field].to_numpy()\n",
    "\n",
    "    X = (rates_array - np.nanmean(train_rates_array,axis=0))/np.nanstd(train_rates_array,axis=0)\n",
    "    Y_hat = X@weights.T + offset\n",
    "    Y_hat_reshaped = Y_hat.reshape(n_trials, n_timepoints, vel_array.shape[-1])\n",
    "\n",
    "    if filter_type == 'causal':\n",
    "        filter_list = [causal_filter(x_range,sigmas[j],filter_length) for j in range(len(sigmas))]\n",
    "    elif filter_type == 'gaussian':\n",
    "        filter_list = [gaussian_filter(x_range,sigmas[j],filter_length) for j in range(len(sigmas))]\n",
    "    elif filter_type == 'anti-causal':\n",
    "        filter_list = [anticausal_filter(x_range,sigmas[j],filter_length) for j in range(len(sigmas))]\n",
    "    else:\n",
    "        print('error: filter type not specified')\n",
    "        return\n",
    "    \n",
    "    Y_hat_with_filter = np.array([np.apply_along_axis(convolve, 1, Y_hat_reshaped[:,:,j], filter, mode='same') for j, filter in enumerate(filter_list)]).transpose(1, 2, 0)\n",
    "\n",
    "\n",
    "    train_vel_df = dataset.make_trial_data(align_field=train_align_field, align_range=train_range, ignored_trials=~train_mask, allow_overlap=True)\n",
    "    train_vel_array = train_vel_df[y_field].to_numpy()\n",
    "    Y_hat_trunc = Y_hat_with_filter[:,filter_length//2:-filter_length//2+1,:]\n",
    "    y_pred = (Y_hat_trunc + np.nanmean(train_vel_array,axis=0))\n",
    "    pred_vel = y_pred.reshape(-1, vel_array.shape[-1])\n",
    "\n",
    "    y = vel_array.reshape(n_trials, n_timepoints, -1)\n",
    "    y_trunc = y[:,filter_length//2:-filter_length//2+1,:] #match valid size\n",
    "    true_vel = y_trunc.reshape(-1, vel_array.shape[-1])\n",
    "            \n",
    "    R2_array = nans([true_vel.shape[1]])\n",
    "    for i in range(true_vel.shape[1]):\n",
    "        sses =get_sses_pred(true_vel[:,i],pred_vel[:,i])\n",
    "        sses_mean=get_sses_mean(true_vel[:,i])\n",
    "        R2_array[i] =1-np.sum(sses)/np.sum(sses_mean)    \n",
    "\n",
    "    \n",
    "    sses =get_sses_pred(true_vel,pred_vel)\n",
    "    sses_mean=get_sses_mean(true_vel)\n",
    "    R2 =1-np.sum(sses)/np.sum(sses_mean)     \n",
    "    \n",
    "    r = scipy.stats.pearsonr(true_vel.reshape(-1), pred_vel.reshape(-1))[0]\n",
    "    \n",
    "    if vel_array.shape[-1] == 2:\n",
    "        vel_df = pd.concat([vel_df, pd.DataFrame(pred_vel, columns=dataset._make_midx('pred_vel', ['x', 'y'], 2))], axis=1)\n",
    "    elif vel_array.shape[-1] == 3:\n",
    "        vel_df = pd.concat([vel_df, pd.DataFrame(pred_vel, columns=dataset._make_midx('pred_vel', ['x', 'y','z'], 3))], axis=1)\n",
    "    else:\n",
    "        vel_df = pd.concat([vel_df, pd.DataFrame(pred_vel, columns=dataset._make_midx('pred_vel', num_channels=vel_array.shape[-1]))], axis=1)\n",
    "    \n",
    "    return R2, r, R2_array, vel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_10ms\n",
    "x_field = 'CD_proj'\n",
    "y_field ='hand_vel'\n",
    "n_features = 2\n",
    "lag_axis = np.arange(-300,320,20)\n",
    "trial_mask = passive_mask\n",
    "cond_dict = passive_cond_dict\n",
    "best_coef = cd_best_coef\n",
    "best_offset = cd_best_intercept\n",
    "best_sigmas = cd_best_sigmas\n",
    "best_lag = cd_time_max.flatten()[0]\n",
    "dim = dataset_10ms.data[x_field].shape[1]\n",
    "print(dim)\n",
    "\n",
    "pred_range = (-500, 520)\n",
    "cd_time_max = nans([len(lag_axis)]); \n",
    "cd_r2_array = nans([len(lag_axis)]);\n",
    "cd_x_r2_array = nans([len(lag_axis)]); cd_y_r2_array = nans([len(lag_axis)])\n",
    "cd_r_array = nans([len(lag_axis)])\n",
    "\n",
    "if filter:\n",
    "    for i in range(len(lag_axis)):\n",
    "        lag = lag_axis[i]\n",
    "        # print(lag)\n",
    "        r2, r, r2_arr, vel_df = pred_with_new_LDGF(dataset, filter_type,trial_mask, 'move_onset_time', pred_range, lag, x_field, y_field, best_coef, best_offset, best_sigmas, 'move_onset_time', (-500, 1500), best_lag, active_mask)\n",
    "        cd_r2_array[i] = r2; \n",
    "        # r2_wo_filter_array[i] = r2_wo\n",
    "        cd_x_r2_array[i] = r2_arr[0]; cd_y_r2_array[i] = r2_arr[1]\n",
    "        cd_r_array[i] = r\n",
    "        # print('R2=',r2)\n",
    "    cd_time_max = lag_axis[np.argmax(cd_time_max)]\n",
    "    print(np.max(cd_r2_array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_10ms\n",
    "x_field = 'FB_proj'\n",
    "y_field ='hand_vel'\n",
    "n_features = 2\n",
    "lag_axis = np.arange(-300,320,20)\n",
    "trial_mask = passive_mask\n",
    "cond_dict = passive_cond_dict\n",
    "best_coef = fb_best_coef\n",
    "best_offset = fb_best_intercept\n",
    "best_sigmas = fb_best_sigmas\n",
    "best_lag = fb_time_max.flatten()[0]\n",
    "dim = dataset_10ms.data[x_field].shape[1]\n",
    "print(dim)\n",
    "\n",
    "pred_range = (-500, 520)\n",
    "fb_time_max = nans([len(lag_axis)]); \n",
    "fb_r2_array = nans([len(lag_axis)]);\n",
    "fb_x_r2_array = nans([len(lag_axis)]); fb_y_r2_array = nans([len(lag_axis)])\n",
    "fb_r_array = nans([len(lag_axis)])\n",
    "\n",
    "if filter:\n",
    "    for i in range(len(lag_axis)):\n",
    "        lag = lag_axis[i]\n",
    "        # print(lag)\n",
    "        r2, r, r2_arr, vel_df = pred_with_new_LDGF(dataset, filter_type,trial_mask, 'move_onset_time', pred_range, lag, x_field, y_field, best_coef, best_offset, best_sigmas, 'move_onset_time', (-500, 1500), best_lag, active_mask)\n",
    "        fb_r2_array[i] = r2; \n",
    "        # r2_wo_filter_array[i] = r2_wo\n",
    "        fb_x_r2_array[i] = r2_arr[0]; fb_y_r2_array[i] = r2_arr[1]\n",
    "        fb_r_array[i] = r\n",
    "        # print('R2=',r2)\n",
    "    fb_time_max = lag_axis[np.argmax(fb_time_max)]\n",
    "    print(np.max(fb_r2_array))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_10ms\n",
    "x_field = 'CD_FB_proj'\n",
    "y_field ='hand_vel'\n",
    "n_features = 2\n",
    "lag_axis = np.arange(-300,320,20)\n",
    "trial_mask = passive_mask\n",
    "cond_dict = passive_cond_dict\n",
    "best_coef = cd_fb_best_coef\n",
    "best_offset = cd_fb_best_intercept\n",
    "best_sigmas = cd_fb_best_sigmas\n",
    "best_lag = cd_fb_time_max.flatten()[0]\n",
    "dim = dataset_10ms.data[x_field].shape[1]\n",
    "print(dim)\n",
    "\n",
    "pred_range = (-500, 520)\n",
    "cd_fb_time_max = nans([len(lag_axis)]); \n",
    "cd_fb_r2_array = nans([len(lag_axis)]);\n",
    "cd_fb_x_r2_array = nans([len(lag_axis)]); cd_fb_y_r2_array = nans([len(lag_axis)])\n",
    "cd_fb_r_array = nans([len(lag_axis)])\n",
    "\n",
    "if filter:\n",
    "    for i in range(len(lag_axis)):\n",
    "        lag = lag_axis[i]\n",
    "        # print(lag)\n",
    "        r2, r, r2_arr, vel_df = pred_with_new_LDGF(dataset, filter_type,trial_mask, 'move_onset_time', pred_range, lag, x_field, y_field, best_coef, best_offset, best_sigmas, 'move_onset_time', (-500, 1500), best_lag, active_mask)\n",
    "        cd_fb_r2_array[i] = r2; \n",
    "        # r2_wo_filter_array[i] = r2_wo\n",
    "        cd_fb_x_r2_array[i] = r2_arr[0]; cd_fb_y_r2_array[i] = r2_arr[1]\n",
    "        cd_fb_r_array[i] = r\n",
    "        # print('R2=',r2)\n",
    "    cd_fb_time_max = lag_axis[np.argmax(cd_fb_time_max)]\n",
    "    print(np.max(cd_fb_r2_array))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(monkey+'_'+x_field+'_cdfb_'+filter_type+'_81filter_Xpas_v4', cd_r2_array = cd_r2_array, cd_x_r2_array = cd_x_r2_array, cd_y_r2_array = cd_y_r2_array, cd_r_array = cd_r_array, cd_time_max = cd_time_max,\\\n",
    "        fb_r2_array = fb_r2_array, fb_x_r2_array = fb_x_r2_array, fb_y_r2_array = fb_y_r2_array, fb_r_array = fb_r_array, fb_time_max = fb_time_max,\\\n",
    "        cd_fb_r2_array = cd_fb_r2_array, cd_fb_x_r2_array = cd_fb_x_r2_array, cd_fb_y_r2_array = cd_fb_y_r2_array, cd_fb_r_array = cd_fb_r_array, cd_fb_time_max = cd_fb_time_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw = 2\n",
    "plt.plot(lag_axis,cd_r2_array, label='CD',linewidth=lw,color='green')\n",
    "print(np.max(cd_r2_array))\n",
    "print(lag_axis[np.argmax(cd_r2_array)])\n",
    "plt.plot(lag_axis,fb_r2_array, label='FB',linewidth=lw,color='magenta')\n",
    "print(np.max(fb_r2_array))\n",
    "print(lag_axis[np.argmax(fb_r2_array)])\n",
    "plt.plot(lag_axis,cd_fb_r2_array, linewidth=lw,label='CD+FB',color='brown')\n",
    "print(np.max(cd_fb_r2_array))\n",
    "print(lag_axis[np.argmax(cd_fb_r2_array)])\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lag_axis,cd_x_r2_array, linewidth=lw,label='CD',color='green')\n",
    "print(np.max(cd_x_r2_array))\n",
    "print(lag_axis[np.argmax(cd_x_r2_array)])\n",
    "plt.plot(lag_axis,fb_x_r2_array, linewidth=lw,label='FB',color='magenta')\n",
    "print(np.max(fb_x_r2_array))\n",
    "print(lag_axis[np.argmax(fb_x_r2_array)])\n",
    "plt.plot(lag_axis,cd_fb_x_r2_array, linewidth=lw,label='CD+FB',color='brown')\n",
    "print(np.max(cd_fb_x_r2_array))\n",
    "print(lag_axis[np.argmax(cd_fb_x_r2_array)])\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('X-R2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lag_axis,cd_y_r2_array, linewidth=lw,label='CD',color='green')\n",
    "print(np.max(cd_y_r2_array))\n",
    "print(lag_axis[np.argmax(cd_y_r2_array)])\n",
    "plt.plot(lag_axis,fb_y_r2_array,linewidth=lw,label='FB',color='magenta')\n",
    "print(np.max(fb_y_r2_array))\n",
    "print(lag_axis[np.argmax(fb_y_r2_array)])\n",
    "plt.plot(lag_axis,cd_fb_y_r2_array, linewidth=lw,label='CD+FB',color='brown')\n",
    "print(np.max(cd_fb_y_r2_array))\n",
    "print(lag_axis[np.argmax(cd_fb_y_r2_array)])\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('Y-R2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_10ms\n",
    "x_field = 'SCA'\n",
    "y_field ='hand_vel'\n",
    "lag_axis = np.arange(-300,320,20)\n",
    "trial_mask = active_mask\n",
    "cond_dict = active_cond_dict\n",
    "# trial_mask = passive_mask\n",
    "# cond_dict = passive_cond_dict\n",
    "\n",
    "dim = n_dims\n",
    "filter = True\n",
    "init = None\n",
    "pred_range = (-100, 1000)\n",
    "\n",
    "with np.load(monkey+'_'+x_field+'_'+y_field+'_filter'+'.npz', allow_pickle=True) as data:\n",
    "    r2_array = data['r2_array']\n",
    "    x_r2_array = data['x_r2_array']\n",
    "    y_r2_array = data['y_r2_array']\n",
    "    r_array = data['r_array']\n",
    "    r2_wo_filter_array = data['r2_wo_filter_array']\n",
    "    coef_array = data['coef_array']\n",
    "    ldgf_best = data['ldgf_best']\n",
    "    time_max = data['time_max']\n",
    "    best_coef = data['best_coef']\n",
    "    best_intercept = data['best_intercept']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "filter = True\n",
    "init = None\n",
    "pred_range = (-100, 1000)\n",
    "r2_array_sub = nans([len(lag_axis)]); r2_wo_filter_array_sub = nans([len(lag_axis)]); \n",
    "x_r2_array_sub = nans([len(lag_axis)]); y_r2_array_sub = nans([len(lag_axis)])\n",
    "r_array_sub = nans([len(lag_axis)])\n",
    "coef_array_sub = nans([len(lag_axis),2,dim])\n",
    "if filter:\n",
    "    for i in range(len(lag_axis)):\n",
    "        lag = lag_axis[i]\n",
    "        r2, r2_wo, ldgf, vel_df, x_r2, y_r2 = sub_and_predict_LDGF(dataset, trial_mask, 'move_onset_time',pred_range, lag, x_field, y_field, best_coef, cond_dict=cond_dict,filter=filter,init=init)\n",
    "        coef,bias = ldgf.params['weight'],ldgf.params['bias']\n",
    "        r2_array_sub[i] = r2; r2_wo_filter_array_sub[i] = r2_wo\n",
    "        x_r2_array_sub[i] = x_r2; y_r2_array_sub[i] = y_r2\n",
    "        r = scipy.stats.pearsonr(vel_df[y_field].to_numpy().reshape(-1), vel_df['pred_vel'].to_numpy().reshape(-1))[0]\n",
    "        r_array_sub[i] = r\n",
    "        coef_array_sub[i,:,:] = coef\n",
    "    time_max_sub = lag_axis[np.argmax(r2_array_sub)]\n",
    "    print(np.max(r2_array_sub))\n",
    "    _, _, ldgf_best_sub, _, _, _ = sub_and_predict_LDGF(dataset, trial_mask, 'move_onset_time',pred_range, lag, x_field, y_field, best_coef,cond_dict=cond_dict,filter=filter,init=init)\n",
    "    best_coef_sub,best_intercept_sub = ldgf_best_sub.params['weight'], ldgf_best_sub.params['bias']\n",
    "    np.savez(monkey+'_'+x_field+'_'+y_field +'_filter_sub', r2_array_sub = r2_array_sub, r2_wo_filter_array_sub = r2_wo_filter_array_sub, x_r2_array_sub = x_r2_array_sub, y_r2_array_sub = y_r2_array_sub, r_array_sub = r_array_sub,\\\n",
    "         coef_array_sub = coef_array_sub, ldgf_best_sub = ldgf_best_sub, time_max = time_max, time_max_sub = time_max_sub, best_coef = best_coef, best_coef_sub = best_coef_sub, best_intercept_sub = best_intercept_sub) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_10ms\n",
    "x_field = 'spikes'\n",
    "y_field ='hand_acc'\n",
    "lag_axis = np.arange(-300,320,20)\n",
    "trial_mask = active_mask\n",
    "cond_dict = active_cond_dict\n",
    "# trial_mask = passive_mask\n",
    "# cond_dict = passive_cond_dict\n",
    "\n",
    "figDir = '/Users/sherryan/area2_population_analysis/figures_plus/PCA/'\n",
    "dim = n_neurons\n",
    "filter = True\n",
    "init = None\n",
    "pred_range = (-100, 1000)\n",
    "\n",
    "with np.load(monkey+'_'+x_field+'_'+y_field+'_filter'+'.npz', allow_pickle=True) as data:\n",
    "    r2_array = data['r2_array']\n",
    "    x_r2_array = data['x_r2_array']\n",
    "    y_r2_array = data['y_r2_array']\n",
    "    r_array = data['r_array']\n",
    "    r2_wo_filter_array = data['r2_wo_filter_array']\n",
    "    coef_array = data['coef_array']\n",
    "    ldgf_best = data['ldgf_best']\n",
    "    time_max = data['time_max']\n",
    "    best_coef = data['best_coef']\n",
    "    best_intercept = data['best_intercept']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "filter = True\n",
    "init = None\n",
    "pred_range = (-100, 1000)\n",
    "r2_array_sub = nans([len(lag_axis)]); r2_wo_filter_array_sub = nans([len(lag_axis)]); \n",
    "x_r2_array_sub = nans([len(lag_axis)]); y_r2_array_sub = nans([len(lag_axis)])\n",
    "r_array_sub = nans([len(lag_axis)])\n",
    "coef_array_sub = nans([len(lag_axis),2,dim])\n",
    "if filter:\n",
    "    for i in range(len(lag_axis)):\n",
    "        lag = lag_axis[i]\n",
    "        r2, r2_wo, ldgf, vel_df, x_r2, y_r2 = sub_and_predict_LDGF(dataset, trial_mask, 'move_onset_time',pred_range, lag, x_field, y_field, best_coef, cond_dict=cond_dict,filter=filter,init=init)\n",
    "        coef,bias = ldgf.params['weight'],ldgf.params['bias']\n",
    "        r2_array_sub[i] = r2; r2_wo_filter_array_sub[i] = r2_wo\n",
    "        x_r2_array_sub[i] = x_r2; y_r2_array_sub[i] = y_r2\n",
    "        r = scipy.stats.pearsonr(vel_df[y_field].to_numpy().reshape(-1), vel_df['pred_vel'].to_numpy().reshape(-1))[0]\n",
    "        r_array_sub[i] = r\n",
    "        coef_array_sub[i,:,:] = coef\n",
    "    time_max_sub = lag_axis[np.argmax(r2_array_sub)]\n",
    "    print(np.max(r2_array_sub))\n",
    "    _, _, ldgf_best_sub, _, _, _ = sub_and_predict_LDGF(dataset, trial_mask, 'move_onset_time',pred_range, lag, x_field, y_field, best_coef,cond_dict=cond_dict,filter=filter,init=init)\n",
    "    best_coef_sub,best_intercept_sub = ldgf_best_sub.params['weight'], ldgf_best_sub.params['bias']\n",
    "    np.savez(monkey+'_'+x_field+'_'+y_field +'_filter_sub', r2_array_sub = r2_array_sub, r2_wo_filter_array_sub =r2_wo_filter_array_sub, x_r2_array_sub = x_r2_array_sub, y_r2_array_sub = y_r2_array_sub, r_array_sub = r_array_sub,\\\n",
    "         coef_array_sub = coef_array_sub, ldgf_best_sub = ldgf_best_sub,  time_max = time_max, time_max_sub = time_max_sub, best_coef = best_coef, best_coef_sub = best_coef_sub, best_intercept_sub = best_intercept_sub) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_10ms\n",
    "x_field = 'PCA'\n",
    "y_field ='hand_acc'\n",
    "lag_axis = np.arange(-300,320,20)\n",
    "trial_mask = active_mask\n",
    "cond_dict = active_cond_dict\n",
    "# trial_mask = passive_mask\n",
    "# cond_dict = passive_cond_dict\n",
    "\n",
    "figDir = '/Users/sherryan/area2_population_analysis/figures_plus/PCA/'\n",
    "dim = n_dims\n",
    "filter = True\n",
    "init = None\n",
    "pred_range = (-100, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(monkey+'_'+x_field+'_'+y_field+'_filter'+'.npz', allow_pickle=True) as data:\n",
    "    time_max = data['time_max']\n",
    "    best_coef = data['best_coef']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(monkey+'_'+x_field+'_'+y_field+'_filter_sub'+'.npz', allow_pickle=True) as data:\n",
    "    r2_array_sub = data['r2_array_sub']\n",
    "    x_r2_array_sub = data['x_r2_array_sub']\n",
    "    y_r2_array_sub = data['y_r2_array_sub']\n",
    "    r_array_sub = data['r_array_sub']\n",
    "    # r2_wo_filter_array_sub = data['r2_wo_filter_array_sub']\n",
    "    coef_array_sub = data['coef_array_sub']\n",
    "    ldgf_best_sub = data['ldgf_best_sub']\n",
    "    time_max_sub = data['time_max_sub']\n",
    "    best_coef_sub = data['best_coef_sub']\n",
    "    best_intercept_sub = data['best_intercept_sub']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time_max_sub)\n",
    "r2, r2_wo, ldgf, vel_df, x_r2, y_r2 = sub_and_predict_LDGF(dataset, trial_mask, 'move_onset_time',pred_range, time_max_sub, x_field, y_field, best_coef, cond_dict=cond_dict,filter=filter,init=init)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdt_env",
   "language": "python",
   "name": "sdt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
