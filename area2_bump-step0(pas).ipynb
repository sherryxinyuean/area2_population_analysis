{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8d2edb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: Xgboost package is not installed. You will be unable to use the xgboost decoder\n",
      "\n",
      "WARNING: Keras package is not installed. You will be unable to use all neural net decoders\n"
     ]
    }
   ],
   "source": [
    "from nlb_tools.nwb_interface import NWBDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "\n",
    "            >>> angle_between((1, 0, 0), (0, 1, 0))\n",
    "            1.5707963267948966 (90 deg)\n",
    "            >>> angle_between((1, 0, 0), (1, 0, 0))\n",
    "            0.0 (0 deg)\n",
    "            >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
    "            3.141592653589793 (180 deg)\n",
    "    \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
    "\n",
    "#Import standard packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import io\n",
    "from scipy import stats\n",
    "import pickle\n",
    "\n",
    "# If you would prefer to load the '.h5' example file rather than the '.pickle' example file. You need the deepdish package\n",
    "# import deepdish as dd \n",
    "\n",
    "#Import function to get the covariate matrix that includes spike history from previous bins\n",
    "from Neural_Decoding.preprocessing_funcs import get_spikes_with_history\n",
    "\n",
    "#Import metrics\n",
    "from Neural_Decoding.metrics import get_R2\n",
    "from Neural_Decoding.metrics import get_rho\n",
    "\n",
    "#Import decoder functions\n",
    "from Neural_Decoding.decoders import WienerCascadeDecoder\n",
    "from Neural_Decoding.decoders import WienerFilterDecoder\n",
    "from Neural_Decoding.decoders import DenseNNDecoder\n",
    "from Neural_Decoding.decoders import SimpleRNNDecoder\n",
    "from Neural_Decoding.decoders import GRUDecoder\n",
    "from Neural_Decoding.decoders import LSTMDecoder\n",
    "from Neural_Decoding.decoders import XGBoostDecoder\n",
    "from Neural_Decoding.decoders import SVRDecoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def get_sses_pred(y_test,y_test_pred):\n",
    "    sse=np.sum((y_test_pred-y_test)**2,axis=0)\n",
    "    return sse\n",
    "\n",
    "def get_sses_mean(y_test):\n",
    "    y_mean=np.mean(y_test,axis=0)\n",
    "    sse_mean=np.sum((y_test-y_mean)**2,axis=0)\n",
    "    return sse_mean\n",
    "\n",
    "def nans(shape, dtype=float):\n",
    "    a = np.empty(shape, dtype)\n",
    "    a.fill(np.nan)\n",
    "    return a\n",
    "\n",
    "def vector_reject(u,v):\n",
    "    #project u on v, subtract u1 from u\n",
    "    P = np.outer(v,(v.T))/(v@(v.T))\n",
    "    u_sub = u - P@u\n",
    "#     another calculation, to double-check\n",
    "#     v_norm = np.sqrt(sum(v**2))    \n",
    "#     proj_u_on_v = (np.dot(u, v)/v_norm**2)*v\n",
    "#     u_sub = u - proj_u_on_v\n",
    "    return u_sub\n",
    "\n",
    "def calc_proj_matrix(A):\n",
    "    return A@np.linalg.inv(A.T@A)@A.T\n",
    "def calc_proj(b, A):\n",
    "    P = calc_proj_matrix(A)\n",
    "    return P@b.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dca024",
   "metadata": {},
   "source": [
    "# Single Lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dc8cc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "foldername = \"~/area2_population_analysis/s1-kinematics/actpas_NWB/\"\n",
    "monkey = \"Lando_20170731\"\n",
    "filename = foldername + monkey + \"_COactpas_TD.nwb\"\n",
    "\n",
    "dataset_5ms = NWBDataset(filename, split_heldout=False)\n",
    "dataset_5ms.resample(5)\n",
    "dataset_5ms.smooth_spk(40, name='smth_40')\n",
    "bin_width = dataset_5ms.bin_width\n",
    "print(bin_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34b3102a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346 trials\n",
      "87 neurons\n",
      "(538986, 87)\n",
      "(538986, 87)\n",
      "(538986, 20)\n",
      "PCA total var explained: 0.47237034354476026\n"
     ]
    }
   ],
   "source": [
    "n_dims = 20 # for PCA\n",
    "\n",
    "active_mask = (~dataset_5ms.trial_info.ctr_hold_bump) & (dataset_5ms.trial_info.split != 'none')\n",
    "passive_mask = (dataset_5ms.trial_info.ctr_hold_bump) & (dataset_5ms.trial_info.split != 'none')\n",
    "\n",
    "\n",
    "trial_mask = passive_mask\n",
    "n_trials = dataset_5ms.trial_info.loc[trial_mask].shape[0]\n",
    "print(n_trials,'trials')\n",
    "n_neurons = dataset_5ms.data.spikes.shape[1]\n",
    "print(n_neurons,'neurons')\n",
    "\n",
    "all_data = np.array(dataset_5ms.data.spikes_smth_40)\n",
    "print(all_data.shape)\n",
    "data_for_pca = all_data[~np.isnan(all_data).any(axis=1)]\n",
    "print(data_for_pca.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(data_for_pca)\n",
    "pca = PCA(n_components=n_dims)\n",
    "X = pca.fit(X)\n",
    "\n",
    "PCA_data = nans([all_data.shape[0],n_dims])\n",
    "idx = 0\n",
    "for dp in all_data:\n",
    "    dp = dp.reshape((1, -1))\n",
    "    if np.isnan(dp).any():\n",
    "        dp_pca = nans([1,n_dims])\n",
    "    else:\n",
    "        dp_pca = pca.transform(scaler.transform(dp))\n",
    "    PCA_data[idx,:] = dp_pca\n",
    "    idx+=1\n",
    "print(PCA_data.shape)\n",
    "dataset_5ms.add_continuous_data(PCA_data,'PCA')\n",
    "print('PCA total var explained:',sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fb1fa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_train_test(X,y,training_set,test_set):\n",
    "    X_train = X[training_set,:,:]\n",
    "    X_test = X[test_set,:,:]\n",
    "    y_train = y[training_set,:,:]\n",
    "    y_test = y[test_set,:,:]\n",
    "\n",
    "    #flat by trials\n",
    "    X_flat_train = X_train.reshape((X_train.shape[0]*X_train.shape[1]),X_train.shape[2])\n",
    "    X_flat_test = X_test.reshape((X_test.shape[0]*X_test.shape[1]),X_test.shape[2])\n",
    "    y_train=y_train.reshape((y_train.shape[0]*y_train.shape[1]),y_train.shape[2])\n",
    "    y_test=y_test.reshape((y_test.shape[0]*y_test.shape[1]),y_test.shape[2])\n",
    "    \n",
    "    X_flat_train_mean=np.nanmean(X_flat_train,axis=0)\n",
    "    X_flat_train_std=np.nanstd(X_flat_train,axis=0)   \n",
    "    #array with only 0 will have 0 std and cause errors\n",
    "    X_flat_train_std[X_flat_train_std==0] = 1\n",
    "    \n",
    "    X_flat_train=(X_flat_train-X_flat_train_mean)/X_flat_train_std\n",
    "    X_flat_test=(X_flat_test-X_flat_train_mean)/X_flat_train_std\n",
    "    y_train_mean=np.mean(y_train,axis=0)\n",
    "    y_train=y_train-y_train_mean\n",
    "    y_test=y_test-y_train_mean    \n",
    "    \n",
    "    return X_flat_train,X_flat_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9f0762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict(dataset, trial_mask, align_field, align_range, lag, x_field, y_field):\n",
    "    \"\"\"Extracts spiking and kinematic data from selected trials and fits linear decoder\"\"\"\n",
    "    # Extract rate data from selected trials\n",
    "    vel_df = dataset.make_trial_data(align_field=align_field, align_range=align_range, ignored_trials=~trial_mask)\n",
    "    # Lag alignment for kinematics and extract kinematics data from selected trials\n",
    "    lag_align_range = (align_range[0] + lag, align_range[1] + lag)\n",
    "    rates_df = dataset.make_trial_data(align_field=align_field, align_range=lag_align_range, ignored_trials=~trial_mask)\n",
    "    \n",
    "    n_trials = rates_df['trial_id'].nunique()\n",
    "    n_timepoints = int((align_range[1] - align_range[0])/dataset.bin_width)\n",
    "    n_neurons = rates_df[x_field].shape[1]\n",
    "    \n",
    "    lr_all = GridSearchCV(Ridge(), {'alpha': np.logspace(-4, 1, 6)})\n",
    "    rates_array = rates_df[x_field].to_numpy()\n",
    "    vel_array = vel_df[y_field].to_numpy()\n",
    "    lr_all.fit(rates_array, vel_array)\n",
    "    pred_vel = lr_all.predict(rates_array)\n",
    "    vel_df = pd.concat([vel_df, pd.DataFrame(pred_vel, columns=dataset._make_midx('pred_vel', ['x', 'y'], 2))], axis=1)\n",
    "     \n",
    "    rates_array = rates_array.reshape(n_trials, n_timepoints, n_neurons)\n",
    "    vel_array = vel_array.reshape(n_trials, n_timepoints, 2)\n",
    "    \n",
    "    kf = KFold(n_splits=5,shuffle=True,random_state = 42)   \n",
    "    true_concat = nans([n_trials*n_timepoints,2])\n",
    "    pred_concat = nans([n_trials*n_timepoints,2])\n",
    "    trial_save_idx = 0\n",
    "    for training_set, test_set in kf.split(range(0,n_trials)):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test, y_train, y_test = process_train_test(rates_array,vel_array,training_set,test_set)\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-4, 1, 6)}) \n",
    "        lr.fit(X_train, y_train)\n",
    "        y_test_predicted = lr.predict(X_test)\n",
    "        \n",
    "        n = y_test_predicted.shape[0]\n",
    "        true_concat[trial_save_idx:trial_save_idx+n,:] = y_test\n",
    "        pred_concat[trial_save_idx:trial_save_idx+n,:] = y_test_predicted\n",
    "        trial_save_idx += n\n",
    "    \n",
    "    sses =get_sses_pred(true_concat,pred_concat)\n",
    "    sses_mean=get_sses_mean(true_concat)\n",
    "    R2 =1-np.sum(sses)/np.sum(sses_mean)     \n",
    "    print('R2:',R2) \n",
    "    return R2, lr_all.best_estimator_.coef_, vel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb0e7e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_and_predict(dataset, trial_mask, align_field, align_range, lag, x_field, y_field, weights):\n",
    "    \"\"\"Extracts spiking and kinematic data from selected trials and fits linear decoder\"\"\"\n",
    "    # Extract rate data from selected trials\n",
    "    vel_df = dataset.make_trial_data(align_field=align_field, align_range=align_range, ignored_trials=~trial_mask)\n",
    "    # Lag alignment for kinematics and extract kinematics data from selected trials\n",
    "    lag_align_range = (align_range[0] + lag, align_range[1] + lag)\n",
    "    rates_df = dataset.make_trial_data(align_field=align_field, align_range=lag_align_range, ignored_trials=~trial_mask)\n",
    "    \n",
    "    n_trials = rates_df['trial_id'].nunique()\n",
    "    n_timepoints = int((align_range[1] - align_range[0])/dataset.bin_width)\n",
    "    n_neurons = rates_df[x_field].shape[1]\n",
    "\n",
    "    rates_array = rates_df[x_field].to_numpy() - calc_proj(rates_df[x_field].to_numpy(),weights.T).T\n",
    "    vel_array = vel_df[y_field].to_numpy()\n",
    "    \n",
    "    lr_all = GridSearchCV(Ridge(), {'alpha': np.logspace(-4, 1, 6)})\n",
    "    lr_all.fit(rates_array, vel_array)\n",
    "    pred_vel = lr_all.predict(rates_array)\n",
    "    vel_df = pd.concat([vel_df, pd.DataFrame(pred_vel, columns=dataset._make_midx('pred_vel', ['x', 'y'], 2))], axis=1)\n",
    "         \n",
    "    rates_array = rates_array.reshape(n_trials, n_timepoints, n_neurons)\n",
    "    vel_array = vel_array.reshape(n_trials, n_timepoints, 2)\n",
    "    \n",
    "    kf = KFold(n_splits=5,shuffle=True,random_state = 42)   \n",
    "    true_concat = nans([n_trials*n_timepoints,2])\n",
    "    pred_concat = nans([n_trials*n_timepoints,2])\n",
    "    trial_save_idx = 0\n",
    "    for training_set, test_set in kf.split(range(0,n_trials)):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test, y_train, y_test = process_train_test(rates_array,vel_array,training_set,test_set)\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-4, 1, 6)}) \n",
    "        lr.fit(X_train, y_train)\n",
    "        y_test_predicted = lr.predict(X_test)\n",
    "        \n",
    "        n = y_test_predicted.shape[0]\n",
    "        true_concat[trial_save_idx:trial_save_idx+n,:] = y_test\n",
    "        pred_concat[trial_save_idx:trial_save_idx+n,:] = y_test_predicted\n",
    "        trial_save_idx += n\n",
    "    \n",
    "    sses =get_sses_pred(true_concat,pred_concat)\n",
    "    sses_mean=get_sses_mean(true_concat)\n",
    "    R2 =1-np.sum(sses)/np.sum(sses_mean)     \n",
    "    print('R2:',R2) \n",
    "    return R2, lr_all.best_estimator_.coef_, vel_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257bacfe",
   "metadata": {},
   "source": [
    "## with Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "106dc233",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_axis = np.arange(-300,300,20)\n",
    "x_field = 'spikes_smth_40'\n",
    "y_field ='hand_vel'\n",
    "trial_mask = passive_mask\n",
    "\n",
    "# Prepare for plotting\n",
    "plot_dir = [0.0, 90.0, 180.0, 270.0] # limit plot directions to reduce cluttering\n",
    "plot_dim = 'x' # plot x velocity\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "\n",
    "figDir = \"/Users/sherryan/area2_population_analysis/figures/neurons/pas/\"\n",
    "dim = n_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad8771f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: -0.2657866346771307\n",
      "R2: -0.2504498991873032\n",
      "R2: -0.19668325482165883\n",
      "R2: -0.14342303373978882\n",
      "R2: -0.13011723516685736\n",
      "R2: -0.1534979771533762\n",
      "R2: -0.18661063899047514\n",
      "R2: -0.21220335396382506\n",
      "R2: -0.21906986870085765\n",
      "R2: -0.20675364373055172\n",
      "R2: -0.13497398622505785\n",
      "R2: -0.0011737675187790675\n",
      "R2: 0.19575514051502385\n",
      "R2: 0.40274232489982753\n",
      "R2: 0.556532150457304\n",
      "R2: 0.6300162779799912\n",
      "R2: 0.6292173037205039\n",
      "R2: 0.5828241878190619\n",
      "R2: 0.5239542212678354\n",
      "R2: 0.46487203735658045\n",
      "R2: 0.4123407555664741\n",
      "R2: 0.354552143245753\n",
      "R2: 0.28349516948415576\n",
      "R2: 0.21970602247348248\n",
      "R2: 0.19299628281663816\n",
      "R2: 0.2036154549323408\n",
      "R2: 0.21597975151191529\n",
      "R2: 0.1973693596308519\n",
      "R2: 0.15905489437501552\n",
      "R2: 0.12189996810272452\n",
      "R2: 0.6300162779799912\n",
      "R2: -0.23732081017182893\n",
      "R2: -0.21950828218892582\n",
      "R2: -0.1726963718219574\n",
      "R2: -0.1316856091721308\n",
      "R2: -0.12856563775648988\n",
      "R2: -0.1498291645267602\n",
      "R2: -0.18226208457042103\n",
      "R2: -0.21109542882736587\n",
      "R2: -0.21815423178272964\n",
      "R2: -0.19823537644561928\n",
      "R2: -0.13779852173807416\n",
      "R2: -0.030495829291321996\n",
      "R2: 0.11515313045492748\n",
      "R2: 0.25421687032495544\n",
      "R2: 0.3537637270310663\n",
      "R2: 0.4148651122508904\n",
      "R2: 0.449425947186619\n",
      "R2: 0.46742102983762235\n",
      "R2: 0.46956939631445305\n",
      "R2: 0.44796102675281857\n",
      "R2: 0.4109717171279923\n",
      "R2: 0.3595121519268757\n",
      "R2: 0.2911114117401684\n",
      "R2: 0.22672177828506357\n",
      "R2: 0.1945912508810539\n",
      "R2: 0.1990034513189719\n",
      "R2: 0.2101769685759748\n",
      "R2: 0.1935989985883525\n",
      "R2: 0.15448846426681095\n",
      "R2: 0.11527895635862462\n",
      "R2: 0.46956939631445305\n",
      "R2: -0.2524283734255368\n",
      "R2: -0.2266966435061315\n",
      "R2: -0.17576751464985274\n",
      "R2: -0.13865906471907108\n",
      "R2: -0.13780473546126681\n",
      "R2: -0.1599323766992502\n",
      "R2: -0.18982919353237238\n",
      "R2: -0.21182517959853175\n",
      "R2: -0.21216588017810478\n",
      "R2: -0.1928961003377614\n",
      "R2: -0.13798636144729626\n",
      "R2: -0.034386506293158314\n",
      "R2: 0.09121234172535975\n",
      "R2: 0.20744529262847688\n",
      "R2: 0.29020320027037516\n",
      "R2: 0.3285373004277512\n",
      "R2: 0.3265837210546114\n",
      "R2: 0.2964024311411475\n",
      "R2: 0.2693762233686524\n",
      "R2: 0.2655598663241273\n",
      "R2: 0.27828188197670467\n",
      "R2: 0.278853364346201\n",
      "R2: 0.2485572062085808\n",
      "R2: 0.20493972279448314\n",
      "R2: 0.18607112841939621\n",
      "R2: 0.20105233805640366\n",
      "R2: 0.21609735079769576\n",
      "R2: 0.19919848425972708\n",
      "R2: 0.16215270193671116\n",
      "R2: 0.12649384852256595\n",
      "R2: 0.3285373004277512\n",
      "R2: -0.22087822550695058\n",
      "R2: -0.19920371761310052\n",
      "R2: -0.1539124217952259\n",
      "R2: -0.12182280842936666\n",
      "R2: -0.12351962686588691\n",
      "R2: -0.14700848185158644\n",
      "R2: -0.17794834249311808\n",
      "R2: -0.2002953777715606\n",
      "R2: -0.20854539433662866\n",
      "R2: -0.2044275520513461\n",
      "R2: -0.17959724328723725\n",
      "R2: -0.1352075248599045\n",
      "R2: -0.06966077013987326\n",
      "R2: 0.0080009268807274\n",
      "R2: 0.07953647225180338\n",
      "R2: 0.13317875432653026\n",
      "R2: 0.1644015090719937\n",
      "R2: 0.1848445636942294\n",
      "R2: 0.20873557822836275\n",
      "R2: 0.23923991205714468\n",
      "R2: 0.267534333634604\n",
      "R2: 0.2731468892228448\n",
      "R2: 0.24376981930094566\n",
      "R2: 0.198828710205186\n",
      "R2: 0.18177237813114377\n",
      "R2: 0.2008182194471041\n",
      "R2: 0.21637089519685027\n",
      "R2: 0.20145510525393007\n",
      "R2: 0.16820511602467914\n",
      "R2: 0.13114488430620697\n",
      "R2: 0.2731468892228448\n"
     ]
    }
   ],
   "source": [
    "pred_range = (0, 120)\n",
    "label = '_early_'\n",
    "x_axis = np.arange(pred_range[0], pred_range[1], dataset_5ms.bin_width)\n",
    "\n",
    "curr_r2_array = nans([len(lag_axis)])\n",
    "curr_coef_array = nans([len(lag_axis),2,dim])\n",
    "for i in range(len(lag_axis)):\n",
    "    lag = lag_axis[i]\n",
    "    r2, coef, _ = fit_and_predict(dataset_5ms, trial_mask, 'move_onset_time', pred_range, lag, x_field, y_field)\n",
    "    curr_r2_array[i] = r2\n",
    "    curr_coef_array[i,:,:] = coef\n",
    "\n",
    "idx_max = np.argmax(curr_r2_array)\n",
    "time_max = lag_axis[idx_max]\n",
    "_, _, vel_df = fit_and_predict(dataset_5ms, trial_mask, 'move_onset_time', pred_range, time_max, x_field, y_field)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset_5ms.trial_info[dataset_5ms.trial_info.cond_dir == trial_dir].trial_id\n",
    "    for _, trial in vel_df[np.isin(vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        plt.plot(x_axis, trial.hand_vel[plot_dim], color=color, linewidth=0.5)\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('x-vel')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + label + 'true.png', dpi = 'figure')\n",
    "plt.close()\n",
    "\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset_5ms.trial_info[dataset_5ms.trial_info.cond_dir == trial_dir].trial_id\n",
    "    for _, trial in vel_df[np.isin(vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        plt.plot(x_axis, trial.pred_vel[plot_dim], color=color, linewidth=0.5)\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('x-vel')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + label + str(0) +'_pred.png', dpi = 'figure')\n",
    "plt.close()\n",
    "\n",
    "plt.plot(lag_axis, curr_r2_array)\n",
    "plt.axvline(time_max, color = 'r', label='t_max = ' + str(time_max))\n",
    "plt.legend()\n",
    "plt.title('R2 score predicting hand velocity [0,120]')\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R2')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + label + str(0) +'.png', dpi = 'figure')\n",
    "plt.close()\n",
    "\n",
    "weights = curr_coef_array[idx_max,:,:]\n",
    "for iter in range(0,3):  \n",
    "    #subtract predictions with primary decoding dimensions (at time with max R2)\n",
    "    sub_coef_array = nans([len(lag_axis),2,dim])\n",
    "    sub_r2_array = nans([len(lag_axis)])\n",
    "\n",
    "    for i in range(len(lag_axis)):\n",
    "        lag = lag_axis[i]\n",
    "        r2, coef,_ = sub_and_predict(dataset_5ms, trial_mask, 'move_onset_time', pred_range, lag,x_field,y_field,weights)\n",
    "        sub_r2_array[i] = r2\n",
    "        sub_coef_array[i,:,:] = coef\n",
    "\n",
    "    plt.plot(lag_axis,sub_r2_array)\n",
    "    plt.title('R2 score projecting out #'+ str(iter+1) +' t_max dim')\n",
    "    idx_max = np.argmax(sub_r2_array)\n",
    "    time_max = lag_axis[idx_max]\n",
    "    plt.axvline(time_max, color = 'r', label='t_max = ' + str(time_max))\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time lag (ms)')\n",
    "    plt.ylabel('R2')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figDir + monkey + label + str(iter+1) +'.png', dpi = 'figure')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    _, _, vel_df = sub_and_predict(dataset_5ms, trial_mask, 'move_onset_time', pred_range, time_max, x_field, y_field,weights)\n",
    "    for trial_dir, color in zip(plot_dir, colors):\n",
    "        cond_ids = dataset_5ms.trial_info[dataset_5ms.trial_info.cond_dir == trial_dir].trial_id\n",
    "        for _, trial in vel_df[np.isin(vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "            plt.plot(x_axis, trial.pred_vel[plot_dim], color=color, linewidth=0.5)\n",
    "    plt.xlabel('Time (ms)')\n",
    "    plt.ylabel('x-vel')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figDir + monkey + label + str(iter+1) +'_pred.png', dpi = 'figure')\n",
    "    plt.close()\n",
    "\n",
    "    #stack the decoding dimensions to be projected out\n",
    "    weights = np.vstack((weights,sub_coef_array[idx_max,:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f030f4",
   "metadata": {},
   "source": [
    "## with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56d6d3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'PCA'\n",
    "y_field ='hand_vel'\n",
    "lag_axis = np.arange(-300,300,20)\n",
    "\n",
    "figDir = \"/Users/sherryan/area2_population_analysis/figures/PCA/pas/\"\n",
    "dim = n_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f37ed90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: -0.010162755265458623\n",
      "R2: -0.011983244366491563\n",
      "R2: -0.010333450353063878\n",
      "R2: -0.008085707320044655\n",
      "R2: -0.009413419913876009\n",
      "R2: -0.01402458132089568\n",
      "R2: -0.01723334280471911\n",
      "R2: -0.016869238452749125\n",
      "R2: -0.011975096211862324\n",
      "R2: 0.0035295720179276957\n",
      "R2: 0.04409373810272055\n",
      "R2: 0.12272372794156927\n",
      "R2: 0.2448176375929233\n",
      "R2: 0.3930682971825138\n",
      "R2: 0.5183715312268111\n",
      "R2: 0.5862197167068957\n",
      "R2: 0.5973270049786052\n",
      "R2: 0.5670727327300331\n",
      "R2: 0.514277292366893\n",
      "R2: 0.4537157361637886\n",
      "R2: 0.3888176468874692\n",
      "R2: 0.31847056759549097\n",
      "R2: 0.2514111254665671\n",
      "R2: 0.20580131379795552\n",
      "R2: 0.19261768243999433\n",
      "R2: 0.20034086713174215\n",
      "R2: 0.20205789969543142\n",
      "R2: 0.1876138079828884\n",
      "R2: 0.16462957908299802\n",
      "R2: 0.13978290427887696\n",
      "R2: 0.5973270049786052\n",
      "R2: -0.007659421876512029\n",
      "R2: -0.01021840739345925\n",
      "R2: -0.009151340905782401\n",
      "R2: -0.008525470739263241\n",
      "R2: -0.013498419942265416\n",
      "R2: -0.02282282834846283\n",
      "R2: -0.030552500975762786\n",
      "R2: -0.03478443406159726\n",
      "R2: -0.03642900452399345\n",
      "R2: -0.032257074373921446\n",
      "R2: -0.018046093501239424\n",
      "R2: 0.0030672261351821772\n",
      "R2: 0.02553203100745749\n",
      "R2: 0.04444621568391316\n",
      "R2: 0.04937630948299754\n",
      "R2: 0.03439178324072745\n",
      "R2: 0.021842481185143314\n",
      "R2: 0.03655048459354637\n",
      "R2: 0.07825647135438973\n",
      "R2: 0.1312511549226093\n",
      "R2: 0.17776280888138984\n",
      "R2: 0.19710923879673237\n",
      "R2: 0.18274508111939447\n",
      "R2: 0.15341321463979207\n",
      "R2: 0.13834038828887496\n",
      "R2: 0.14990569982570268\n",
      "R2: 0.16994853585616598\n",
      "R2: 0.1729500631919486\n",
      "R2: 0.153411480531485\n",
      "R2: 0.1225501672809538\n",
      "R2: 0.19710923879673237\n",
      "R2: -0.008788084103820815\n",
      "R2: -0.00886232104976803\n",
      "R2: -0.005255688460110042\n",
      "R2: -0.0023673321120323276\n",
      "R2: -0.0058777433387284805\n",
      "R2: -0.015036850186077144\n",
      "R2: -0.023036309667954447\n",
      "R2: -0.026207621058244923\n",
      "R2: -0.02594503431317685\n",
      "R2: -0.02235223449345214\n",
      "R2: -0.012772062200955503\n",
      "R2: 0.0019166127269818478\n",
      "R2: 0.017033096704846984\n",
      "R2: 0.030277321086153552\n",
      "R2: 0.03725348712382104\n",
      "R2: 0.030447964601329036\n",
      "R2: 0.020794879108261277\n",
      "R2: 0.01894983803722161\n",
      "R2: 0.019329126230242855\n",
      "R2: 0.012352772879144935\n",
      "R2: 0.00019665152103598427\n",
      "R2: -0.008283989512156031\n",
      "R2: -0.004993851164971197\n",
      "R2: 0.0167492167013662\n",
      "R2: 0.057535478576134946\n",
      "R2: 0.10329514256795913\n",
      "R2: 0.13140238854316255\n",
      "R2: 0.1306123101094101\n",
      "R2: 0.10672828913636512\n",
      "R2: 0.0745621209320656\n",
      "R2: 0.13140238854316255\n",
      "R2: -0.0023197484796237955\n",
      "R2: 0.0011948055102461996\n",
      "R2: 0.00579328328711326\n",
      "R2: 0.008391288036910804\n",
      "R2: 0.0051449478727266795\n",
      "R2: -0.002986549769649516\n",
      "R2: -0.010440786442142524\n",
      "R2: -0.01506995822838908\n",
      "R2: -0.018578867198516757\n",
      "R2: -0.02117084220407972\n",
      "R2: -0.02013500580553962\n",
      "R2: -0.011051293922471839\n",
      "R2: 0.0037842670446374127\n",
      "R2: 0.017987109085532027\n",
      "R2: 0.026766680776944773\n",
      "R2: 0.026064607757021463\n",
      "R2: 0.021168372685703374\n",
      "R2: 0.016130872387958628\n",
      "R2: 0.010092628978771612\n",
      "R2: 0.003204375830413664\n",
      "R2: -0.002886745360786813\n",
      "R2: -0.007591735829162172\n",
      "R2: -0.012017400229190622\n",
      "R2: -0.017611396447468675\n",
      "R2: -0.024768248866537945\n",
      "R2: -0.03126073430818477\n",
      "R2: -0.03126556785532464\n",
      "R2: -0.024183399584177145\n",
      "R2: -0.013942617300644278\n",
      "R2: -0.003592884566109733\n",
      "R2: 0.026766680776944773\n"
     ]
    }
   ],
   "source": [
    "pred_range = (0, 120)\n",
    "label = '_early_'\n",
    "x_axis = np.arange(pred_range[0], pred_range[1], dataset_5ms.bin_width)\n",
    "\n",
    "curr_r2_array = nans([len(lag_axis)])\n",
    "curr_coef_array = nans([len(lag_axis),2,dim])\n",
    "for i in range(len(lag_axis)):\n",
    "    lag = lag_axis[i]\n",
    "    r2, coef, _ = fit_and_predict(dataset_5ms, trial_mask, 'move_onset_time', pred_range, lag, x_field, y_field)\n",
    "    curr_r2_array[i] = r2\n",
    "    curr_coef_array[i,:,:] = coef\n",
    "\n",
    "idx_max = np.argmax(curr_r2_array)\n",
    "time_max = lag_axis[idx_max]\n",
    "_, _, vel_df = fit_and_predict(dataset_5ms, trial_mask, 'move_onset_time', pred_range, time_max, x_field, y_field)\n",
    "\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset_5ms.trial_info[dataset_5ms.trial_info.cond_dir == trial_dir].trial_id\n",
    "    for _, trial in vel_df[np.isin(vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        plt.plot(x_axis, trial.pred_vel[plot_dim], color=color, linewidth=0.5)\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('x-vel')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + label + str(0) +'_pred.png', dpi = 'figure')\n",
    "plt.close()\n",
    "\n",
    "plt.plot(lag_axis, curr_r2_array)\n",
    "plt.axvline(time_max, color = 'r', label='t_max = ' + str(time_max))\n",
    "plt.legend()\n",
    "plt.title('R2 score predicting hand velocity [0,120]')\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R2')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figDir + monkey + label + str(0) +'.png', dpi = 'figure')\n",
    "plt.close()\n",
    "\n",
    "weights = curr_coef_array[idx_max,:,:]\n",
    "for iter in range(0,3):  \n",
    "    #subtract predictions with primary decoding dimensions (at time with max R2)\n",
    "    sub_coef_array = nans([len(lag_axis),2,dim])\n",
    "    sub_r2_array = nans([len(lag_axis)])\n",
    "\n",
    "    for i in range(len(lag_axis)):\n",
    "        lag = lag_axis[i]\n",
    "        r2, coef,_ = sub_and_predict(dataset_5ms, trial_mask, 'move_onset_time', pred_range, lag,x_field,y_field,weights)\n",
    "        sub_r2_array[i] = r2\n",
    "        sub_coef_array[i,:,:] = coef\n",
    "\n",
    "    plt.plot(lag_axis,sub_r2_array)\n",
    "    plt.title('R2 score projecting out #'+ str(iter+1) +' t_max dim')\n",
    "    idx_max = np.argmax(sub_r2_array)\n",
    "    time_max = lag_axis[idx_max]\n",
    "    plt.axvline(time_max, color = 'r', label='t_max = ' + str(time_max))\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time lag (ms)')\n",
    "    plt.ylabel('R2')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figDir + monkey + label + str(iter+1) +'.png', dpi = 'figure')\n",
    "    plt.close()\n",
    "\n",
    "    _, _, vel_df = sub_and_predict(dataset_5ms, trial_mask, 'move_onset_time', pred_range, time_max, x_field, y_field,weights)\n",
    "    for trial_dir, color in zip(plot_dir, colors):\n",
    "        cond_ids = dataset_5ms.trial_info[dataset_5ms.trial_info.cond_dir == trial_dir].trial_id\n",
    "        for _, trial in vel_df[np.isin(vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "            plt.plot(x_axis, trial.pred_vel[plot_dim], color=color, linewidth=0.5)\n",
    "    plt.xlabel('Time (ms)')\n",
    "    plt.ylabel('x-vel')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figDir + monkey + label + str(iter+1) +'_pred.png', dpi = 'figure')\n",
    "    plt.close()\n",
    "\n",
    "    #stack the decoding dimensions to be projected out\n",
    "    weights = np.vstack((weights,sub_coef_array[idx_max,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fab41e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bf152a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e99868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80e66b89",
   "metadata": {},
   "source": [
    "# Multi Lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8649848a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "dataset_50ms = NWBDataset(filename, split_heldout=False)\n",
    "dataset_50ms.resample(50)\n",
    "print(dataset_50ms.bin_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fda3bfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346 trials\n",
      "87 neurons\n",
      "(53899, 87)\n",
      "(53899, 87)\n",
      "(53899, 20)\n",
      "PCA total var explained: 0.43309952685401104\n"
     ]
    }
   ],
   "source": [
    "n_dims = 20 # for PCA\n",
    "\n",
    "active_mask = (~dataset_50ms.trial_info.ctr_hold_bump) & (dataset_50ms.trial_info.split != 'none')\n",
    "passive_mask = (dataset_50ms.trial_info.ctr_hold_bump) & (dataset_50ms.trial_info.split != 'none')\n",
    "\n",
    "\n",
    "trial_mask = passive_mask\n",
    "n_trials = dataset_50ms.trial_info.loc[trial_mask].shape[0]\n",
    "print(n_trials,'trials')\n",
    "n_neurons = dataset_50ms.data.spikes.shape[1]\n",
    "print(n_neurons,'neurons')\n",
    "\n",
    "all_data = np.array(dataset_50ms.data.spikes)\n",
    "print(all_data.shape)\n",
    "data_for_pca = all_data[~np.isnan(all_data).any(axis=1)]\n",
    "print(data_for_pca.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(data_for_pca)\n",
    "pca = PCA(n_components=n_dims)\n",
    "X = pca.fit(X)\n",
    "\n",
    "PCA_data = nans([all_data.shape[0],n_dims])\n",
    "idx = 0\n",
    "for dp in all_data:\n",
    "    dp = dp.reshape((1, -1))\n",
    "    if np.isnan(dp).any():\n",
    "        dp_pca = nans([1,n_dims])\n",
    "    else:\n",
    "        dp_pca = pca.transform(scaler.transform(dp))\n",
    "    PCA_data[idx,:] = dp_pca\n",
    "    idx+=1\n",
    "print(PCA_data.shape)\n",
    "dataset_50ms.add_continuous_data(PCA_data,'PCA')\n",
    "print('PCA total var explained:',sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a348315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 time bins\n",
      "(346, 20, 87)\n",
      "(346, 20, 2)\n",
      "(346, 20, 20)\n"
     ]
    }
   ],
   "source": [
    "passive_data = dataset_50ms.make_trial_data(align_field='move_onset_time', align_range=(-300, 700), ignored_trials=~trial_mask)\n",
    "for idx, trial in passive_data.groupby('trial_id'):\n",
    "    n_timepoints = trial.shape[0]\n",
    "    break\n",
    "print(n_timepoints,'time bins')\n",
    "\n",
    "passive_trials_neuron = nans([n_trials,n_timepoints,n_neurons])\n",
    "passive_trials_vel = nans([n_trials,n_timepoints,2])\n",
    "passive_trials_pca = nans([n_trials,n_timepoints,n_dims])\n",
    "i = 0\n",
    "for idx, trial in passive_data.groupby('trial_id'):\n",
    "    passive_trials_neuron[i,:,:]=trial.spikes.to_numpy()\n",
    "    passive_trials_vel[i,:,:]=trial.hand_vel.to_numpy()\n",
    "    passive_trials_pca[i,:,:]=trial.PCA.to_numpy()\n",
    "    i+=1\n",
    "print(passive_trials_neuron.shape)\n",
    "print(passive_trials_vel.shape)\n",
    "print(passive_trials_pca.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef332cd6",
   "metadata": {},
   "source": [
    "## with Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c8378b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with 0 to 0 ms neural data\n",
      "R2: 0.442395227873264\n",
      "Predicting with 0 to 50 ms neural data\n",
      "R2: 0.4967299915049057\n",
      "Predicting with 0 to 100 ms neural data\n",
      "R2: 0.5122219704410045\n",
      "Predicting with 0 to 150 ms neural data\n",
      "R2: 0.5251433345544985\n",
      "Predicting with 0 to 200 ms neural data\n",
      "R2: 0.5379994914089021\n",
      "Predicting with 0 to 250 ms neural data\n",
      "R2: 0.5381783555793649\n",
      "Predicting with 0 to 300 ms neural data\n",
      "R2: 0.5339449217789096\n",
      "Predicting with 0 to 350 ms neural data\n",
      "R2: 0.5304032911823597\n",
      "Predicting with 0 to 400 ms neural data\n",
      "R2: 0.5217587285650269\n",
      "Predicting with 0 to 450 ms neural data\n",
      "R2: 0.5159527211334158\n",
      "Predicting with 0 to 500 ms neural data\n",
      "R2: 0.5129695728230019\n",
      "Predicting with -50 to 0 ms neural data\n",
      "R2: 0.5414783178218514\n",
      "Predicting with -50 to 50 ms neural data\n",
      "R2: 0.5704712407363908\n",
      "Predicting with -50 to 100 ms neural data\n",
      "R2: 0.5824215745036465\n",
      "Predicting with -50 to 150 ms neural data\n",
      "R2: 0.6001870895632291\n",
      "Predicting with -50 to 200 ms neural data\n",
      "R2: 0.6107475027469843\n",
      "Predicting with -50 to 250 ms neural data\n",
      "R2: 0.6102698950074561\n",
      "Predicting with -50 to 300 ms neural data\n",
      "R2: 0.6048753644971747\n",
      "Predicting with -50 to 350 ms neural data\n",
      "R2: 0.6020277467014832\n",
      "Predicting with -50 to 400 ms neural data\n",
      "R2: 0.5945804897462852\n",
      "Predicting with -50 to 450 ms neural data\n",
      "R2: 0.5722087235607374\n",
      "Predicting with -50 to 500 ms neural data\n",
      "R2: 0.5711504815075048\n",
      "Predicting with -100 to 0 ms neural data\n",
      "R2: 0.48691236689071926\n",
      "Predicting with -100 to 50 ms neural data\n",
      "R2: 0.5374658725806594\n",
      "Predicting with -100 to 100 ms neural data\n",
      "R2: 0.5824697837712195\n",
      "Predicting with -100 to 150 ms neural data\n",
      "R2: 0.605892266681824\n",
      "Predicting with -100 to 200 ms neural data\n",
      "R2: 0.6177998861057221\n",
      "Predicting with -100 to 250 ms neural data\n",
      "R2: 0.6189081096909829\n",
      "Predicting with -100 to 300 ms neural data\n",
      "R2: 0.613574019080213\n",
      "Predicting with -100 to 350 ms neural data\n",
      "R2: 0.6100549421268548\n",
      "Predicting with -100 to 400 ms neural data\n",
      "R2: 0.5835898831648141\n",
      "Predicting with -100 to 450 ms neural data\n",
      "R2: 0.5809693164414057\n",
      "Predicting with -100 to 500 ms neural data\n",
      "R2: 0.5795170062641103\n",
      "Predicting with -150 to 0 ms neural data\n",
      "R2: 0.44938966090409416\n",
      "Predicting with -150 to 50 ms neural data\n",
      "R2: 0.5691012212265569\n",
      "Predicting with -150 to 100 ms neural data\n",
      "R2: 0.5940627833565012\n",
      "Predicting with -150 to 150 ms neural data\n",
      "R2: 0.601876433045972\n",
      "Predicting with -150 to 200 ms neural data\n",
      "R2: 0.595609296219872\n",
      "Predicting with -150 to 250 ms neural data\n",
      "R2: 0.6173407427759874\n",
      "Predicting with -150 to 300 ms neural data\n",
      "R2: 0.6128511550139375\n",
      "Predicting with -150 to 350 ms neural data\n",
      "R2: 0.6009399127336275\n",
      "Predicting with -150 to 400 ms neural data\n",
      "R2: 0.5853257238793164\n",
      "Predicting with -150 to 450 ms neural data\n",
      "R2: 0.5738212925342618\n",
      "Predicting with -150 to 500 ms neural data\n",
      "R2: 0.5692938517473181\n",
      "Predicting with -200 to 0 ms neural data\n",
      "R2: 0.4674296556291049\n",
      "Predicting with -200 to 50 ms neural data\n",
      "R2: 0.565798511521783\n",
      "Predicting with -200 to 100 ms neural data\n",
      "R2: 0.5926005653390638\n",
      "Predicting with -200 to 150 ms neural data\n",
      "R2: 0.6005577801446847\n",
      "Predicting with -200 to 200 ms neural data\n",
      "R2: 0.597960253401717\n",
      "Predicting with -200 to 250 ms neural data\n",
      "R2: 0.6000982261154584\n",
      "Predicting with -200 to 300 ms neural data\n",
      "R2: 0.6022107073764392\n",
      "Predicting with -200 to 350 ms neural data\n",
      "R2: 0.6000530746071001\n",
      "Predicting with -200 to 400 ms neural data\n",
      "R2: 0.600018519533489\n",
      "Predicting with -200 to 450 ms neural data\n",
      "R2: 0.5834717958250223\n",
      "Predicting with -200 to 500 ms neural data\n",
      "R2: 0.5827427850517342\n",
      "Predicting with -250 to 0 ms neural data\n",
      "R2: 0.4597135685039586\n",
      "Predicting with -250 to 50 ms neural data\n",
      "R2: 0.5634352411119189\n",
      "Predicting with -250 to 100 ms neural data\n",
      "R2: 0.5901154437955054\n",
      "Predicting with -250 to 150 ms neural data\n",
      "R2: 0.5986095640202351\n",
      "Predicting with -250 to 200 ms neural data\n",
      "R2: 0.5937309876374506\n",
      "Predicting with -250 to 250 ms neural data\n",
      "R2: 0.6031299507930517\n",
      "Predicting with -250 to 300 ms neural data\n",
      "R2: 0.6045020065028633\n",
      "Predicting with -250 to 350 ms neural data\n",
      "R2: 0.5985366554968723\n",
      "Predicting with -250 to 400 ms neural data\n",
      "R2: 0.5906055042531809\n",
      "Predicting with -250 to 450 ms neural data\n",
      "R2: 0.5848273663774688\n",
      "Predicting with -250 to 500 ms neural data\n",
      "R2: 0.5856869172300369\n",
      "Predicting with -300 to 0 ms neural data\n",
      "R2: 0.4467952355552297\n",
      "Predicting with -300 to 50 ms neural data\n",
      "R2: 0.5553517435681454\n",
      "Predicting with -300 to 100 ms neural data\n",
      "R2: 0.5836882671913378\n",
      "Predicting with -300 to 150 ms neural data\n",
      "R2: 0.5910743957238292\n",
      "Predicting with -300 to 200 ms neural data\n",
      "R2: 0.5865704382834862\n",
      "Predicting with -300 to 250 ms neural data\n",
      "R2: 0.5838371362938775\n",
      "Predicting with -300 to 300 ms neural data\n",
      "R2: 0.5993933018532694\n",
      "Predicting with -300 to 350 ms neural data\n",
      "R2: 0.5917371756699094\n",
      "Predicting with -300 to 400 ms neural data\n",
      "R2: 0.5889683179141965\n",
      "Predicting with -300 to 450 ms neural data\n",
      "R2: 0.5857868112345048\n",
      "Predicting with -300 to 500 ms neural data\n",
      "R2: 0.5853191215187654\n"
     ]
    }
   ],
   "source": [
    "data_range = [-300,700]\n",
    "pred_start = 0\n",
    "pred_end = 120\n",
    "\n",
    "idx1 = int((pred_start - data_range[0])/dataset_50ms.bin_width)\n",
    "idx2 = int(n_timepoints - (data_range[1]-pred_end)/dataset_50ms.bin_width)\n",
    "\n",
    "t_before_range = range(0,301,50);\n",
    "t_after_range = range(0,501,50);\n",
    "\n",
    "whole_multi_R2s = nans([len(t_before_range),len(t_after_range)])\n",
    "whole_multi_coefs = []\n",
    "j,k=0,0\n",
    "for time_before in t_before_range:\n",
    "    coef_arr = []\n",
    "    for time_after in t_after_range:\n",
    "        print('Predicting with',-time_before, 'to', time_after,'ms neural data')\n",
    "        \n",
    "        bins_before= int(time_before/dataset_50ms.bin_width) #How many bins of neural data prior to the output are used for decoding\n",
    "        bins_current= 1 #Whether to use concurrent time bin of neural data\n",
    "        bins_after= int(time_after/dataset_50ms.bin_width) #How many bins of neural data after the output are used for decoding\n",
    "\n",
    "        n_total_bins = bins_before + bins_current + bins_after\n",
    "\n",
    "        X =  nans([n_trials,idx2-idx1,n_total_bins*n_neurons])\n",
    "        i = 0\n",
    "        for trial_data in passive_trials_neuron:\n",
    "            trial_hist=get_spikes_with_history(trial_data,bins_before,bins_after,bins_current)\n",
    "            trial_hist = trial_hist[idx1:idx2,:,:]\n",
    "            trial_hist_flat=trial_hist.reshape(trial_hist.shape[0],(trial_hist.shape[1]*trial_hist.shape[2]))\n",
    "            X[i,:,:] = trial_hist_flat\n",
    "            i+=1\n",
    "        \n",
    "        y = passive_trials_vel[:,idx1:idx2,:]\n",
    "    \n",
    "        lr_all = GridSearchCV(Ridge(), {'alpha': np.logspace(-4, 4, 9)})\n",
    "        X_reshaped = X.reshape((X.shape[0]*X.shape[1]),X.shape[2])\n",
    "        y_reshaped = y.reshape((y.shape[0]*y.shape[1]),y.shape[2])\n",
    "        lr_all.fit(X_reshaped, y_reshaped)\n",
    "\n",
    "        \n",
    "        kf = KFold(n_splits=5,shuffle=True,random_state = 42)   \n",
    "        true_concat = nans([(n_trials*(idx2-idx1)),2])\n",
    "        pred_concat = nans([(n_trials*(idx2-idx1)),2])\n",
    "        trial_save_idx = 0\n",
    "        for training_set, test_set in kf.split(range(0,n_trials)):\n",
    "            #split training and testing by trials\n",
    "            X_train, X_test, y_train, y_test = process_train_test(X,y,training_set,test_set)\n",
    "            lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-4, 4, 9)}) \n",
    "            lr.fit(X_train, y_train)\n",
    "            y_test_predicted = lr.predict(X_test)\n",
    "            n = y_test_predicted.shape[0]\n",
    "            true_concat[trial_save_idx:trial_save_idx+n,:] = y_test\n",
    "            pred_concat[trial_save_idx:trial_save_idx+n,:] = y_test_predicted\n",
    "            trial_save_idx += n\n",
    "\n",
    "        sses =get_sses_pred(true_concat,pred_concat)\n",
    "        sses_mean=get_sses_mean(true_concat)\n",
    "        whole_multi_R2s[j,k] =1-np.sum(sses)/np.sum(sses_mean)     \n",
    "        print('R2:',whole_multi_R2s[j,k])\n",
    "        coef_arr.append(lr_all.best_estimator_.coef_)\n",
    "        k += 1\n",
    "    j += 1\n",
    "    k = 0\n",
    "    whole_multi_coefs.append(coef_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf34e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(whole_multi_R2s)\n",
    "ax.set_xlabel('Length of lagging info')\n",
    "ax.set_ylabel('Length of leading info')\n",
    "\n",
    "ax.set_xticks(np.arange(len(t_after_range)))\n",
    "ax.set_yticks(np.arange(len(t_before_range)))\n",
    "ax.set_xticklabels(labels=t_after_range)\n",
    "ax.set_yticklabels(labels=t_before_range)\n",
    "\n",
    "ax.set_title(\"R2 predicting [0, 120] velocity \\nwith different lagging/leading info\")\n",
    "fig.tight_layout()\n",
    " \n",
    "for i in range(len(t_before_range)):\n",
    "    for j in range(len(t_after_range)):\n",
    "        text = ax.text(j, i, str(int(whole_multi_R2s[i, j]*1000)/1000),\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "plt.tight_layout()\n",
    "figDir = \"/Users/sherryan/area2_population_analysis/figures/neurons/pas/\"\n",
    "plt.savefig(figDir + monkey + '_multi_early.png', dpi = 'figure')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8958e911",
   "metadata": {},
   "source": [
    "## with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6de7686",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with 0 to 0 ms neural data\n",
      "R2: 0.39462536575634644\n",
      "Predicting with 0 to 50 ms neural data\n",
      "R2: 0.47612661225217945\n",
      "Predicting with 0 to 100 ms neural data\n",
      "R2: 0.48745527707450165\n",
      "Predicting with 0 to 150 ms neural data\n",
      "R2: 0.4903891719966681\n",
      "Predicting with 0 to 200 ms neural data\n",
      "R2: 0.4937458144187501\n",
      "Predicting with 0 to 250 ms neural data\n",
      "R2: 0.4806902450146404\n",
      "Predicting with 0 to 300 ms neural data\n",
      "R2: 0.465668562087044\n",
      "Predicting with 0 to 350 ms neural data\n",
      "R2: 0.4561260847055133\n",
      "Predicting with 0 to 400 ms neural data\n",
      "R2: 0.4438537467575615\n",
      "Predicting with 0 to 450 ms neural data\n",
      "R2: 0.438791624340525\n",
      "Predicting with 0 to 500 ms neural data\n",
      "R2: 0.43666298011361415\n",
      "Predicting with -50 to 0 ms neural data\n",
      "R2: 0.4795354844120735\n",
      "Predicting with -50 to 50 ms neural data\n",
      "R2: 0.5515727712122829\n",
      "Predicting with -50 to 100 ms neural data\n",
      "R2: 0.5564804695690322\n",
      "Predicting with -50 to 150 ms neural data\n",
      "R2: 0.557052648547448\n",
      "Predicting with -50 to 200 ms neural data\n",
      "R2: 0.5596717425796913\n",
      "Predicting with -50 to 250 ms neural data\n",
      "R2: 0.5572782137372473\n",
      "Predicting with -50 to 300 ms neural data\n",
      "R2: 0.5460759139862522\n",
      "Predicting with -50 to 350 ms neural data\n",
      "R2: 0.5414104495617463\n",
      "Predicting with -50 to 400 ms neural data\n",
      "R2: 0.5295263322583879\n",
      "Predicting with -50 to 450 ms neural data\n",
      "R2: 0.530381979470375\n",
      "Predicting with -50 to 500 ms neural data\n",
      "R2: 0.5321748978646934\n",
      "Predicting with -100 to 0 ms neural data\n",
      "R2: 0.48102103587787537\n",
      "Predicting with -100 to 50 ms neural data\n",
      "R2: 0.5548312429557323\n",
      "Predicting with -100 to 100 ms neural data\n",
      "R2: 0.5608758354784305\n",
      "Predicting with -100 to 150 ms neural data\n",
      "R2: 0.5638349166825923\n",
      "Predicting with -100 to 200 ms neural data\n",
      "R2: 0.5721978363811338\n",
      "Predicting with -100 to 250 ms neural data\n",
      "R2: 0.572883537826218\n",
      "Predicting with -100 to 300 ms neural data\n",
      "R2: 0.5643286788734339\n",
      "Predicting with -100 to 350 ms neural data\n",
      "R2: 0.5581699369569885\n",
      "Predicting with -100 to 400 ms neural data\n",
      "R2: 0.542653344394604\n",
      "Predicting with -100 to 450 ms neural data\n",
      "R2: 0.5426732380195296\n",
      "Predicting with -100 to 500 ms neural data\n",
      "R2: 0.5476120566037944\n",
      "Predicting with -150 to 0 ms neural data\n",
      "R2: 0.4647054436577157\n",
      "Predicting with -150 to 50 ms neural data\n",
      "R2: 0.5460271210163028\n",
      "Predicting with -150 to 100 ms neural data\n",
      "R2: 0.5529846359922225\n",
      "Predicting with -150 to 150 ms neural data\n",
      "R2: 0.5549153319436213\n",
      "Predicting with -150 to 200 ms neural data\n",
      "R2: 0.5637973240605384\n",
      "Predicting with -150 to 250 ms neural data\n",
      "R2: 0.5661386165710961\n",
      "Predicting with -150 to 300 ms neural data\n",
      "R2: 0.5563281255408914\n",
      "Predicting with -150 to 350 ms neural data\n",
      "R2: 0.5498086996804452\n",
      "Predicting with -150 to 400 ms neural data\n",
      "R2: 0.5359843749019009\n",
      "Predicting with -150 to 450 ms neural data\n",
      "R2: 0.5342612496337231\n",
      "Predicting with -150 to 500 ms neural data\n",
      "R2: 0.5398137680496877\n",
      "Predicting with -200 to 0 ms neural data\n",
      "R2: 0.4571023301221364\n",
      "Predicting with -200 to 50 ms neural data\n",
      "R2: 0.5351824845177868\n",
      "Predicting with -200 to 100 ms neural data\n",
      "R2: 0.5432756145901154\n",
      "Predicting with -200 to 150 ms neural data\n",
      "R2: 0.5470892694399496\n",
      "Predicting with -200 to 200 ms neural data\n",
      "R2: 0.556301880026483\n",
      "Predicting with -200 to 250 ms neural data\n",
      "R2: 0.5597415900497255\n",
      "Predicting with -200 to 300 ms neural data\n",
      "R2: 0.5486795323875182\n",
      "Predicting with -200 to 350 ms neural data\n",
      "R2: 0.5450463784615075\n",
      "Predicting with -200 to 400 ms neural data\n",
      "R2: 0.5310971714727837\n",
      "Predicting with -200 to 450 ms neural data\n",
      "R2: 0.5224049280744383\n",
      "Predicting with -200 to 500 ms neural data\n",
      "R2: 0.5341222170470097\n",
      "Predicting with -250 to 0 ms neural data\n",
      "R2: 0.4332669759306542\n",
      "Predicting with -250 to 50 ms neural data\n",
      "R2: 0.5206201288320376\n",
      "Predicting with -250 to 100 ms neural data\n",
      "R2: 0.5270475612214724\n",
      "Predicting with -250 to 150 ms neural data\n",
      "R2: 0.5254100936126502\n",
      "Predicting with -250 to 200 ms neural data\n",
      "R2: 0.5225867413584908\n",
      "Predicting with -250 to 250 ms neural data\n",
      "R2: 0.5277778869344467\n",
      "Predicting with -250 to 300 ms neural data\n",
      "R2: 0.5218234084728857\n",
      "Predicting with -250 to 350 ms neural data\n",
      "R2: 0.5226491282659381\n",
      "Predicting with -250 to 400 ms neural data\n",
      "R2: 0.5138695768668373\n",
      "Predicting with -250 to 450 ms neural data\n",
      "R2: 0.5167547139660751\n",
      "Predicting with -250 to 500 ms neural data\n",
      "R2: 0.5151662911579127\n",
      "Predicting with -300 to 0 ms neural data\n",
      "R2: 0.42219025246449293\n",
      "Predicting with -300 to 50 ms neural data\n",
      "R2: 0.5073643566309004\n",
      "Predicting with -300 to 100 ms neural data\n",
      "R2: 0.5150991190553643\n",
      "Predicting with -300 to 150 ms neural data\n",
      "R2: 0.5162492222513888\n",
      "Predicting with -300 to 200 ms neural data\n",
      "R2: 0.5136002127792686\n",
      "Predicting with -300 to 250 ms neural data\n",
      "R2: 0.5218204279434266\n",
      "Predicting with -300 to 300 ms neural data\n",
      "R2: 0.515658148703938\n",
      "Predicting with -300 to 350 ms neural data\n",
      "R2: 0.5126030027855308\n",
      "Predicting with -300 to 400 ms neural data\n",
      "R2: 0.4970873387630236\n",
      "Predicting with -300 to 450 ms neural data\n",
      "R2: 0.519900706643992\n",
      "Predicting with -300 to 500 ms neural data\n",
      "R2: 0.5235563235684277\n"
     ]
    }
   ],
   "source": [
    "PCA_whole_multi_R2s = nans([len(t_before_range),len(t_after_range)])\n",
    "PCA_whole_multi_coefs = []\n",
    "j,k=0,0\n",
    "for time_before in t_before_range:\n",
    "    coef_arr = []\n",
    "    for time_after in t_after_range:\n",
    "        print('Predicting with',-time_before, 'to', time_after,'ms neural data')\n",
    "        \n",
    "        bins_before= int(time_before/dataset_50ms.bin_width) #How many bins of neural data prior to the output are used for decoding\n",
    "        bins_current= 1 #Whether to use concurrent time bin of neural data\n",
    "        bins_after= int(time_after/dataset_50ms.bin_width) #How many bins of neural data after the output are used for decoding\n",
    "\n",
    "        n_total_bins = bins_before + bins_current + bins_after\n",
    "\n",
    "        X =  nans([n_trials,idx2-idx1,n_total_bins*n_dims])\n",
    "        i = 0\n",
    "        for trial_data in passive_trials_pca:\n",
    "            trial_hist=get_spikes_with_history(trial_data,bins_before,bins_after,bins_current)\n",
    "            trial_hist = trial_hist[idx1:idx2,:,:]\n",
    "            trial_hist_flat=trial_hist.reshape(trial_hist.shape[0],(trial_hist.shape[1]*trial_hist.shape[2]))\n",
    "            X[i,:,:] = trial_hist_flat\n",
    "            i+=1\n",
    "        \n",
    "        y = passive_trials_vel[:,idx1:idx2,:]\n",
    "    \n",
    "        lr_all = GridSearchCV(Ridge(), {'alpha': np.logspace(-4, 4, 9)})\n",
    "        X_reshaped = X.reshape((X.shape[0]*X.shape[1]),X.shape[2])\n",
    "        y_reshaped = y.reshape((y.shape[0]*y.shape[1]),y.shape[2])\n",
    "        lr_all.fit(X_reshaped, y_reshaped)\n",
    "\n",
    "        \n",
    "        kf =KFold(n_splits=5,shuffle=True,random_state = 42)   \n",
    "        true_concat = nans([(n_trials*(idx2-idx1)),2])\n",
    "        pred_concat = nans([(n_trials*(idx2-idx1)),2])\n",
    "        trial_save_idx = 0\n",
    "        for training_set, test_set in kf.split(range(0,n_trials)):\n",
    "            #split training and testing by trials\n",
    "            X_train, X_test, y_train, y_test = process_train_test(X,y,training_set,test_set)\n",
    "            lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-4, 4, 9)}) \n",
    "            lr.fit(X_train, y_train)\n",
    "            y_test_predicted = lr.predict(X_test)\n",
    "            n = y_test_predicted.shape[0]\n",
    "            true_concat[trial_save_idx:trial_save_idx+n,:] = y_test\n",
    "            pred_concat[trial_save_idx:trial_save_idx+n,:] = y_test_predicted\n",
    "            trial_save_idx += n\n",
    "\n",
    "        sses =get_sses_pred(true_concat,pred_concat)\n",
    "        sses_mean=get_sses_mean(true_concat)\n",
    "        PCA_whole_multi_R2s[j,k] =1-np.sum(sses)/np.sum(sses_mean)     \n",
    "        print('R2:',PCA_whole_multi_R2s[j,k])\n",
    "        coef_arr.append(lr_all.best_estimator_.coef_)\n",
    "        k += 1\n",
    "    j += 1\n",
    "    k = 0\n",
    "    PCA_whole_multi_coefs.append(coef_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8f8ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(PCA_whole_multi_R2s)\n",
    "ax.set_xlabel('Length of lagging info')\n",
    "ax.set_ylabel('Length of leading info')\n",
    "\n",
    "ax.set_xticks(np.arange(len(t_after_range)))\n",
    "ax.set_yticks(np.arange(len(t_before_range)))\n",
    "ax.set_xticklabels(labels=t_after_range)\n",
    "ax.set_yticklabels(labels=t_before_range)\n",
    "\n",
    "ax.set_title(\"R2 predicting [0, 120] velocity \\nwith different lagging/leading info\")\n",
    "fig.tight_layout()\n",
    "\n",
    "for i in range(len(t_before_range)):\n",
    "    for j in range(len(t_after_range)):\n",
    "        text = ax.text(j, i, str(int(PCA_whole_multi_R2s[i, j]*1000)/1000),\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "plt.tight_layout()\n",
    "figDir = \"/Users/sherryan/area2_population_analysis/figures/PCA/pas/\"\n",
    "plt.savefig(figDir + monkey + '_multi_early.png', dpi = 'figure')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd444f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318916ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7170769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
