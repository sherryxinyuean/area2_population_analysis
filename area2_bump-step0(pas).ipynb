{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8d2edb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: Xgboost package is not installed. You will be unable to use the xgboost decoder\n",
      "\n",
      "WARNING: Keras package is not installed. You will be unable to use all neural net decoders\n"
     ]
    }
   ],
   "source": [
    "from nlb_tools.nwb_interface import NWBDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "\n",
    "            >>> angle_between((1, 0, 0), (0, 1, 0))\n",
    "            1.5707963267948966 (90 deg)\n",
    "            >>> angle_between((1, 0, 0), (1, 0, 0))\n",
    "            0.0 (0 deg)\n",
    "            >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
    "            3.141592653589793 (180 deg)\n",
    "    \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
    "\n",
    "#Import standard packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import io\n",
    "from scipy import stats\n",
    "import pickle\n",
    "\n",
    "# If you would prefer to load the '.h5' example file rather than the '.pickle' example file. You need the deepdish package\n",
    "# import deepdish as dd \n",
    "\n",
    "#Import function to get the covariate matrix that includes spike history from previous bins\n",
    "from Neural_Decoding.preprocessing_funcs import get_spikes_with_history\n",
    "\n",
    "#Import metrics\n",
    "from Neural_Decoding.metrics import get_R2\n",
    "from Neural_Decoding.metrics import get_rho\n",
    "\n",
    "#Import decoder functions\n",
    "from Neural_Decoding.decoders import WienerCascadeDecoder\n",
    "from Neural_Decoding.decoders import WienerFilterDecoder\n",
    "from Neural_Decoding.decoders import DenseNNDecoder\n",
    "from Neural_Decoding.decoders import SimpleRNNDecoder\n",
    "from Neural_Decoding.decoders import GRUDecoder\n",
    "from Neural_Decoding.decoders import LSTMDecoder\n",
    "from Neural_Decoding.decoders import XGBoostDecoder\n",
    "from Neural_Decoding.decoders import SVRDecoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def get_sses_pred(y_test,y_test_pred):\n",
    "    sse=np.sum((y_test_pred-y_test)**2,axis=0)\n",
    "    return sse\n",
    "\n",
    "def get_sses_mean(y_test):\n",
    "    y_mean=np.mean(y_test,axis=0)\n",
    "    sse_mean=np.sum((y_test-y_mean)**2,axis=0)\n",
    "    return sse_mean\n",
    "\n",
    "def nans(shape, dtype=float):\n",
    "    a = np.empty(shape, dtype)\n",
    "    a.fill(np.nan)\n",
    "    return a\n",
    "\n",
    "def vector_reject(u,v):\n",
    "    #project u on v, subtract u1 from u\n",
    "    P = np.outer(v,(v.T))/(v@(v.T))\n",
    "    u_sub = u - P@u\n",
    "#     another calculation, to double-check\n",
    "#     v_norm = np.sqrt(sum(v**2))    \n",
    "#     proj_u_on_v = (np.dot(u, v)/v_norm**2)*v\n",
    "#     u_sub = u - proj_u_on_v\n",
    "    return u_sub\n",
    "\n",
    "def calc_proj_matrix(A):\n",
    "    return A@np.linalg.inv(A.T@A)@A.T\n",
    "def calc_proj(b, A):\n",
    "    P = calc_proj_matrix(A)\n",
    "    return P@b.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dca024",
   "metadata": {},
   "source": [
    "# Single Lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc8cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"~/area2_population_analysis/sub-Han_desc-train_behavior+ecephys.nwb\"\n",
    "dataset_5ms = NWBDataset(filename, split_heldout=False)\n",
    "dataset_5ms.resample(5)\n",
    "dataset_5ms.smooth_spk(40, name='smth_40')\n",
    "bin_width = dataset_5ms.bin_width\n",
    "print(bin_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b3102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dims = 20 # for PCA\n",
    "\n",
    "active_mask = (~dataset_5ms.trial_info.ctr_hold_bump) & (dataset_5ms.trial_info.split != 'none')\n",
    "passive_mask = (dataset_5ms.trial_info.ctr_hold_bump) & (dataset_5ms.trial_info.split != 'none')\n",
    "\n",
    "\n",
    "trial_mask = passive_mask\n",
    "n_trials = dataset_5ms.trial_info.loc[trial_mask].shape[0]\n",
    "print(n_trials,'trials')\n",
    "n_neurons = dataset_5ms.data.spikes.shape[1]\n",
    "print(n_neurons,'neurons')\n",
    "\n",
    "all_data = np.array(dataset_5ms.data.spikes_smth_40)\n",
    "print(all_data.shape)\n",
    "data_for_pca = all_data[~np.isnan(all_data).any(axis=1)]\n",
    "print(data_for_pca.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(data_for_pca)\n",
    "pca = PCA(n_components=n_dims)\n",
    "X = pca.fit(X)\n",
    "\n",
    "PCA_data = nans([all_data.shape[0],n_dims])\n",
    "idx = 0\n",
    "for dp in all_data:\n",
    "    dp = dp.reshape((1, -1))\n",
    "    if np.isnan(dp).any():\n",
    "        dp_pca = nans([1,n_dims])\n",
    "    else:\n",
    "        dp_pca = pca.transform(scaler.transform(dp))\n",
    "    PCA_data[idx,:] = dp_pca\n",
    "    idx+=1\n",
    "print(PCA_data.shape)\n",
    "dataset_5ms.add_continuous_data(PCA_data,'PCA')\n",
    "print('PCA total var explained:',sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb1fa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_train_test(X,y,training_set,test_set):\n",
    "    X_train = X[training_set,:,:]\n",
    "    X_test = X[test_set,:,:]\n",
    "    y_train = y[training_set,:,:]\n",
    "    y_test = y[test_set,:,:]\n",
    "\n",
    "    #flat by trials\n",
    "    X_flat_train = X_train.reshape((X_train.shape[0]*X_train.shape[1]),X_train.shape[2])\n",
    "    X_flat_test = X_test.reshape((X_test.shape[0]*X_test.shape[1]),X_test.shape[2])\n",
    "    y_train=y_train.reshape((y_train.shape[0]*y_train.shape[1]),y_train.shape[2])\n",
    "    y_test=y_test.reshape((y_test.shape[0]*y_test.shape[1]),y_test.shape[2])\n",
    "    \n",
    "    X_flat_train_mean=np.nanmean(X_flat_train,axis=0)\n",
    "    X_flat_train_std=np.nanstd(X_flat_train,axis=0)   \n",
    "    #array with only 0 will have 0 std and cause errors\n",
    "    X_flat_train_std[X_flat_train_std==0] = 1\n",
    "    \n",
    "    X_flat_train=(X_flat_train-X_flat_train_mean)/X_flat_train_std\n",
    "    X_flat_test=(X_flat_test-X_flat_train_mean)/X_flat_train_std\n",
    "    y_train_mean=np.mean(y_train,axis=0)\n",
    "    y_train=y_train-y_train_mean\n",
    "    y_test=y_test-y_train_mean    \n",
    "    \n",
    "    return X_flat_train,X_flat_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f0762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict(dataset, trial_mask, align_field, align_range, lag, x_field, y_field):\n",
    "    \"\"\"Extracts spiking and kinematic data from selected trials and fits linear decoder\"\"\"\n",
    "    # Extract rate data from selected trials\n",
    "    vel_df = dataset.make_trial_data(align_field=align_field, align_range=align_range, ignored_trials=~trial_mask)\n",
    "    # Lag alignment for kinematics and extract kinematics data from selected trials\n",
    "    lag_align_range = (align_range[0] + lag, align_range[1] + lag)\n",
    "    rates_df = dataset.make_trial_data(align_field=align_field, align_range=lag_align_range, ignored_trials=~trial_mask)\n",
    "    \n",
    "    n_trials = rates_df['trial_id'].nunique()\n",
    "    n_timepoints = int((align_range[1] - align_range[0])/dataset.bin_width)\n",
    "    n_neurons = rates_df[x_field].shape[1]\n",
    "    \n",
    "    lr_all = GridSearchCV(Ridge(), {'alpha': np.logspace(-4, 1, 6)})\n",
    "    rates_array = rates_df[x_field].to_numpy()\n",
    "    vel_array = vel_df[y_field].to_numpy()\n",
    "    lr_all.fit(rates_array, vel_array)\n",
    "    pred_vel = lr_all.predict(rates_array)\n",
    "    vel_df = pd.concat([vel_df, pd.DataFrame(pred_vel, columns=dataset._make_midx('pred_vel', ['x', 'y'], 2))], axis=1)\n",
    "     \n",
    "    rates_array = rates_array.reshape(n_trials, n_timepoints, n_neurons)\n",
    "    vel_array = vel_array.reshape(n_trials, n_timepoints, 2)\n",
    "    \n",
    "    kf = KFold(n_splits=5,shuffle=True,random_state = 42)   \n",
    "    true_concat = nans([n_trials*n_timepoints,2])\n",
    "    pred_concat = nans([n_trials*n_timepoints,2])\n",
    "    trial_save_idx = 0\n",
    "    for training_set, test_set in kf.split(range(0,n_trials)):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test, y_train, y_test = process_train_test(rates_array,vel_array,training_set,test_set)\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-4, 1, 6)}) \n",
    "        lr.fit(X_train, y_train)\n",
    "        y_test_predicted = lr.predict(X_test)\n",
    "        \n",
    "        n = y_test_predicted.shape[0]\n",
    "        true_concat[trial_save_idx:trial_save_idx+n,:] = y_test\n",
    "        pred_concat[trial_save_idx:trial_save_idx+n,:] = y_test_predicted\n",
    "        trial_save_idx += n\n",
    "    \n",
    "    sses =get_sses_pred(true_concat,pred_concat)\n",
    "    sses_mean=get_sses_mean(true_concat)\n",
    "    R2 =1-np.sum(sses)/np.sum(sses_mean)     \n",
    "    print('R2:',R2) \n",
    "    return R2, lr_all.best_estimator_.coef_, vel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0e7e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_and_predict(dataset, trial_mask, align_field, align_range, lag, x_field, y_field, weights):\n",
    "    \"\"\"Extracts spiking and kinematic data from selected trials and fits linear decoder\"\"\"\n",
    "    # Extract rate data from selected trials\n",
    "    vel_df = dataset.make_trial_data(align_field=align_field, align_range=align_range, ignored_trials=~trial_mask)\n",
    "    # Lag alignment for kinematics and extract kinematics data from selected trials\n",
    "    lag_align_range = (align_range[0] + lag, align_range[1] + lag)\n",
    "    rates_df = dataset.make_trial_data(align_field=align_field, align_range=lag_align_range, ignored_trials=~trial_mask)\n",
    "    \n",
    "    n_trials = rates_df['trial_id'].nunique()\n",
    "    n_timepoints = int((align_range[1] - align_range[0])/dataset.bin_width)\n",
    "    n_neurons = rates_df[x_field].shape[1]\n",
    "\n",
    "    rates_array = rates_df[x_field].to_numpy() - calc_proj(rates_df[x_field].to_numpy(),weights.T).T\n",
    "    vel_array = vel_df[y_field].to_numpy()\n",
    "    \n",
    "    lr_all = GridSearchCV(Ridge(), {'alpha': np.logspace(-4, 1, 6)})\n",
    "    lr_all.fit(rates_array, vel_array)\n",
    "    pred_vel = lr_all.predict(rates_array)\n",
    "    vel_df = pd.concat([vel_df, pd.DataFrame(pred_vel, columns=dataset._make_midx('pred_vel', ['x', 'y'], 2))], axis=1)\n",
    "         \n",
    "    rates_array = rates_array.reshape(n_trials, n_timepoints, n_neurons)\n",
    "    vel_array = vel_array.reshape(n_trials, n_timepoints, 2)\n",
    "    \n",
    "    kf = KFold(n_splits=5,shuffle=True,random_state = 42)   \n",
    "    true_concat = nans([n_trials*n_timepoints,2])\n",
    "    pred_concat = nans([n_trials*n_timepoints,2])\n",
    "    trial_save_idx = 0\n",
    "    for training_set, test_set in kf.split(range(0,n_trials)):\n",
    "        #split training and testing by trials\n",
    "        X_train, X_test, y_train, y_test = process_train_test(rates_array,vel_array,training_set,test_set)\n",
    "        lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-4, 1, 6)}) \n",
    "        lr.fit(X_train, y_train)\n",
    "        y_test_predicted = lr.predict(X_test)\n",
    "        \n",
    "        n = y_test_predicted.shape[0]\n",
    "        true_concat[trial_save_idx:trial_save_idx+n,:] = y_test\n",
    "        pred_concat[trial_save_idx:trial_save_idx+n,:] = y_test_predicted\n",
    "        trial_save_idx += n\n",
    "    \n",
    "    sses =get_sses_pred(true_concat,pred_concat)\n",
    "    sses_mean=get_sses_mean(true_concat)\n",
    "    R2 =1-np.sum(sses)/np.sum(sses_mean)     \n",
    "    print('R2:',R2) \n",
    "    return R2, lr_all.best_estimator_.coef_, vel_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257bacfe",
   "metadata": {},
   "source": [
    "## with Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106dc233",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_axis = np.arange(-300,300,20)\n",
    "x_field = 'spikes_smth_40'\n",
    "y_field ='hand_vel'\n",
    "trial_mask = passive_mask\n",
    "\n",
    "# Prepare for plotting\n",
    "plot_dir = [0.0, 90.0, 180.0, 270.0] # limit plot directions to reduce cluttering\n",
    "plot_dim = 'x' # plot x velocity\n",
    "colors = ['red', 'blue', 'green', 'orange']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8771f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_range = (0, 120)\n",
    "x_axis = np.arange(pred_range[0], pred_range[1], dataset_5ms.bin_width)\n",
    "\n",
    "early_r2_array = nans([len(lag_axis)])\n",
    "early_coef_array = nans([len(lag_axis),2,n_neurons])\n",
    "for i in range(len(lag_axis)):\n",
    "    lag = lag_axis[i]\n",
    "    r2, coef, _ = fit_and_predict(dataset_5ms, trial_mask, 'move_onset_time', (0, 120), lag, x_field, y_field)\n",
    "    early_r2_array[i] = r2\n",
    "    early_coef_array[i,:,:] = coef\n",
    "\n",
    "curr_r2_array = early_r2_array\n",
    "curr_coef_array = early_coef_array\n",
    "    \n",
    "idx_max = np.argmax(curr_r2_array)\n",
    "time_max = lag_axis[idx_max]\n",
    "\n",
    "_, _, vel_df = fit_and_predict(dataset_5ms, trial_mask, 'move_onset_time', pred_range, time_max, x_field, y_field)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset_5ms.trial_info[dataset_5ms.trial_info.cond_dir == trial_dir].trial_id\n",
    "    for _, trial in vel_df[np.isin(vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        plt.plot(x_axis, trial.hand_vel[plot_dim], color=color, linewidth=0.5)\n",
    "plt.show()\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset_5ms.trial_info[dataset_5ms.trial_info.cond_dir == trial_dir].trial_id\n",
    "    for _, trial in vel_df[np.isin(vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        plt.plot(x_axis, trial.pred_vel[plot_dim], color=color, linewidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(lag_axis, curr_r2_array)\n",
    "plt.axvline(time_max, color = 'r', label='t_max = ' + str(time_max))\n",
    "plt.legend()\n",
    "plt.title('R2 score predicting hand velocity [0,120]')\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R2')\n",
    "plt.show()\n",
    "\n",
    "# ang_dist_to_max = nans([len(lag_axis)])\n",
    "# for i in range(0, len(curr_coef_array)):\n",
    "#     ang_dist_to_max[i] = math.degrees(angle_between(curr_coef_array[i,0,:],curr_coef_array[idx_max,0,:]))\n",
    "# plt.scatter(lag_axis, ang_dist_to_max)\n",
    "# plt.title('Angular distance to X-vel decoding dim at t_max')\n",
    "# plt.xlabel('Time lag (ms)')\n",
    "# plt.ylabel('Angular distance (degrees)')\n",
    "# plt.show()\n",
    "\n",
    "weights = curr_coef_array[idx_max,:,:]\n",
    "for iter in range(0,3):  \n",
    "    #subtract predictions with primary decoding dimensions (at time with max R2)\n",
    "    sub_coef_array = nans([len(lag_axis),2,n_neurons])\n",
    "    sub_r2_array = nans([len(lag_axis)])\n",
    "\n",
    "    for i in range(len(lag_axis)):\n",
    "        lag = lag_axis[i]\n",
    "        r2, coef,_ = sub_and_predict(dataset_5ms, trial_mask, 'move_onset_time', pred_range, lag,x_field,y_field,weights)\n",
    "        sub_r2_array[i] = r2\n",
    "        sub_coef_array[i,:,:] = coef\n",
    "\n",
    "    plt.plot(lag_axis,sub_r2_array)\n",
    "    plt.title('R2 score projecting out #'+ str(iter+1) +' t_max dim')\n",
    "    idx_max = np.argmax(sub_r2_array)\n",
    "    time_max = lag_axis[idx_max]\n",
    "    plt.axvline(time_max, color = 'r', label='t_max = ' + str(time_max))\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time lag (ms)')\n",
    "    plt.ylabel('R2')\n",
    "    plt.show()\n",
    "\n",
    "    _, _, vel_df = sub_and_predict(dataset_5ms, trial_mask, 'move_onset_time', pred_range, time_max, x_field, y_field,weights)\n",
    "    for trial_dir, color in zip(plot_dir, colors):\n",
    "        cond_ids = dataset_5ms.trial_info[dataset_5ms.trial_info.cond_dir == trial_dir].trial_id\n",
    "        for _, trial in vel_df[np.isin(vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "            plt.plot(x_axis, trial.pred_vel[plot_dim], color=color, linewidth=0.5)\n",
    "    plt.show()\n",
    "#     plt.plot(lag_axis,np.subtract(sub_r2_array,curr_r2_array))\n",
    "#     plt.axhline(0,color = 'k',linestyle='--')\n",
    "#     plt.title('R2 difference after projecting out t_max dim')\n",
    "#     plt.xlabel('Time lag (ms)')\n",
    "#     plt.ylabel('R2 difference')\n",
    "#     plt.show()\n",
    "#     curr_r2_array = sub_r2_array\n",
    "\n",
    "    #stack the decoding dimensions to be projected out\n",
    "    weights = np.vstack((weights,sub_coef_array[idx_max,:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f030f4",
   "metadata": {},
   "source": [
    "## with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d6d3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_field = 'PCA'\n",
    "y_field ='hand_vel'\n",
    "lag_axis = np.arange(-300,300,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f37ed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_range = (0, 120)\n",
    "x_axis = np.arange(pred_range[0], pred_range[1], dataset_5ms.bin_width)\n",
    "\n",
    "PCA_early_r2_array = nans([len(lag_axis)])\n",
    "PCA_early_coef_array = nans([len(lag_axis),2,n_dims])\n",
    "for i in range(len(lag_axis)):\n",
    "    lag = lag_axis[i]\n",
    "    r2, coef, _ = fit_and_predict(dataset_5ms, trial_mask, 'move_onset_time', pred_range, lag, x_field, y_field)\n",
    "    PCA_early_r2_array[i] = r2\n",
    "    PCA_early_coef_array[i,:,:] = coef\n",
    "\n",
    "curr_r2_array = PCA_early_r2_array\n",
    "curr_coef_array = PCA_early_coef_array\n",
    "idx_max = np.argmax(curr_r2_array)\n",
    "time_max = lag_axis[idx_max]\n",
    "_, _, vel_df = fit_and_predict(dataset_5ms, trial_mask, 'move_onset_time', pred_range, time_max, x_field, y_field)\n",
    "for trial_dir, color in zip(plot_dir, colors):\n",
    "    cond_ids = dataset_5ms.trial_info[dataset_5ms.trial_info.cond_dir == trial_dir].trial_id\n",
    "    for _, trial in vel_df[np.isin(vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "        plt.plot(x_axis, trial.pred_vel[plot_dim], color=color, linewidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(lag_axis, curr_r2_array)\n",
    "plt.axvline(time_max, color = 'r', label='t_max = ' + str(time_max))\n",
    "plt.legend()\n",
    "plt.title('R2 score predicting hand velocity [0,120]')\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R2')\n",
    "plt.show()\n",
    "\n",
    "weights = curr_coef_array[idx_max,:,:]\n",
    "for iter in range(0,3):  \n",
    "    #subtract predictions with primary decoding dimensions (at time with max R2)\n",
    "    sub_coef_array = nans([len(lag_axis),2,n_dims])\n",
    "    sub_r2_array = nans([len(lag_axis)])\n",
    "    for i in range(len(lag_axis)):\n",
    "        lag = lag_axis[i]\n",
    "        r2, coef,_ = sub_and_predict(dataset_5ms, trial_mask, 'move_onset_time', pred_range, lag,x_field,y_field,weights)\n",
    "        sub_r2_array[i] = r2\n",
    "        sub_coef_array[i,:,:] = coef\n",
    "\n",
    "    plt.plot(lag_axis,sub_r2_array)\n",
    "    plt.title('R2 score projecting out #'+ str(iter+1) +' t_max dim')\n",
    "    idx_max = np.argmax(sub_r2_array)\n",
    "    time_max = lag_axis[idx_max]\n",
    "    plt.axvline(time_max, color = 'r', label='t_max = ' + str(time_max))\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time lag (ms)')\n",
    "    plt.ylabel('R2')\n",
    "    plt.show()\n",
    "    \n",
    "    _, _, vel_df = sub_and_predict(dataset_5ms, trial_mask, 'move_onset_time', pred_range, time_max, x_field, y_field,weights)\n",
    "    for trial_dir, color in zip(plot_dir, colors):\n",
    "        cond_ids = dataset_5ms.trial_info[dataset_5ms.trial_info.cond_dir == trial_dir].trial_id\n",
    "        for _, trial in vel_df[np.isin(vel_df.trial_id, cond_ids)].groupby('trial_id'):\n",
    "            plt.plot(x_axis, trial.pred_vel[plot_dim], color=color, linewidth=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    weights = np.vstack((weights,sub_coef_array[idx_max,:,:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fab41e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bf152a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e99868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80e66b89",
   "metadata": {},
   "source": [
    "# Multi Lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8649848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_50ms = NWBDataset(filename, split_heldout=False)\n",
    "dataset_50ms.resample(50)\n",
    "print(dataset_50ms.bin_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda3bfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dims = 20 # for PCA\n",
    "\n",
    "active_mask = (~dataset_50ms.trial_info.ctr_hold_bump) & (dataset_50ms.trial_info.split != 'none')\n",
    "passive_mask = (dataset_50ms.trial_info.ctr_hold_bump) & (dataset_50ms.trial_info.split != 'none')\n",
    "\n",
    "\n",
    "trial_mask = passive_mask\n",
    "n_trials = dataset_50ms.trial_info.loc[trial_mask].shape[0]\n",
    "print(n_trials,'trials')\n",
    "n_neurons = dataset_50ms.data.spikes.shape[1]\n",
    "print(n_neurons,'neurons')\n",
    "\n",
    "all_data = np.array(dataset_50ms.data.spikes)\n",
    "print(all_data.shape)\n",
    "data_for_pca = all_data[~np.isnan(all_data).any(axis=1)]\n",
    "print(data_for_pca.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(data_for_pca)\n",
    "pca = PCA(n_components=n_dims)\n",
    "X = pca.fit(X)\n",
    "\n",
    "PCA_data = nans([all_data.shape[0],n_dims])\n",
    "idx = 0\n",
    "for dp in all_data:\n",
    "    dp = dp.reshape((1, -1))\n",
    "    if np.isnan(dp).any():\n",
    "        dp_pca = nans([1,n_dims])\n",
    "    else:\n",
    "        dp_pca = pca.transform(scaler.transform(dp))\n",
    "    PCA_data[idx,:] = dp_pca\n",
    "    idx+=1\n",
    "print(PCA_data.shape)\n",
    "dataset_50ms.add_continuous_data(PCA_data,'PCA')\n",
    "print('PCA total var explained:',sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a348315",
   "metadata": {},
   "outputs": [],
   "source": [
    "passive_data = dataset_50ms.make_trial_data(align_field='move_onset_time', align_range=(-300, 700), ignored_trials=~trial_mask)\n",
    "for idx, trial in passive_data.groupby('trial_id'):\n",
    "    n_timepoints = trial.shape[0]\n",
    "    break\n",
    "print(n_timepoints,'time bins')\n",
    "\n",
    "passive_trials_neuron = nans([n_trials,n_timepoints,n_neurons])\n",
    "passive_trials_vel = nans([n_trials,n_timepoints,2])\n",
    "passive_trials_pca = nans([n_trials,n_timepoints,n_dims])\n",
    "i = 0\n",
    "for idx, trial in passive_data.groupby('trial_id'):\n",
    "    passive_trials_neuron[i,:,:]=trial.spikes.to_numpy()\n",
    "    passive_trials_vel[i,:,:]=trial.hand_vel.to_numpy()\n",
    "    passive_trials_pca[i,:,:]=trial.PCA.to_numpy()\n",
    "    i+=1\n",
    "print(passive_trials_neuron.shape)\n",
    "print(passive_trials_vel.shape)\n",
    "print(passive_trials_pca.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef332cd6",
   "metadata": {},
   "source": [
    "## with Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8378b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_range = [-300,700]\n",
    "pred_start = 0\n",
    "pred_end = 120\n",
    "\n",
    "idx1 = int((pred_start - data_range[0])/dataset_50ms.bin_width)\n",
    "idx2 = int(n_timepoints - (data_range[1]-pred_end)/dataset_50ms.bin_width)\n",
    "\n",
    "t_before_range = range(0,301,50);\n",
    "t_after_range = range(0,501,50);\n",
    "\n",
    "whole_multi_R2s = nans([len(t_before_range),len(t_after_range)])\n",
    "whole_multi_coefs = []\n",
    "j,k=0,0\n",
    "for time_before in t_before_range:\n",
    "    coef_arr = []\n",
    "    for time_after in t_after_range:\n",
    "        print('Predicting with',-time_before, 'to', time_after,'ms neural data')\n",
    "        \n",
    "        bins_before= int(time_before/dataset_50ms.bin_width) #How many bins of neural data prior to the output are used for decoding\n",
    "        bins_current= 1 #Whether to use concurrent time bin of neural data\n",
    "        bins_after= int(time_after/dataset_50ms.bin_width) #How many bins of neural data after the output are used for decoding\n",
    "\n",
    "        n_total_bins = bins_before + bins_current + bins_after\n",
    "\n",
    "        X =  nans([n_trials,idx2-idx1,n_total_bins*n_neurons])\n",
    "        i = 0\n",
    "        for trial_data in passive_trials_neuron:\n",
    "            trial_hist=get_spikes_with_history(trial_data,bins_before,bins_after,bins_current)\n",
    "            trial_hist = trial_hist[idx1:idx2,:,:]\n",
    "            trial_hist_flat=trial_hist.reshape(trial_hist.shape[0],(trial_hist.shape[1]*trial_hist.shape[2]))\n",
    "            X[i,:,:] = trial_hist_flat\n",
    "            i+=1\n",
    "        \n",
    "        y = passive_trials_vel[:,idx1:idx2,:]\n",
    "    \n",
    "        lr_all = GridSearchCV(Ridge(), {'alpha': np.logspace(-4, 4, 9)})\n",
    "        X_reshaped = X.reshape((X.shape[0]*X.shape[1]),X.shape[2])\n",
    "        y_reshaped = y.reshape((y.shape[0]*y.shape[1]),y.shape[2])\n",
    "        lr_all.fit(X_reshaped, y_reshaped)\n",
    "\n",
    "        \n",
    "        kf = KFold(n_splits=5,shuffle=True,random_state = 42)   \n",
    "        true_concat = nans([(n_trials*(idx2-idx1)),2])\n",
    "        pred_concat = nans([(n_trials*(idx2-idx1)),2])\n",
    "        trial_save_idx = 0\n",
    "        for training_set, test_set in kf.split(range(0,n_trials)):\n",
    "            #split training and testing by trials\n",
    "            X_train, X_test, y_train, y_test = process_train_test(X,y,training_set,test_set)\n",
    "            lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-4, 4, 9)}) \n",
    "            lr.fit(X_train, y_train)\n",
    "            y_test_predicted = lr.predict(X_test)\n",
    "            n = y_test_predicted.shape[0]\n",
    "            true_concat[trial_save_idx:trial_save_idx+n,:] = y_test\n",
    "            pred_concat[trial_save_idx:trial_save_idx+n,:] = y_test_predicted\n",
    "            trial_save_idx += n\n",
    "\n",
    "        sses =get_sses_pred(true_concat,pred_concat)\n",
    "        sses_mean=get_sses_mean(true_concat)\n",
    "        whole_multi_R2s[j,k] =1-np.sum(sses)/np.sum(sses_mean)     \n",
    "        print('R2:',whole_multi_R2s[j,k])\n",
    "        coef_arr.append(lr_all.best_estimator_.coef_)\n",
    "        k += 1\n",
    "    j += 1\n",
    "    k = 0\n",
    "    whole_multi_coefs.append(coef_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf34e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(whole_multi_R2s)\n",
    "ax.set_xlabel('Length of lagging info')\n",
    "ax.set_ylabel('Length of leading info')\n",
    "\n",
    "ax.set_xticks(np.arange(len(t_after_range)))\n",
    "ax.set_yticks(np.arange(len(t_before_range)))\n",
    "ax.set_xticklabels(labels=t_after_range)\n",
    "ax.set_yticklabels(labels=t_before_range)\n",
    "\n",
    "ax.set_title(\"R2 predicting [0, 120] velocity \\nwith different lagging/leading info\")\n",
    "fig.tight_layout()\n",
    " \n",
    "for i in range(len(t_before_range)):\n",
    "    for j in range(len(t_after_range)):\n",
    "        text = ax.text(j, i, str(int(whole_multi_R2s[i, j]*1000)/1000),\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8958e911",
   "metadata": {},
   "source": [
    "## with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6de7686",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_whole_multi_R2s = nans([len(t_before_range),len(t_after_range)])\n",
    "PCA_whole_multi_coefs = []\n",
    "j,k=0,0\n",
    "for time_before in t_before_range:\n",
    "    coef_arr = []\n",
    "    for time_after in t_after_range:\n",
    "        print('Predicting with',-time_before, 'to', time_after,'ms neural data')\n",
    "        \n",
    "        bins_before= int(time_before/dataset_50ms.bin_width) #How many bins of neural data prior to the output are used for decoding\n",
    "        bins_current= 1 #Whether to use concurrent time bin of neural data\n",
    "        bins_after= int(time_after/dataset_50ms.bin_width) #How many bins of neural data after the output are used for decoding\n",
    "\n",
    "        n_total_bins = bins_before + bins_current + bins_after\n",
    "\n",
    "        X =  nans([n_trials,idx2-idx1,n_total_bins*n_dims])\n",
    "        i = 0\n",
    "        for trial_data in passive_trials_pca:\n",
    "            trial_hist=get_spikes_with_history(trial_data,bins_before,bins_after,bins_current)\n",
    "            trial_hist = trial_hist[idx1:idx2,:,:]\n",
    "            trial_hist_flat=trial_hist.reshape(trial_hist.shape[0],(trial_hist.shape[1]*trial_hist.shape[2]))\n",
    "            X[i,:,:] = trial_hist_flat\n",
    "            i+=1\n",
    "        \n",
    "        y = passive_trials_vel[:,idx1:idx2,:]\n",
    "    \n",
    "        lr_all = GridSearchCV(Ridge(), {'alpha': np.logspace(-4, 4, 9)})\n",
    "        X_reshaped = X.reshape((X.shape[0]*X.shape[1]),X.shape[2])\n",
    "        y_reshaped = y.reshape((y.shape[0]*y.shape[1]),y.shape[2])\n",
    "        lr_all.fit(X_reshaped, y_reshaped)\n",
    "\n",
    "        \n",
    "        kf =KFold(n_splits=5,shuffle=True,random_state = 42)   \n",
    "        true_concat = nans([(n_trials*(idx2-idx1)),2])\n",
    "        pred_concat = nans([(n_trials*(idx2-idx1)),2])\n",
    "        trial_save_idx = 0\n",
    "        for training_set, test_set in kf.split(range(0,n_trials)):\n",
    "            #split training and testing by trials\n",
    "            X_train, X_test, y_train, y_test = process_train_test(X,y,training_set,test_set)\n",
    "            lr = GridSearchCV(Ridge(), {'alpha': np.logspace(-4, 4, 9)}) \n",
    "            lr.fit(X_train, y_train)\n",
    "            y_test_predicted = lr.predict(X_test)\n",
    "            n = y_test_predicted.shape[0]\n",
    "            true_concat[trial_save_idx:trial_save_idx+n,:] = y_test\n",
    "            pred_concat[trial_save_idx:trial_save_idx+n,:] = y_test_predicted\n",
    "            trial_save_idx += n\n",
    "\n",
    "        sses =get_sses_pred(true_concat,pred_concat)\n",
    "        sses_mean=get_sses_mean(true_concat)\n",
    "        PCA_whole_multi_R2s[j,k] =1-np.sum(sses)/np.sum(sses_mean)     \n",
    "        print('R2:',PCA_whole_multi_R2s[j,k])\n",
    "        coef_arr.append(lr_all.best_estimator_.coef_)\n",
    "        k += 1\n",
    "    j += 1\n",
    "    k = 0\n",
    "    PCA_whole_multi_coefs.append(coef_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f8ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(PCA_whole_multi_R2s)\n",
    "ax.set_xlabel('Length of lagging info')\n",
    "ax.set_ylabel('Length of leading info')\n",
    "\n",
    "ax.set_xticks(np.arange(len(t_after_range)))\n",
    "ax.set_yticks(np.arange(len(t_before_range)))\n",
    "ax.set_xticklabels(labels=t_after_range)\n",
    "ax.set_yticklabels(labels=t_before_range)\n",
    "\n",
    "ax.set_title(\"R2 predicting [0, 120] velocity \\nwith different lagging/leading info\")\n",
    "fig.tight_layout()\n",
    "\n",
    "for i in range(len(t_before_range)):\n",
    "    for j in range(len(t_after_range)):\n",
    "        text = ax.text(j, i, str(int(PCA_whole_multi_R2s[i, j]*1000)/1000),\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd444f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
