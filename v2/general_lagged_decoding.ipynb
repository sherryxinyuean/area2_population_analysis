{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import math\n",
    "matplotlib.rc('font', size=18)\n",
    "import scipy\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothing functions\n",
    "import scipy.signal as signal\n",
    "def smooth_column(x, window, dtype):\n",
    "    y = signal.convolve(x.astype(dtype),window,'same')\n",
    "    return y\n",
    "def smooth_spk(neural_data, gauss_width, bin_width,dtype=\"float64\"):\n",
    "    \"\"\" Smooth spikes by Gaussian kernel \"\"\"\n",
    "    gauss_bin_std = gauss_width / bin_width\n",
    "    win_len = int(6*gauss_bin_std)\n",
    "    window = signal.gaussian(win_len, gauss_bin_std, sym = True)\n",
    "    window /= np.sum(window)\n",
    "    smoothed_spikes = np.apply_along_axis(lambda x: smooth_column(x, window, dtype), 0, neural_data)\n",
    "    return smoothed_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 function\n",
    "def get_sses_pred(y_test,y_test_pred):\n",
    "    sse=np.sum((y_test_pred-y_test)**2,axis=0)\n",
    "    return sse\n",
    "def get_sses_mean(y_test):\n",
    "    y_mean=np.mean(y_test,axis=0)\n",
    "    sse_mean=np.sum((y_test-y_mean)**2,axis=0)\n",
    "    return sse_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing for cross-validation loops\n",
    "def process_train_test(X,y,training_set,test_set):\n",
    "    \"\"\" z-score X, mean-center Y\"\"\"\n",
    "    X_train = X[training_set,:]\n",
    "    X_test = X[test_set,:]\n",
    "    y_train = y[training_set,:]\n",
    "    y_test = y[test_set,:]\n",
    "\n",
    "    X_train_mean=np.nanmean(X_train,axis=0)\n",
    "    X_train_std=np.nanstd(X_train,axis=0)   \n",
    "    y_train_mean=np.nanmean(y_train,axis=0)\n",
    "\n",
    "    X_train=(X_train-X_train_mean)/X_train_std\n",
    "    X_test=(X_test-X_train_mean)/X_train_std\n",
    "\n",
    "    y_train=y_train-y_train_mean\n",
    "    y_test=y_test-y_train_mean\n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "def nans(shape, dtype=float):\n",
    "    \"\"\" Returns array of NaNs with defined shape\"\"\"\n",
    "    a = np.empty(shape, dtype)\n",
    "    a.fill(np.nan)\n",
    "    return a\n",
    "\n",
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'\"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and formatting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "foldername = \"/Users/sherryan/area2_population_analysis/random-target/\"\n",
    "data = sio.loadmat(foldername+'s1_data_trialized.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinematics = data['acceleration']\n",
    "neural_data = data['neural_data']\n",
    "\n",
    "print(kinematics.shape,'kinematics')\n",
    "print(neural_data.shape,'neural data')\n",
    "n_trials = len(neural_data)\n",
    "print(n_trials,'trials')\n",
    "n_neurons = neural_data[0,0].shape[1]\n",
    "print(n_neurons,'neurons')\n",
    "\n",
    "# kinematics (Y) and neural_data (X) are shaped (n_trials,1) \n",
    "# each is an 1d array of 2d arrays; I did not use a 3d array so to allow for variable trial length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each trial of neural data is shaped (n_bins, n_neurons)\n",
    "# each trial of kinematics is shaped (n_bins, n_kinematic_features)\n",
    "\n",
    "print('Data shape for the 1st trial:')\n",
    "print(neural_data[0,0].shape)\n",
    "print(kinematics[0,0].shape)\n",
    "print('')\n",
    "print('Data shape for the 2nd trial:')\n",
    "print(neural_data[1,0].shape)\n",
    "print(kinematics[1,0].shape)\n",
    "print('')\n",
    "print('Data shape for the 3rd trial:')\n",
    "print(neural_data[2,0].shape)\n",
    "print(kinematics[2,0].shape)\n",
    "\n",
    "\n",
    "# Note: length of kinematics is the actual trial length,\n",
    "# neural data should have more timepoints than kinematics to allow time-lagged decoding, can be wider than the range of time lags of interest\n",
    "# i.e. here I'm interested in testing efferent/afferent signals between -300 to +300ms, but I got neural data -500ms to +500ms;\n",
    "# which means I included 50 more timepoints prior to trial start (10-ms bin), and 50 more timepoints post trial end, thus in total 100 more timepoints\n",
    "\n",
    "# let the model know the amount of previous/future bins are included in neural data\n",
    "n_previous_bins = 50\n",
    "n_future_bins = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data smoothing (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this data has 0.01 s = 10 ms bins \n",
    "# dt = 0.01\n",
    "# gauss_width = 40 #in ms\n",
    "# bin_width = dt*1000 #in ms\n",
    "\n",
    "# # smooth neural data by 40-ms gaussian kernel\n",
    "# neural_smth_40 = []\n",
    "# for i in range(n_trials):\n",
    "#     neural_d = neural_data[i,0]\n",
    "#     smth_40 = smooth_spk(neural_d, gauss_width, bin_width)\n",
    "#     neural_smth_40.append(smth_40)\n",
    "# neural_smth_40 = np.array(neural_smth_40,dtype=object).reshape(n_trials,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Check effects of smoothing for 10 neurons in the first trial\n",
    "# plt.plot(neural_data[0,0][:,:10])\n",
    "# plt.show()\n",
    "# plt.plot(neural_smth_40[0,0][:,:10])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFY THIS PART\n",
    "\n",
    "lag_axis = np.arange(-300, 300, 20) # array of time lags to try, here -300 to +300 ms with 20-ms step\n",
    "bin_width = 10 #in ms\n",
    "x = neural_smth_40\n",
    "y = kinematics\n",
    "\n",
    "filter = False #default filter option as False, can set in the next cell\n",
    "\n",
    "n_total_datapoints = 0\n",
    "for i in range(len(y)): \n",
    "    n_total_datapoints+=y[i,0].shape[0] # Count total number of data points to fit in model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "def flat_lagged_neural(x, n_previous_bins, n_future_bins, lagged_bins, n_total_datapoints):\n",
    "    \"\"\" Flatten time-lagged & trial-ized neural data into 2d array \"\"\"\n",
    "    x_flat = nans([n_total_datapoints,n_neurons])    \n",
    "    trial_save_idx = 0\n",
    "    for tr in range(len(x)):\n",
    "        trial_data = x[tr,0]\n",
    "        start, end = int(n_previous_bins+lagged_bins), int(len(trial_data)-n_future_bins+lagged_bins)\n",
    "        n = len(trial_data[start:end,:])\n",
    "        x_flat[trial_save_idx:trial_save_idx+n,:] = trial_data[start:end,:]\n",
    "        trial_save_idx+=n\n",
    "    return x_flat\n",
    "def flat_kinematics(y,n_total_timepoints):\n",
    "    \"\"\" Flatten trial-ized kinematics into 2d array \"\"\"\n",
    "    y_flat = nans([n_total_timepoints,2])\n",
    "    trial_save_idx = 0\n",
    "    for tr in range(len(y)):\n",
    "        n = len(y[tr,0])\n",
    "        y_flat[trial_save_idx:trial_save_idx+n,:] = y[tr,0]\n",
    "        trial_save_idx+=n\n",
    "    return y_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #OPTIONAL FILTER, filter out invalid nrns that are not very active during trial time\n",
    "# filter = True\n",
    "# x_flat = flat_lagged_neural(neural_data, n_previous_bins, n_future_bins, 0, n_total_datapoints)\n",
    "# fr_thresh = 1 #threshold in Hz\n",
    "# fr = np.sum(x_flat,axis=0)/x_flat.shape[0]*(1000/bin_width)\n",
    "# neuron_filter = fr > fr_thresh\n",
    "# print(neuron_filter)\n",
    "# print(sum(~neuron_filter),'invalid neurons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_flat = flat_kinematics(y, n_total_datapoints) #flatten y\n",
    "y_proc = y_flat - np.nanmean(y_flat,axis=0) #mean-center y\n",
    "\n",
    "#save R2, r, and coeffients across lags\n",
    "r2_array = nans([len(lag_axis)])\n",
    "r_array = nans([len(lag_axis)])\n",
    "if filter:\n",
    "    coef_array = nans([len(lag_axis),2,sum(neuron_filter)]) #OPTIONAL FILTER\n",
    "else:\n",
    "    coef_array = nans([len(lag_axis),2,n_neurons]) \n",
    "\n",
    "for i in range(len(lag_axis)):\n",
    "    current_lagged_bins = lag_axis[i]/bin_width\n",
    "    x_flat = flat_lagged_neural(x, n_previous_bins, n_future_bins, current_lagged_bins, n_total_datapoints) #flatten x\n",
    "    if filter:\n",
    "        x_flat = x_flat[:,neuron_filter] #OPTIONAL FILTER\n",
    "    x_proc = (x_flat - np.nanmean(x_flat,axis=0))/np.nanstd(x_flat,axis=0) #z-score x\n",
    "\n",
    "    lr_all = GridSearchCV(Ridge(),{'alpha':np.logspace(-3, 3, 7)}) #Grid-search penalty term for Ridge regularization\n",
    "    lr_all.fit(x_proc, y_proc)\n",
    "    coef_array[i,:,:] = lr_all.best_estimator_.coef_ #Save model's coefficient as fitted to all data points\n",
    "\n",
    "    #Cross-validate R2\n",
    "    kf = KFold(n_splits=5, shuffle=False) #Simple 5-fold CV (not the most rigorous); no shuffle, otherwise there'll be too much leakage across trials\n",
    "    true_concat = nans([n_total_datapoints,2])\n",
    "    pred_concat = nans([n_total_datapoints,2]) #Concatenate true and predicted kinematics for final R2 calculation\n",
    "    save_idx = 0\n",
    "    for training_set, test_set in kf.split(range(0,len(x_flat))):\n",
    "        X_train, X_test, y_train, y_test = process_train_test(x_flat,y_flat,training_set,test_set)\n",
    "        lr = GridSearchCV(Ridge(),{'alpha':np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_test_predicted = lr.predict(X_test)\n",
    "        n = y_test_predicted.shape[0]\n",
    "        true_concat[save_idx:save_idx+n,:] = y_test\n",
    "        pred_concat[save_idx:save_idx+n,:] = y_test_predicted\n",
    "        save_idx += n\n",
    "    # r2 across features\n",
    "    sses =get_sses_pred(true_concat,pred_concat)\n",
    "    sses_mean=get_sses_mean(true_concat)\n",
    "    r2 =1-np.sum(sses)/np.sum(sses_mean)     \n",
    "    # mean spearman's r across features\n",
    "    rs = []\n",
    "    for feature in range(y_flat.shape[1]):\n",
    "        rs.append(scipy.stats.spearmanr(pred_concat[:,feature], true_concat[:,feature]).correlation)\n",
    "    r = np.mean(rs) \n",
    "    r2_array[i] = r2; r_array[i] = r;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 across lags (are there two peaks?)\n",
    "idx_max = np.argmax(r2_array)\n",
    "plt.plot(lag_axis,r2_array, color = 'k')\n",
    "plt.axvline(lag_axis[idx_max],color = 'k',linestyle='--')\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation across lags\n",
    "plt.plot(lag_axis,r_array, color = 'k')\n",
    "plt.axvline(lag_axis[idx_max],color = 'k',linestyle='--')\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('Correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Angle between decoder at each lag with the best decoder (relationship between decoders across lags)\n",
    "idx_max = np.argmax(r2_array)\n",
    "ang_to_max_0 = nans([len(lag_axis)])\n",
    "ang_to_max_1 = nans([len(lag_axis)])\n",
    "for i in range(0, len(coef_array)):\n",
    "    ang_to_max_0[i] = math.degrees(angle_between(coef_array[i,0,:],coef_array[idx_max,0,:]))\n",
    "    ang_to_max_1[i] = math.degrees(angle_between(coef_array[i,1,:],coef_array[idx_max,1,:]))\n",
    "plt.scatter(lag_axis, ang_to_max_0,label = 'Feature 1',color = 'green')\n",
    "plt.scatter(lag_axis, ang_to_max_1,label = 'Feature 2',color = 'blue')\n",
    "plt.legend()\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('Angle (degrees)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit with projection out\n",
    "def calc_proj_matrix(A):\n",
    "    return A@np.linalg.inv(A.T@A)@A.T\n",
    "def calc_proj(R, w):\n",
    "    \"\"\" Returns projection of R onto the space defined by w \"\"\"\n",
    "    P = calc_proj_matrix(w)\n",
    "    return P@R.T\n",
    "\n",
    "best_decoder = coef_array[idx_max,:,:]\n",
    "proj_r2_array = nans([len(lag_axis)])\n",
    "proj_r_array = nans([len(lag_axis)])\n",
    "\n",
    "for i in range(len(lag_axis)):\n",
    "    current_lagged_bins = lag_axis[i]/bin_width\n",
    "    x_flat = flat_lagged_neural(x, n_previous_bins, n_future_bins, current_lagged_bins, n_total_datapoints) #flatten x\n",
    "    if filter:\n",
    "        x_flat = x_flat[:,neuron_filter] #OPTIONAL\n",
    "    x_proc = (x_flat - np.nanmean(x_flat,axis=0))/np.nanstd(x_flat,axis=0) #z-score x\n",
    "    x_sub = x_proc - calc_proj(x_proc,best_decoder.T).T\n",
    "    x_new = x_sub * np.nanstd(x_flat,axis=0) + np.nanmean(x_flat,axis=0)\n",
    "\n",
    "    #Cross-validate R2\n",
    "    kf = KFold(n_splits=5, shuffle=False) #Simple 5-fold CV (not the most rigorous); no shuffle, otherwise there'll be too much leakage across trials\n",
    "    true_concat = nans([n_total_datapoints,2])\n",
    "    pred_concat = nans([n_total_datapoints,2]) #Concatenate true and predicted kinematics for final R2 calculation\n",
    "    save_idx = 0\n",
    "    for training_set, test_set in kf.split(range(0,len(x_new))):\n",
    "        X_train, X_test, y_train, y_test = process_train_test(x_new,y_flat,training_set,test_set)\n",
    "        lr = GridSearchCV(Ridge(),{'alpha':np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_test_predicted = lr.predict(X_test)\n",
    "        n = y_test_predicted.shape[0]\n",
    "        true_concat[save_idx:save_idx+n,:] = y_test\n",
    "        pred_concat[save_idx:save_idx+n,:] = y_test_predicted\n",
    "        save_idx += n\n",
    "    # r2 across features\n",
    "    sses =get_sses_pred(true_concat,pred_concat)\n",
    "    sses_mean=get_sses_mean(true_concat)\n",
    "    r2 =1-np.sum(sses)/np.sum(sses_mean)     \n",
    "    # mean spearman's r across features\n",
    "    rs = []\n",
    "    for feature in range(y_flat.shape[1]):\n",
    "        rs.append(scipy.stats.spearmanr(pred_concat[:,feature], true_concat[:,feature]).correlation)\n",
    "    r = np.mean(rs) \n",
    "    proj_r2_array[i] = r2; proj_r_array[i] = r;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 across lags after projection out\n",
    "idx_max = np.argmax(proj_r2_array)\n",
    "plt.plot(lag_axis,proj_r2_array, color = 'k')\n",
    "plt.axvline(lag_axis[idx_max],color = 'k',linestyle='--')\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation across lags after projection out\n",
    "idx_max = np.argmax(proj_r_array)\n",
    "plt.plot(lag_axis,proj_r_array, color = 'k')\n",
    "plt.axvline(lag_axis[idx_max],color = 'k',linestyle='--')\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('Correlation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdt_env",
   "language": "python",
   "name": "sdt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
