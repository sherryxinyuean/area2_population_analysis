{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import math\n",
    "matplotlib.rc('font', size=18)\n",
    "import scipy\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothing functions\n",
    "import scipy.signal as signal\n",
    "def smooth_column(x, window, dtype):\n",
    "    y = signal.convolve(x.astype(dtype),window,'same')\n",
    "    return y\n",
    "def smooth_spk(neural_data, gauss_width, bin_width,dtype=\"float64\"):\n",
    "    \"\"\" Smooth spikes by Gaussian kernel \"\"\"\n",
    "    gauss_bin_std = gauss_width / bin_width\n",
    "    win_len = int(6*gauss_bin_std)\n",
    "    window = signal.gaussian(win_len, gauss_bin_std, sym = True)\n",
    "    window /= np.sum(window)\n",
    "    smoothed_spikes = np.apply_along_axis(lambda x: smooth_column(x, window, dtype), 0, neural_data)\n",
    "    return smoothed_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 function\n",
    "def get_sses_pred(y_test,y_test_pred):\n",
    "    sse=np.sum((y_test_pred-y_test)**2,axis=0)\n",
    "    return sse\n",
    "def get_sses_mean(y_test):\n",
    "    y_mean=np.mean(y_test,axis=0)\n",
    "    sse_mean=np.sum((y_test-y_mean)**2,axis=0)\n",
    "    return sse_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing for cross-validation loops\n",
    "def process_train_test(X,y,training_set,test_set):\n",
    "    \"\"\" z-score X, mean-center Y\"\"\"\n",
    "    X_train = X[training_set,:]\n",
    "    X_test = X[test_set,:]\n",
    "    y_train = y[training_set,:]\n",
    "    y_test = y[test_set,:]\n",
    "\n",
    "    X_train_mean=np.nanmean(X_train,axis=0)\n",
    "    X_train_std=np.nanstd(X_train,axis=0)   \n",
    "    y_train_mean=np.nanmean(y_train,axis=0)\n",
    "\n",
    "    X_train=(X_train-X_train_mean)/X_train_std\n",
    "    X_test=(X_test-X_train_mean)/X_train_std\n",
    "\n",
    "    y_train=y_train-y_train_mean\n",
    "    y_test=y_test-y_train_mean\n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "def nans(shape, dtype=float):\n",
    "    \"\"\" Returns array of NaNs with defined shape\"\"\"\n",
    "    a = np.empty(shape, dtype)\n",
    "    a.fill(np.nan)\n",
    "    return a\n",
    "\n",
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'\"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "foldername = \"/Users/sherryan/area2_population_analysis/random_target/\"\n",
    "data = sio.loadmat(foldername+'s1_all_units_trialized.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = data['position']\n",
    "velocity = data['velocity']\n",
    "acceleration = data['acceleration']\n",
    "good_neural_data = data['good_neural_data']\n",
    "all_neural_data = data['all_neural_data']\n",
    "target_variable = data['tgtVar']\n",
    "target_position = target_variable[:,0:2]\n",
    "directions = target_variable[:,-1]\n",
    "\n",
    "print(position.shape,'kinematics')\n",
    "print(good_neural_data.shape,'good neural data')\n",
    "print(all_neural_data.shape,'all neural data')\n",
    "print(target_position.shape,'target position')\n",
    "print(directions.shape,'directions')\n",
    "\n",
    "n_trials = len(all_neural_data)\n",
    "print(n_trials,'trials')\n",
    "n_good_neurons = good_neural_data[0,0].shape[1]\n",
    "print(n_good_neurons,'good neurons')\n",
    "n_neurons = all_neural_data[0,0].shape[1]\n",
    "print(n_neurons,'neurons')\n",
    "\n",
    "# kinematics (Y) and neural_data (X) are shaped (n_trials,1) \n",
    "# each is an 1d array of 2d arrays; I did not use a 3d array so to allow for variable trial length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each trial of neural data is shaped (n_bins, n_neurons)\n",
    "# each trial of kinematics is shaped (n_bins, n_kinematic_features)\n",
    "\n",
    "print('Data shape for the 1st trial:')\n",
    "print(good_neural_data[0,0].shape)\n",
    "print(position[0,0].shape)\n",
    "print('')\n",
    "print('Data shape for the 2nd trial:')\n",
    "print(good_neural_data[1,0].shape)\n",
    "print(position[1,0].shape)\n",
    "print('')\n",
    "print('Data shape for the 3rd trial:')\n",
    "print(good_neural_data[2,0].shape)\n",
    "print(position[2,0].shape)\n",
    "\n",
    "\n",
    "# Note: length of kinematics is the actual trial length,\n",
    "# neural data should have more timepoints than kinematics to allow time-lagged decoding, can be wider than the range of time lags of interest\n",
    "# i.e. here I'm interested in testing efferent/afferent signals between -300 to +300ms, but I got neural data -500ms to +500ms;\n",
    "# which means I included 50 more timepoints prior to trial start (10-ms bin), and 50 more timepoints post trial end, thus in total 100 more timepoints\n",
    "\n",
    "# let the model know the amount of previous/future bins are included in neural data\n",
    "n_previous_bins = 50\n",
    "n_future_bins = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_start_filter = []\n",
    "for i in range(n_trials):\n",
    "    center_start_filter.append((target_position[i]==[0])[0]==True & (target_position[i]==[0])[1]==True)\n",
    "center_start_filter = np.array(center_start_filter)\n",
    "print(center_start_filter.shape)\n",
    "print(sum(center_start_filter),'center start trials')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data smoothing (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this data has 0.01 s = 10 ms bins \n",
    "dt = 0.01\n",
    "gauss_width = 40 #in ms\n",
    "bin_width = dt*1000 #in ms\n",
    "\n",
    "# smooth neural data by 40-ms gaussian kernel\n",
    "neural_smth_40 = []\n",
    "for i in range(n_trials):\n",
    "    neural_d = good_neural_data[i,0]\n",
    "    smth_40 = smooth_spk(neural_d, gauss_width, bin_width)\n",
    "    neural_smth_40.append(smth_40)\n",
    "good_neural_smth_40 = np.array(neural_smth_40,dtype=object).reshape(n_trials,1)\n",
    "\n",
    "neural_smth_40 = []\n",
    "for i in range(n_trials):\n",
    "    neural_d = all_neural_data[i,0]\n",
    "    smth_40 = smooth_spk(neural_d, gauss_width, bin_width)\n",
    "    neural_smth_40.append(smth_40)\n",
    "all_neural_smth_40 = np.array(neural_smth_40,dtype=object).reshape(n_trials,1)\n",
    "\n",
    "print(good_neural_smth_40.shape)\n",
    "print(all_neural_smth_40.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check effects of smoothing for 10 neurons in the first trial\n",
    "plt.plot(good_neural_data[0,0][:,:10])\n",
    "plt.show()\n",
    "plt.plot(good_neural_smth_40[0,0][:,:10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFY THIS PART\n",
    "\n",
    "lag_axis = np.arange(-300, 300, 20) # array of time lags to try, here -300 to +300 ms with 20-ms step\n",
    "bin_width = 10 #in ms\n",
    "# x = good_neural_smth_40\n",
    "# y = velocity\n",
    "\n",
    "x = good_neural_smth_40[center_start_filter]\n",
    "y = velocity[center_start_filter]\n",
    "\n",
    "filter = True #default filter option as False, can set in the next cell\n",
    "\n",
    "n_total_datapoints = 0\n",
    "for i in range(len(y)): \n",
    "    n_total_datapoints+=y[i,0].shape[0] # Count total number of data points to fit in model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(sum(center_start_filter)):\n",
    "    # plt.plot(np.sqrt(np.sum(y[i,0]**2,axis=1)))\n",
    "    plt.axvline(x=5)\n",
    "    plt.plot(y[i,0][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=7\n",
    "plt.plot(np.sqrt(np.sum(y[i,0]**2,axis=1)))\n",
    "plt.show()\n",
    "plt.plot(y[i,0][:,0])\n",
    "plt.plot(y[i,0][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "def flat_lagged_neural(x, n_previous_bins, n_future_bins, lagged_bins, n_total_datapoints):\n",
    "    \"\"\" Flatten time-lagged & trial-ized neural data into 2d array \"\"\"\n",
    "    n_neurons = x[0,0].shape[1]\n",
    "    x_flat = nans([n_total_datapoints,n_neurons])    \n",
    "    trial_save_idx = 0\n",
    "    for tr in range(len(x)):\n",
    "        trial_data = x[tr,0]\n",
    "        start, end = int(n_previous_bins+lagged_bins), int(len(trial_data)-n_future_bins+lagged_bins)\n",
    "        n = len(trial_data[start:end,:])\n",
    "        x_flat[trial_save_idx:trial_save_idx+n,:] = trial_data[start:end,:]\n",
    "        trial_save_idx+=n\n",
    "    return x_flat\n",
    "def flat_kinematics(y,n_total_timepoints):\n",
    "    \"\"\" Flatten trial-ized kinematics into 2d array \"\"\"\n",
    "    y_flat = nans([n_total_timepoints,2])\n",
    "    trial_save_idx = 0\n",
    "    for tr in range(len(y)):\n",
    "        n = len(y[tr,0])\n",
    "        y_flat[trial_save_idx:trial_save_idx+n,:] = y[tr,0]\n",
    "        trial_save_idx+=n\n",
    "    return y_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL FILTER, filter out invalid nrns that are not very active during trial time\n",
    "filter = True\n",
    "x_flat = flat_lagged_neural(x, n_previous_bins, n_future_bins, 0, n_total_datapoints)\n",
    "fr_thresh = 0.1 #threshold in Hz\n",
    "fr = np.sum(x_flat,axis=0)/x_flat.shape[0]*(1000/bin_width)\n",
    "neuron_filter = fr > fr_thresh\n",
    "print(neuron_filter)\n",
    "print(sum(neuron_filter),'valid neurons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "all_data = flat_lagged_neural(x, n_previous_bins, n_future_bins, 0, n_total_datapoints)[:,neuron_filter]\n",
    "print(all_data.shape)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(all_data)\n",
    "pca = PCA(n_components=40)\n",
    "pca.fit(X)\n",
    "sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_flat = flat_kinematics(y, n_total_datapoints) #flatten y\n",
    "y_proc = y_flat - np.nanmean(y_flat,axis=0) #mean-center y\n",
    "\n",
    "#save R2, r, and coeffients across lags\n",
    "r2_array = nans([len(lag_axis)]); x_r2_array = nans([len(lag_axis)]);\n",
    "y_r2_array = nans([len(lag_axis)]);\n",
    "r_array = nans([len(lag_axis)])\n",
    "# if filter:\n",
    "#     coef_array = nans([len(lag_axis),2,sum(neuron_filter)]) #OPTIONAL FILTER\n",
    "# else:\n",
    "#     coef_array = nans([len(lag_axis),2,n_neurons]) \n",
    "n_dims = 40\n",
    "coef_array = nans([len(lag_axis),2,n_dims]) \n",
    "\n",
    "for i in range(len(lag_axis)):\n",
    "    current_lagged_bins = lag_axis[i]/bin_width\n",
    "    x_flat = flat_lagged_neural(x, n_previous_bins, n_future_bins, current_lagged_bins, n_total_datapoints) #flatten x\n",
    "    # if filter:\n",
    "    #     x_flat = x_flat[:,neuron_filter] #OPTIONAL FILTER\n",
    "    x_flat =  pca.transform(scaler.transform(x_flat[:,neuron_filter]))\n",
    "    x_proc = (x_flat - np.nanmean(x_flat,axis=0))/np.nanstd(x_flat,axis=0) #z-score x\n",
    "\n",
    "    lr_all = GridSearchCV(Ridge(),{'alpha':np.logspace(-3, 3, 7)}) #Grid-search penalty term for Ridge regularization\n",
    "    lr_all.fit(x_proc, y_proc)\n",
    "    coef_array[i,:,:] = lr_all.best_estimator_.coef_ #Save model's coefficient as fitted to all data points\n",
    "\n",
    "    #Cross-validate R2\n",
    "    kf = KFold(n_splits=5, shuffle=False) #Simple 5-fold CV (not the most rigorous); no shuffle, otherwise there'll be too much leakage across trials\n",
    "    true_concat = nans([n_total_datapoints,2])\n",
    "    pred_concat = nans([n_total_datapoints,2]) #Concatenate true and predicted kinematics for final R2 calculation\n",
    "    save_idx = 0\n",
    "    for training_set, test_set in kf.split(range(0,len(x_flat))):\n",
    "        X_train, X_test, y_train, y_test = process_train_test(x_flat,y_flat,training_set,test_set)\n",
    "        lr = GridSearchCV(Ridge(),{'alpha':np.logspace(-3, 3, 7)})\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_test_predicted = lr.predict(X_test)\n",
    "        n = y_test_predicted.shape[0]\n",
    "        true_concat[save_idx:save_idx+n,:] = y_test\n",
    "        pred_concat[save_idx:save_idx+n,:] = y_test_predicted\n",
    "        save_idx += n\n",
    "    # r2 across features\n",
    "    sses =get_sses_pred(true_concat,pred_concat)\n",
    "    sses_mean=get_sses_mean(true_concat)\n",
    "    r2 =1-np.sum(sses)/np.sum(sses_mean)     \n",
    "\n",
    "    sses =get_sses_pred(true_concat[:,0],pred_concat[:,0])\n",
    "    sses_mean=get_sses_mean(true_concat[:,0])\n",
    "    x_r2 =1-np.sum(sses)/np.sum(sses_mean) \n",
    "\n",
    "    sses =get_sses_pred(true_concat[:,1],pred_concat[:,1])\n",
    "    sses_mean=get_sses_mean(true_concat[:,1])\n",
    "    y_r2 =1-np.sum(sses)/np.sum(sses_mean) \n",
    "    # mean spearman's r across features\n",
    "    rs = []\n",
    "    for feature in range(y_flat.shape[1]):\n",
    "        rs.append(scipy.stats.spearmanr(pred_concat[:,feature], true_concat[:,feature]).correlation)\n",
    "    r = np.mean(rs) \n",
    "    r2_array[i] = r2; r_array[i] = r;\n",
    "    x_r2_array[i] = x_r2; y_r2_array[i] = y_r2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 across lags (are there two peaks?)\n",
    "idx_max = np.argmax(r2_array)\n",
    "plt.plot(lag_axis,r2_array, color = 'k')\n",
    "plt.axvline(lag_axis[idx_max],color = 'k',linestyle='--')\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('R2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_max = np.argmax(x_r2_array)\n",
    "plt.plot(lag_axis,x_r2_array, color = 'k')\n",
    "plt.axvline(lag_axis[idx_max],color = 'k',linestyle='--')\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('X R2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_max = np.argmax(y_r2_array)\n",
    "plt.plot(lag_axis,y_r2_array, color = 'k')\n",
    "plt.axvline(lag_axis[idx_max],color = 'k',linestyle='--')\n",
    "plt.xlabel('Time lag (ms)')\n",
    "plt.ylabel('Y R2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdt_env",
   "language": "python",
   "name": "sdt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
